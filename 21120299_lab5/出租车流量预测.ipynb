{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b295ed49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error as mse, mean_absolute_error as mae\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cded3a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a195e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NYCSTDNDataset():\n",
    "    def __init__(self,data_path,window_size=7) -> None:\n",
    "        self.window_size = window_size\n",
    "        self.data = torch.from_numpy(self.loading(data_path)).float()\n",
    "\n",
    "    def loading(self,data_path):\n",
    "        data = np.load(data_path)['volume']\n",
    "        # print(data.shape)\n",
    "        self.max_val,self.min_val = np.max(data),np.min(data)\n",
    "        dataset = slidingWindow(data,self.window_size)\n",
    "        dataset = np.array(dataset).transpose(0,1,4,2,3) # (1914, 7, 2, 10, 20)\n",
    "        dataset = dataset.reshape(dataset.shape[0],dataset.shape[1],-1)\n",
    "        dataset = (dataset - self.min_val) / (self.max_val - self.min_val)\n",
    "        return dataset\n",
    "     \n",
    "    def denormalize(self,x):        \n",
    "        return x * (self.max_val - self.min_val) + self.min_val\n",
    "\n",
    "def slidingWindow(seqs,size):\n",
    "    \"\"\"\n",
    "    seqs: ndarray sequence, shape(seqlen,area_nums,2)\n",
    "    size: sliding window size\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in range(seqs.shape[0] - size + 1):\n",
    "        result.append(seqs[i:i + size,:,:,:]) #(7, 10, 20, 2) \n",
    "    # print(np.array(result).shape)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5479bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawPlot(heights,fname,ylabel):\n",
    "    \"\"\"\n",
    "    功能：绘制训练集上的准确率和测试集上的loss和acc变化曲线\n",
    "    heights: 纵轴值列表\n",
    "    fname：保存的文件名\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    x = [i for i in range(1,len(heights[0]) + 1)]\n",
    "    # 绘制训练集和测试集上的loss变化曲线子图\n",
    "    plt.xlabel(\"epoch\")\n",
    "    # 设置横坐标的刻度间隔\n",
    "    plt.xticks([i for i in range(0,len(heights[0]) + 1,5)])\n",
    "    \n",
    "    axe1 = plt.subplot(2,2,1)\n",
    "    plt.ylabel(ylabel[0])\n",
    "    axe1.plot(x,heights[0],label=\"train\")\n",
    "    axe1.plot(x,heights[1],label=\"test\")\n",
    "    axe1.legend()\n",
    "\n",
    "    axe2 = plt.subplot(2,2,2)\n",
    "    plt.ylabel(ylabel[1])\n",
    "    axe2.plot(x,heights[2],label=\"train\")\n",
    "    axe2.plot(x,heights[3],label=\"test\")\n",
    "    plt.legend()\n",
    "\n",
    "    axe3 = plt.subplot(2,2,3)\n",
    "    plt.ylabel(ylabel[2])\n",
    "    axe3.plot(x,heights[4],label=\"train\")\n",
    "    axe3.plot(x,heights[5],label=\"test\")\n",
    "    plt.legend()\n",
    "\n",
    "    axe4 = plt.subplot(2,2,4)\n",
    "    plt.ylabel(ylabel[3])\n",
    "    axe4.plot(x,heights[6],label=\"train\")\n",
    "    axe4.plot(x,heights[7],label=\"test\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(\"images/{}\".format(fname))\n",
    "    plt.show()\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    non_zero_index = (y_true > 0)\n",
    "    y_true = y_true[non_zero_index]\n",
    "    y_pred = y_pred[non_zero_index]\n",
    "\n",
    "    mape = np.abs((y_true - y_pred) / y_true)\n",
    "    mape[np.isinf(mape)] = 0\n",
    "    return np.mean(mape) * 100\n",
    "\n",
    "def nextBatch(data,batch_size):\n",
    "    \"\"\"\n",
    "    Divide data into mini-batch\n",
    "    \"\"\"\n",
    "    data_length = len(data)\n",
    "    num_batches = math.ceil(data_length / batch_size)\n",
    "    for idx in range(num_batches):\n",
    "        start_idx = batch_size * idx\n",
    "        end_idx = min(start_idx + batch_size, data_length)\n",
    "        yield data[start_idx:end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3b0a512",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size,drop_prob):\n",
    "        super(MyLSTM,self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                          hidden_size=hidden_size,\n",
    "                          batch_first=True,\n",
    "                          num_layers=2)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_size,output_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        x: (batch,seq,feature)\n",
    "        \"\"\"\n",
    "        x,_ = self.lstm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(torch.mean(x,dim=1))\n",
    "        x = F.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2072e7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGRU(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size,drop_prob):\n",
    "        super(MyGRU,self).__init__()\n",
    "        self.gru = nn.GRU(input_size=input_size,\n",
    "                          hidden_size=hidden_size,\n",
    "                          batch_first=True,\n",
    "                          num_layers=2)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_size,output_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        x: (batch,seq,feature)\n",
    "        \"\"\"\n",
    "        x,_ = self.gru(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(torch.mean(x,dim=1))\n",
    "        x = F.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33800a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self,in_channel,out_channels,input_size,hidden_size,output_size,drop_prob):\n",
    "        super(CNNLSTM,self).__init__()\n",
    "\n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=in_channel, out_channels=out_channels[0], kernel_size=(3, 3, 3)),\n",
    "            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
    "            nn.Conv3d(in_channels=out_channels[0], out_channels=out_channels[1], kernel_size=(3, 3, 3)),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                          hidden_size=hidden_size,\n",
    "                          batch_first=True,\n",
    "                          num_layers=2)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(384 + hidden_size,output_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        x: (batch,depth/seqlen,channels*h*w)\n",
    "        \"\"\"\n",
    "        x = x.view(x.shape[0],x.shape[1],2,10,20)\n",
    "        cnn_feats = self.convs(x.transpose(1,2))\n",
    "        # print(cnn_feats.shape)\n",
    "        gru_out,_ = self.lstm(x.view(x.shape[0],x.shape[1],-1))\n",
    "        gru_feats =torch.mean(gru_out,dim=1)\n",
    "        fusion_feats = torch.cat((cnn_feats.view(cnn_feats.shape[0],-1),gru_feats),dim=1)\n",
    "        x = self.fc(self.dropout(fusion_feats))\n",
    "        return F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62128185",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNGRU(nn.Module):\n",
    "    def __init__(self,in_channel,out_channels,input_size,hidden_size,output_size,drop_prob):\n",
    "        super(CNNGRU,self).__init__()\n",
    "\n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=in_channel, out_channels=out_channels[0], kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
    "            nn.Conv3d(in_channels=out_channels[0], out_channels=out_channels[1], kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)),\n",
    "            nn.Conv3d(in_channels=out_channels[0], out_channels=out_channels[1], kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            nn.Conv3d(in_channels=out_channels[0], out_channels=out_channels[1], kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "        )\n",
    "\n",
    "        self.gru = nn.GRU(input_size=input_size,\n",
    "                          hidden_size=hidden_size,\n",
    "                          batch_first=True,\n",
    "                          num_layers=2)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(4 + hidden_size,output_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        x: (batch,depth/seqlen,channels*h*w)\n",
    "        \"\"\"\n",
    "        x = x.view(x.shape[0],x.shape[1],2,10,20)\n",
    "        cnn_feats = self.convs(x.transpose(1,2))\n",
    "        gru_out,_ = self.gru(x.view(x.shape[0],x.shape[1],-1))\n",
    "        gru_feats =torch.mean(gru_out,dim=1)\n",
    "        fusion_feats = torch.cat((cnn_feats.view(cnn_feats.shape[0],-1),gru_feats),dim=1)\n",
    "        x = self.fc(self.dropout(fusion_feats))\n",
    "        return F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed031264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model,train_dataset,loss_fn,optimizer):\n",
    "    model.train()\n",
    "    iteration = 0\n",
    "    for batch in nextBatch(shuffle(train_dataset.data),batch_size):\n",
    "        x,y = batch[:,:-1,:], batch[:,-1,:]\n",
    "        x,y = x.to(device),y.to(device)\n",
    "        y_hat = model(x)\n",
    "        l = loss_fn(y_hat, y)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        l.backward()\n",
    "        # # 梯度裁剪\n",
    "        # nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0, norm_type=2)\n",
    "        optimizer.step()\n",
    "        iteration += 1\n",
    "        if iteration % 10 == 0:\n",
    "            print(\"Iteraion {}, Train Loss: {:.8f}\".format(iteration, l.item()))\n",
    "    \n",
    "    train_rmse,train_mae,train_mape,train_loss = test(model,train_dataset,loss_fn)\n",
    "    \n",
    "    return (train_rmse,train_mae,train_mape,train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9811546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,test_dataset,loss_fn):\n",
    "    model.eval()\n",
    "    y_hats = []\n",
    "    test_l_sum,c = 0,0\n",
    "    with torch.no_grad():\n",
    "        for batch in nextBatch(test_dataset.data, batch_size=batch_size):\n",
    "            x,y = batch[:,:-1,:], batch[:,-1,:]\n",
    "            x,y = x.to(device),y.to(device)\n",
    "            y_hat = model(x)\n",
    "            test_l_sum += loss_fn(y_hat,y).item()\n",
    "            c += 1\n",
    "            y_hats.append(y_hat.detach().cpu().numpy())\n",
    "        y_hats = np.concatenate(y_hats)\n",
    "    y_true = test_dataset.data[:,-1,:]\n",
    "    y_hats = test_dataset.denormalize(y_hats)\n",
    "    y_true = test_dataset.denormalize(y_true)\n",
    "    y_true = y_true.reshape(y_true.size(0),-1)\n",
    "    rmse_score,mae_score,mape_score = math.sqrt(mse(y_true, y_hats)), \\\n",
    "                                mae(y_true, y_hats), \\\n",
    "                                mape(y_true, y_hats)    \n",
    "    return (rmse_score,mae_score,mape_score,test_l_sum / c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc7572f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset done\n"
     ]
    }
   ],
   "source": [
    "train_dataset,test_dataset = NYCSTDNDataset(data_path='NYC-stdn/volume_train.npz'),\\\n",
    "                                NYCSTDNDataset(data_path='NYC-stdn/volume_test.npz')\n",
    "print(\"loading dataset done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a09ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_type = \"lstm\"\n",
    "# model_type = \"gru\"\n",
    "model_type = \"cnnlstm\"\n",
    "# model_type = \"cnngru\"\n",
    "batch_size = 64\n",
    "epochs = 1000\n",
    "lr = 0.001\n",
    "lr_drop = [500,750]\n",
    "drop_prob = 0.3\n",
    "hidden_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9da2b10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model cnnlstm done\n"
     ]
    }
   ],
   "source": [
    "if model_type == \"gru\":\n",
    "    model = MyGRU(input_size=400,\n",
    "                  hidden_size=hidden_size,\n",
    "                  output_size=400,\n",
    "                  drop_prob=drop_prob)\n",
    "elif model_type == \"lstm\":\n",
    "    model = MyLSTM(input_size=400,\n",
    "                   hidden_size=hidden_size,\n",
    "                   output_size=400,\n",
    "                   drop_prob=drop_prob)\n",
    "elif model_type == \"cnngru\":\n",
    "    model = CNNGRU(\n",
    "                   in_channel=2,\n",
    "                   out_channels=[2,2],\n",
    "                   input_size=400,\n",
    "                   hidden_size=hidden_size,\n",
    "                   output_size=400,\n",
    "                   drop_prob=drop_prob)\n",
    "elif model_type == \"cnnlstm\":\n",
    "    model = CNNLSTM(\n",
    "                   in_channel=2,\n",
    "                   out_channels=[64,128],\n",
    "                   input_size=400,\n",
    "                   hidden_size=hidden_size,\n",
    "                   output_size=400,\n",
    "                   drop_prob=drop_prob)\n",
    "print(\"loading model {} done\".format(model_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff9f03e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "optimizer =  optim.Adam(params=model.parameters(),lr=lr)\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, lr_drop)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89ebdd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========epoch 1=========\n",
      "Iteraion 10, Train Loss: 0.00722381\n",
      "Iteraion 20, Train Loss: 0.00511106\n",
      "Iteraion 30, Train Loss: 0.00471900\n",
      "Epoch: 1, RMSE: 84.8969, MAE: 31.3672, MAPE: 108.0310,Train Loss: 0.00437562\n",
      "Epoch: 1, RMSE: 89.6985, MAE: 34.0741, MAPE: 101.0627,Test Loss: 0.00484085\n",
      "=========epoch 2=========\n",
      "Iteraion 10, Train Loss: 0.00462153\n",
      "Iteraion 20, Train Loss: 0.00441196\n",
      "Iteraion 30, Train Loss: 0.00495958\n",
      "Epoch: 2, RMSE: 75.3380, MAE: 27.8388, MAPE: 101.8228,Train Loss: 0.00344556\n",
      "Epoch: 2, RMSE: 79.4415, MAE: 30.1970, MAPE: 97.6448,Test Loss: 0.00379866\n",
      "=========epoch 3=========\n",
      "Iteraion 10, Train Loss: 0.00443056\n",
      "Iteraion 20, Train Loss: 0.00382092\n",
      "Iteraion 30, Train Loss: 0.00367955\n",
      "Epoch: 3, RMSE: 73.6077, MAE: 27.1279, MAPE: 99.9948,Train Loss: 0.00328920\n",
      "Epoch: 3, RMSE: 77.7248, MAE: 29.5133, MAPE: 96.2822,Test Loss: 0.00363617\n",
      "=========epoch 4=========\n",
      "Iteraion 10, Train Loss: 0.00402534\n",
      "Iteraion 20, Train Loss: 0.00310742\n",
      "Iteraion 30, Train Loss: 0.00349074\n",
      "Epoch: 4, RMSE: 68.8065, MAE: 25.1680, MAPE: 98.4972,Train Loss: 0.00287409\n",
      "Epoch: 4, RMSE: 72.5215, MAE: 27.3057, MAPE: 94.7257,Test Loss: 0.00316616\n",
      "=========epoch 5=========\n",
      "Iteraion 10, Train Loss: 0.00311528\n",
      "Iteraion 20, Train Loss: 0.00274765\n",
      "Iteraion 30, Train Loss: 0.00302140\n",
      "Epoch: 5, RMSE: 65.0629, MAE: 23.9233, MAPE: 95.7405,Train Loss: 0.00256976\n",
      "Epoch: 5, RMSE: 68.4546, MAE: 25.9173, MAPE: 91.2280,Test Loss: 0.00282114\n",
      "=========epoch 6=========\n",
      "Iteraion 10, Train Loss: 0.00228287\n",
      "Iteraion 20, Train Loss: 0.00224575\n",
      "Iteraion 30, Train Loss: 0.00149588\n",
      "Epoch: 6, RMSE: 45.0948, MAE: 20.8996, MAPE: 199.8377,Train Loss: 0.00123460\n",
      "Epoch: 6, RMSE: 47.3054, MAE: 22.5045, MAPE: 208.3131,Test Loss: 0.00134699\n",
      "=========epoch 7=========\n",
      "Iteraion 10, Train Loss: 0.00130597\n",
      "Iteraion 20, Train Loss: 0.00072099\n",
      "Iteraion 30, Train Loss: 0.00078754\n",
      "Epoch: 7, RMSE: 31.4419, MAE: 15.0899, MAPE: 162.6337,Train Loss: 0.00060022\n",
      "Epoch: 7, RMSE: 32.5442, MAE: 16.0111, MAPE: 162.9624,Test Loss: 0.00063706\n",
      "=========epoch 8=========\n",
      "Iteraion 10, Train Loss: 0.00080722\n",
      "Iteraion 20, Train Loss: 0.00071797\n",
      "Iteraion 30, Train Loss: 0.00063946\n",
      "Epoch: 8, RMSE: 29.4642, MAE: 13.5078, MAPE: 143.1162,Train Loss: 0.00052713\n",
      "Epoch: 8, RMSE: 31.0502, MAE: 14.4682, MAPE: 142.2651,Test Loss: 0.00057988\n",
      "=========epoch 9=========\n",
      "Iteraion 10, Train Loss: 0.00072254\n",
      "Iteraion 20, Train Loss: 0.00060994\n",
      "Iteraion 30, Train Loss: 0.00061128\n",
      "Epoch: 9, RMSE: 26.4631, MAE: 12.6715, MAPE: 151.8011,Train Loss: 0.00042520\n",
      "Epoch: 9, RMSE: 27.6454, MAE: 13.4583, MAPE: 150.2751,Test Loss: 0.00045961\n",
      "=========epoch 10=========\n",
      "Iteraion 10, Train Loss: 0.00058661\n",
      "Iteraion 20, Train Loss: 0.00056786\n",
      "Iteraion 30, Train Loss: 0.00054926\n",
      "Epoch: 10, RMSE: 24.9751, MAE: 11.9266, MAPE: 144.5094,Train Loss: 0.00037872\n",
      "Epoch: 10, RMSE: 26.1053, MAE: 12.6591, MAPE: 141.3936,Test Loss: 0.00040992\n",
      "=========epoch 11=========\n",
      "Iteraion 10, Train Loss: 0.00052491\n",
      "Iteraion 20, Train Loss: 0.00045020\n",
      "Iteraion 30, Train Loss: 0.00058820\n",
      "Epoch: 11, RMSE: 24.2764, MAE: 11.2109, MAPE: 127.3066,Train Loss: 0.00035783\n",
      "Epoch: 11, RMSE: 25.6831, MAE: 11.9855, MAPE: 122.6123,Test Loss: 0.00039678\n",
      "=========epoch 12=========\n",
      "Iteraion 10, Train Loss: 0.00049305\n",
      "Iteraion 20, Train Loss: 0.00052763\n",
      "Iteraion 30, Train Loss: 0.00045700\n",
      "Epoch: 12, RMSE: 23.4856, MAE: 10.9141, MAPE: 128.0288,Train Loss: 0.00033488\n",
      "Epoch: 12, RMSE: 24.9700, MAE: 11.6910, MAPE: 123.0717,Test Loss: 0.00037511\n",
      "=========epoch 13=========\n",
      "Iteraion 10, Train Loss: 0.00047859\n",
      "Iteraion 20, Train Loss: 0.00038934\n",
      "Iteraion 30, Train Loss: 0.00040844\n",
      "Epoch: 13, RMSE: 22.5389, MAE: 10.5864, MAPE: 129.0729,Train Loss: 0.00030845\n",
      "Epoch: 13, RMSE: 23.8762, MAE: 11.2917, MAPE: 123.7838,Test Loss: 0.00034291\n",
      "=========epoch 14=========\n",
      "Iteraion 10, Train Loss: 0.00044317\n",
      "Iteraion 20, Train Loss: 0.00045015\n",
      "Iteraion 30, Train Loss: 0.00041647\n",
      "Epoch: 14, RMSE: 21.5514, MAE: 10.3415, MAPE: 131.5035,Train Loss: 0.00028199\n",
      "Epoch: 14, RMSE: 22.6608, MAE: 10.9631, MAPE: 125.6513,Test Loss: 0.00030893\n",
      "=========epoch 15=========\n",
      "Iteraion 10, Train Loss: 0.00041078\n",
      "Iteraion 20, Train Loss: 0.00046491\n",
      "Iteraion 30, Train Loss: 0.00036757\n",
      "Epoch: 15, RMSE: 21.2827, MAE: 9.9415, MAPE: 121.1738,Train Loss: 0.00027500\n",
      "Epoch: 15, RMSE: 22.5896, MAE: 10.6236, MAPE: 115.9000,Test Loss: 0.00030693\n",
      "=========epoch 16=========\n",
      "Iteraion 10, Train Loss: 0.00040364\n",
      "Iteraion 20, Train Loss: 0.00035070\n",
      "Iteraion 30, Train Loss: 0.00036761\n",
      "Epoch: 16, RMSE: 21.3402, MAE: 9.7314, MAPE: 111.9502,Train Loss: 0.00027651\n",
      "Epoch: 16, RMSE: 23.0156, MAE: 10.5219, MAPE: 106.4403,Test Loss: 0.00031861\n",
      "=========epoch 17=========\n",
      "Iteraion 10, Train Loss: 0.00039183\n",
      "Iteraion 20, Train Loss: 0.00031234\n",
      "Iteraion 30, Train Loss: 0.00028313\n",
      "Epoch: 17, RMSE: 20.3780, MAE: 9.4701, MAPE: 114.6257,Train Loss: 0.00025212\n",
      "Epoch: 17, RMSE: 21.8091, MAE: 10.1563, MAPE: 108.2094,Test Loss: 0.00028603\n",
      "=========epoch 18=========\n",
      "Iteraion 10, Train Loss: 0.00037672\n",
      "Iteraion 20, Train Loss: 0.00033966\n",
      "Iteraion 30, Train Loss: 0.00038664\n",
      "Epoch: 18, RMSE: 19.8921, MAE: 9.2412, MAPE: 112.4525,Train Loss: 0.00024023\n",
      "Epoch: 18, RMSE: 21.2017, MAE: 9.8896, MAPE: 105.5821,Test Loss: 0.00027039\n",
      "=========epoch 19=========\n",
      "Iteraion 10, Train Loss: 0.00031684\n",
      "Iteraion 20, Train Loss: 0.00034569\n",
      "Iteraion 30, Train Loss: 0.00031870\n",
      "Epoch: 19, RMSE: 19.9138, MAE: 9.0646, MAPE: 105.3415,Train Loss: 0.00024076\n",
      "Epoch: 19, RMSE: 21.6020, MAE: 9.7995, MAPE: 98.3910,Test Loss: 0.00028071\n",
      "=========epoch 20=========\n",
      "Iteraion 10, Train Loss: 0.00034566\n",
      "Iteraion 20, Train Loss: 0.00032662\n",
      "Iteraion 30, Train Loss: 0.00032902\n",
      "Epoch: 20, RMSE: 19.0071, MAE: 8.7617, MAPE: 104.7697,Train Loss: 0.00021932\n",
      "Epoch: 20, RMSE: 20.2300, MAE: 9.3521, MAPE: 97.6110,Test Loss: 0.00024620\n",
      "=========epoch 21=========\n",
      "Iteraion 10, Train Loss: 0.00028062\n",
      "Iteraion 20, Train Loss: 0.00027647\n",
      "Iteraion 30, Train Loss: 0.00030302\n",
      "Epoch: 21, RMSE: 21.3603, MAE: 9.2605, MAPE: 93.5067,Train Loss: 0.00027703\n",
      "Epoch: 21, RMSE: 23.3802, MAE: 10.1295, MAPE: 86.7713,Test Loss: 0.00032876\n",
      "=========epoch 22=========\n",
      "Iteraion 10, Train Loss: 0.00030642\n",
      "Iteraion 20, Train Loss: 0.00030977\n",
      "Iteraion 30, Train Loss: 0.00025943\n",
      "Epoch: 22, RMSE: 19.0232, MAE: 8.5975, MAPE: 101.5715,Train Loss: 0.00021969\n",
      "Epoch: 22, RMSE: 20.7198, MAE: 9.3185, MAPE: 94.7648,Test Loss: 0.00025815\n",
      "=========epoch 23=========\n",
      "Iteraion 10, Train Loss: 0.00025812\n",
      "Iteraion 20, Train Loss: 0.00023830\n",
      "Iteraion 30, Train Loss: 0.00029052\n",
      "Epoch: 23, RMSE: 18.4271, MAE: 8.3729, MAPE: 98.4483,Train Loss: 0.00020613\n",
      "Epoch: 23, RMSE: 19.6040, MAE: 8.9348, MAPE: 90.9842,Test Loss: 0.00023115\n",
      "=========epoch 24=========\n",
      "Iteraion 10, Train Loss: 0.00026999\n",
      "Iteraion 20, Train Loss: 0.00027725\n",
      "Iteraion 30, Train Loss: 0.00027241\n",
      "Epoch: 24, RMSE: 18.1677, MAE: 8.2248, MAPE: 97.3353,Train Loss: 0.00020038\n",
      "Epoch: 24, RMSE: 19.5201, MAE: 8.8146, MAPE: 89.9965,Test Loss: 0.00022915\n",
      "=========epoch 25=========\n",
      "Iteraion 10, Train Loss: 0.00026977\n",
      "Iteraion 20, Train Loss: 0.00026286\n",
      "Iteraion 30, Train Loss: 0.00026286\n",
      "Epoch: 25, RMSE: 17.9818, MAE: 8.1481, MAPE: 96.8597,Train Loss: 0.00019628\n",
      "Epoch: 25, RMSE: 19.3225, MAE: 8.7288, MAPE: 89.5303,Test Loss: 0.00022452\n",
      "=========epoch 26=========\n",
      "Iteraion 10, Train Loss: 0.00027958\n",
      "Iteraion 20, Train Loss: 0.00027973\n",
      "Iteraion 30, Train Loss: 0.00028558\n",
      "Epoch: 26, RMSE: 18.9633, MAE: 8.1837, MAPE: 84.6532,Train Loss: 0.00021832\n",
      "Epoch: 26, RMSE: 20.7347, MAE: 8.9348, MAPE: 78.3736,Test Loss: 0.00025850\n",
      "=========epoch 27=========\n",
      "Iteraion 10, Train Loss: 0.00025801\n",
      "Iteraion 20, Train Loss: 0.00023453\n",
      "Iteraion 30, Train Loss: 0.00024461\n",
      "Epoch: 27, RMSE: 17.8798, MAE: 7.8817, MAPE: 88.7208,Train Loss: 0.00019407\n",
      "Epoch: 27, RMSE: 19.4335, MAE: 8.5312, MAPE: 82.2318,Test Loss: 0.00022710\n",
      "=========epoch 28=========\n",
      "Iteraion 10, Train Loss: 0.00023857\n",
      "Iteraion 20, Train Loss: 0.00028668\n",
      "Iteraion 30, Train Loss: 0.00024068\n",
      "Epoch: 28, RMSE: 17.6923, MAE: 7.7475, MAPE: 84.7202,Train Loss: 0.00019002\n",
      "Epoch: 28, RMSE: 19.1139, MAE: 8.3575, MAPE: 77.6994,Test Loss: 0.00021972\n",
      "=========epoch 29=========\n",
      "Iteraion 10, Train Loss: 0.00024927\n",
      "Iteraion 20, Train Loss: 0.00024965\n",
      "Iteraion 30, Train Loss: 0.00024247\n",
      "Epoch: 29, RMSE: 17.5995, MAE: 7.6924, MAPE: 84.0558,Train Loss: 0.00018800\n",
      "Epoch: 29, RMSE: 18.7106, MAE: 8.1927, MAPE: 77.2027,Test Loss: 0.00021056\n",
      "=========epoch 30=========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraion 10, Train Loss: 0.00021450\n",
      "Iteraion 20, Train Loss: 0.00030373\n",
      "Iteraion 30, Train Loss: 0.00021230\n",
      "Epoch: 30, RMSE: 17.4561, MAE: 7.5751, MAPE: 81.0010,Train Loss: 0.00018499\n",
      "Epoch: 30, RMSE: 18.9425, MAE: 8.1983, MAPE: 73.2547,Test Loss: 0.00021571\n",
      "=========epoch 31=========\n",
      "Iteraion 10, Train Loss: 0.00021055\n",
      "Iteraion 20, Train Loss: 0.00025192\n",
      "Iteraion 30, Train Loss: 0.00023320\n",
      "Epoch: 31, RMSE: 17.1527, MAE: 7.4725, MAPE: 81.4613,Train Loss: 0.00017860\n",
      "Epoch: 31, RMSE: 18.4673, MAE: 8.0267, MAPE: 74.0546,Test Loss: 0.00020507\n",
      "=========epoch 32=========\n",
      "Iteraion 10, Train Loss: 0.00024465\n",
      "Iteraion 20, Train Loss: 0.00024772\n",
      "Iteraion 30, Train Loss: 0.00024626\n",
      "Epoch: 32, RMSE: 17.3690, MAE: 7.3931, MAPE: 77.7912,Train Loss: 0.00018313\n",
      "Epoch: 32, RMSE: 18.8351, MAE: 8.0005, MAPE: 70.5220,Test Loss: 0.00021328\n",
      "=========epoch 33=========\n",
      "Iteraion 10, Train Loss: 0.00028058\n",
      "Iteraion 20, Train Loss: 0.00021222\n",
      "Iteraion 30, Train Loss: 0.00021947\n",
      "Epoch: 33, RMSE: 17.1018, MAE: 7.3500, MAPE: 77.7872,Train Loss: 0.00017753\n",
      "Epoch: 33, RMSE: 18.4012, MAE: 7.8927, MAPE: 70.4108,Test Loss: 0.00020363\n",
      "=========epoch 34=========\n",
      "Iteraion 10, Train Loss: 0.00022895\n",
      "Iteraion 20, Train Loss: 0.00022239\n",
      "Iteraion 30, Train Loss: 0.00022365\n",
      "Epoch: 34, RMSE: 17.1535, MAE: 7.2344, MAPE: 73.2945,Train Loss: 0.00017863\n",
      "Epoch: 34, RMSE: 18.7718, MAE: 7.8936, MAPE: 65.8883,Test Loss: 0.00021182\n",
      "=========epoch 35=========\n",
      "Iteraion 10, Train Loss: 0.00021032\n",
      "Iteraion 20, Train Loss: 0.00022135\n",
      "Iteraion 30, Train Loss: 0.00023381\n",
      "Epoch: 35, RMSE: 17.2157, MAE: 7.4852, MAPE: 81.3823,Train Loss: 0.00017991\n",
      "Epoch: 35, RMSE: 18.0916, MAE: 7.8948, MAPE: 73.9000,Test Loss: 0.00019694\n",
      "=========epoch 36=========\n",
      "Iteraion 10, Train Loss: 0.00021527\n",
      "Iteraion 20, Train Loss: 0.00025428\n",
      "Iteraion 30, Train Loss: 0.00020607\n",
      "Epoch: 36, RMSE: 17.0903, MAE: 7.1142, MAPE: 69.8561,Train Loss: 0.00017732\n",
      "Epoch: 36, RMSE: 18.6697, MAE: 7.7674, MAPE: 63.1298,Test Loss: 0.00020954\n",
      "=========epoch 37=========\n",
      "Iteraion 10, Train Loss: 0.00023246\n",
      "Iteraion 20, Train Loss: 0.00020232\n",
      "Iteraion 30, Train Loss: 0.00020753\n",
      "Epoch: 37, RMSE: 18.2067, MAE: 7.3949, MAPE: 66.2354,Train Loss: 0.00020125\n",
      "Epoch: 37, RMSE: 20.1474, MAE: 8.1675, MAPE: 59.7418,Test Loss: 0.00024402\n",
      "=========epoch 38=========\n",
      "Iteraion 10, Train Loss: 0.00021810\n",
      "Iteraion 20, Train Loss: 0.00020783\n",
      "Iteraion 30, Train Loss: 0.00022898\n",
      "Epoch: 38, RMSE: 16.5828, MAE: 6.9798, MAPE: 71.5213,Train Loss: 0.00016695\n",
      "Epoch: 38, RMSE: 17.7656, MAE: 7.4925, MAPE: 63.7363,Test Loss: 0.00018979\n",
      "=========epoch 39=========\n",
      "Iteraion 10, Train Loss: 0.00023217\n",
      "Iteraion 20, Train Loss: 0.00019196\n",
      "Iteraion 30, Train Loss: 0.00020510\n",
      "Epoch: 39, RMSE: 17.1449, MAE: 7.0793, MAPE: 67.3054,Train Loss: 0.00017847\n",
      "Epoch: 39, RMSE: 18.7244, MAE: 7.7194, MAPE: 59.8045,Test Loss: 0.00021072\n",
      "=========epoch 40=========\n",
      "Iteraion 10, Train Loss: 0.00022239\n",
      "Iteraion 20, Train Loss: 0.00020869\n",
      "Iteraion 30, Train Loss: 0.00020725\n",
      "Epoch: 40, RMSE: 16.6210, MAE: 6.8314, MAPE: 67.2026,Train Loss: 0.00016771\n",
      "Epoch: 40, RMSE: 18.2982, MAE: 7.4881, MAPE: 60.6245,Test Loss: 0.00020124\n",
      "=========epoch 41=========\n",
      "Iteraion 10, Train Loss: 0.00019807\n",
      "Iteraion 20, Train Loss: 0.00020893\n",
      "Iteraion 30, Train Loss: 0.00021816\n",
      "Epoch: 41, RMSE: 16.1816, MAE: 6.7788, MAPE: 68.0856,Train Loss: 0.00015893\n",
      "Epoch: 41, RMSE: 17.3975, MAE: 7.2765, MAPE: 60.8591,Test Loss: 0.00018208\n",
      "=========epoch 42=========\n",
      "Iteraion 10, Train Loss: 0.00020720\n",
      "Iteraion 20, Train Loss: 0.00018064\n",
      "Iteraion 30, Train Loss: 0.00019300\n",
      "Epoch: 42, RMSE: 15.8663, MAE: 6.6442, MAPE: 67.7434,Train Loss: 0.00015283\n",
      "Epoch: 42, RMSE: 17.0201, MAE: 7.1344, MAPE: 60.2096,Test Loss: 0.00017420\n",
      "=========epoch 43=========\n",
      "Iteraion 10, Train Loss: 0.00020422\n",
      "Iteraion 20, Train Loss: 0.00018909\n",
      "Iteraion 30, Train Loss: 0.00019331\n",
      "Epoch: 43, RMSE: 16.1553, MAE: 6.7193, MAPE: 66.6844,Train Loss: 0.00015843\n",
      "Epoch: 43, RMSE: 17.2766, MAE: 7.2068, MAPE: 60.0570,Test Loss: 0.00017954\n",
      "=========epoch 44=========\n",
      "Iteraion 10, Train Loss: 0.00018872\n",
      "Iteraion 20, Train Loss: 0.00019344\n",
      "Iteraion 30, Train Loss: 0.00017427\n",
      "Epoch: 44, RMSE: 16.1169, MAE: 6.6528, MAPE: 65.2339,Train Loss: 0.00015771\n",
      "Epoch: 44, RMSE: 17.3424, MAE: 7.1416, MAPE: 57.2853,Test Loss: 0.00018088\n",
      "=========epoch 45=========\n",
      "Iteraion 10, Train Loss: 0.00017289\n",
      "Iteraion 20, Train Loss: 0.00019416\n",
      "Iteraion 30, Train Loss: 0.00020161\n",
      "Epoch: 45, RMSE: 16.1849, MAE: 6.7494, MAPE: 65.5604,Train Loss: 0.00015902\n",
      "Epoch: 45, RMSE: 17.2667, MAE: 7.2252, MAPE: 58.8254,Test Loss: 0.00017937\n",
      "=========epoch 46=========\n",
      "Iteraion 10, Train Loss: 0.00019398\n",
      "Iteraion 20, Train Loss: 0.00028309\n",
      "Iteraion 30, Train Loss: 0.00021418\n",
      "Epoch: 46, RMSE: 16.1681, MAE: 6.5661, MAPE: 60.4149,Train Loss: 0.00015870\n",
      "Epoch: 46, RMSE: 17.7140, MAE: 7.1856, MAPE: 53.5241,Test Loss: 0.00018870\n",
      "=========epoch 47=========\n",
      "Iteraion 10, Train Loss: 0.00017921\n",
      "Iteraion 20, Train Loss: 0.00021712\n",
      "Iteraion 30, Train Loss: 0.00020510\n",
      "Epoch: 47, RMSE: 16.1963, MAE: 6.5863, MAPE: 61.2447,Train Loss: 0.00015926\n",
      "Epoch: 47, RMSE: 17.8166, MAE: 7.2243, MAPE: 54.5001,Test Loss: 0.00019082\n",
      "=========epoch 48=========\n",
      "Iteraion 10, Train Loss: 0.00020025\n",
      "Iteraion 20, Train Loss: 0.00017261\n",
      "Iteraion 30, Train Loss: 0.00017344\n",
      "Epoch: 48, RMSE: 15.3167, MAE: 6.3356, MAPE: 62.4657,Train Loss: 0.00014241\n",
      "Epoch: 48, RMSE: 16.5176, MAE: 6.8377, MAPE: 56.0782,Test Loss: 0.00016407\n",
      "=========epoch 49=========\n",
      "Iteraion 10, Train Loss: 0.00019909\n",
      "Iteraion 20, Train Loss: 0.00019831\n",
      "Iteraion 30, Train Loss: 0.00019534\n",
      "Epoch: 49, RMSE: 16.1934, MAE: 6.7123, MAPE: 63.8629,Train Loss: 0.00015917\n",
      "Epoch: 49, RMSE: 17.1866, MAE: 7.1660, MAPE: 57.0871,Test Loss: 0.00017773\n",
      "=========epoch 50=========\n",
      "Iteraion 10, Train Loss: 0.00020124\n",
      "Iteraion 20, Train Loss: 0.00019285\n",
      "Iteraion 30, Train Loss: 0.00016993\n",
      "Epoch: 50, RMSE: 15.2934, MAE: 6.2593, MAPE: 59.9678,Train Loss: 0.00014198\n",
      "Epoch: 50, RMSE: 16.4848, MAE: 6.7597, MAPE: 53.8337,Test Loss: 0.00016342\n",
      "=========epoch 51=========\n",
      "Iteraion 10, Train Loss: 0.00017602\n",
      "Iteraion 20, Train Loss: 0.00016699\n",
      "Iteraion 30, Train Loss: 0.00019774\n",
      "Epoch: 51, RMSE: 15.8297, MAE: 6.3327, MAPE: 55.6761,Train Loss: 0.00015213\n",
      "Epoch: 51, RMSE: 17.4208, MAE: 6.9649, MAPE: 50.1586,Test Loss: 0.00018247\n",
      "=========epoch 52=========\n",
      "Iteraion 10, Train Loss: 0.00019044\n",
      "Iteraion 20, Train Loss: 0.00018526\n",
      "Iteraion 30, Train Loss: 0.00020624\n",
      "Epoch: 52, RMSE: 15.1703, MAE: 6.1953, MAPE: 58.5950,Train Loss: 0.00013971\n",
      "Epoch: 52, RMSE: 16.4839, MAE: 6.7298, MAPE: 53.1315,Test Loss: 0.00016337\n",
      "=========epoch 53=========\n",
      "Iteraion 10, Train Loss: 0.00018394\n",
      "Iteraion 20, Train Loss: 0.00020134\n",
      "Iteraion 30, Train Loss: 0.00018503\n",
      "Epoch: 53, RMSE: 15.0100, MAE: 6.1797, MAPE: 59.0982,Train Loss: 0.00013678\n",
      "Epoch: 53, RMSE: 16.3021, MAE: 6.7137, MAPE: 52.7515,Test Loss: 0.00015987\n",
      "=========epoch 54=========\n",
      "Iteraion 10, Train Loss: 0.00020197\n",
      "Iteraion 20, Train Loss: 0.00017286\n",
      "Iteraion 30, Train Loss: 0.00018118\n",
      "Epoch: 54, RMSE: 14.9004, MAE: 6.0862, MAPE: 57.9956,Train Loss: 0.00013479\n",
      "Epoch: 54, RMSE: 16.2826, MAE: 6.6454, MAPE: 52.4481,Test Loss: 0.00015944\n",
      "=========epoch 55=========\n",
      "Iteraion 10, Train Loss: 0.00017026\n",
      "Iteraion 20, Train Loss: 0.00018082\n",
      "Iteraion 30, Train Loss: 0.00019331\n",
      "Epoch: 55, RMSE: 14.9071, MAE: 6.0760, MAPE: 56.4668,Train Loss: 0.00013491\n",
      "Epoch: 55, RMSE: 16.2573, MAE: 6.6426, MAPE: 50.8526,Test Loss: 0.00015895\n",
      "=========epoch 56=========\n",
      "Iteraion 10, Train Loss: 0.00016302\n",
      "Iteraion 20, Train Loss: 0.00017609\n",
      "Iteraion 30, Train Loss: 0.00018863\n",
      "Epoch: 56, RMSE: 14.9364, MAE: 6.2118, MAPE: 59.9603,Train Loss: 0.00013544\n",
      "Epoch: 56, RMSE: 16.1769, MAE: 6.7066, MAPE: 54.1799,Test Loss: 0.00015746\n",
      "=========epoch 57=========\n",
      "Iteraion 10, Train Loss: 0.00020805\n",
      "Iteraion 20, Train Loss: 0.00018070\n",
      "Iteraion 30, Train Loss: 0.00016851\n",
      "Epoch: 57, RMSE: 16.2817, MAE: 6.3693, MAPE: 52.4440,Train Loss: 0.00016096\n",
      "Epoch: 57, RMSE: 17.9866, MAE: 7.0601, MAPE: 47.6113,Test Loss: 0.00019457\n",
      "=========epoch 58=========\n",
      "Iteraion 10, Train Loss: 0.00016786\n",
      "Iteraion 20, Train Loss: 0.00020729\n",
      "Iteraion 30, Train Loss: 0.00014642\n",
      "Epoch: 58, RMSE: 14.7059, MAE: 5.9876, MAPE: 55.2282,Train Loss: 0.00013129\n",
      "Epoch: 58, RMSE: 15.9405, MAE: 6.5148, MAPE: 50.1291,Test Loss: 0.00015284\n",
      "=========epoch 59=========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraion 10, Train Loss: 0.00016840\n",
      "Iteraion 20, Train Loss: 0.00016159\n",
      "Iteraion 30, Train Loss: 0.00016094\n",
      "Epoch: 59, RMSE: 15.2086, MAE: 6.0651, MAPE: 53.2908,Train Loss: 0.00014043\n",
      "Epoch: 59, RMSE: 16.9094, MAE: 6.7366, MAPE: 48.8441,Test Loss: 0.00017193\n",
      "=========epoch 60=========\n",
      "Iteraion 10, Train Loss: 0.00019271\n",
      "Iteraion 20, Train Loss: 0.00016942\n",
      "Iteraion 30, Train Loss: 0.00017403\n",
      "Epoch: 60, RMSE: 14.6801, MAE: 5.9626, MAPE: 54.2663,Train Loss: 0.00013083\n",
      "Epoch: 60, RMSE: 16.1154, MAE: 6.5515, MAPE: 49.7937,Test Loss: 0.00015621\n",
      "=========epoch 61=========\n",
      "Iteraion 10, Train Loss: 0.00018254\n",
      "Iteraion 20, Train Loss: 0.00015375\n",
      "Iteraion 30, Train Loss: 0.00016058\n",
      "Epoch: 61, RMSE: 14.8338, MAE: 5.9393, MAPE: 52.4137,Train Loss: 0.00013359\n",
      "Epoch: 61, RMSE: 16.5570, MAE: 6.6096, MAPE: 47.9174,Test Loss: 0.00016485\n",
      "=========epoch 62=========\n",
      "Iteraion 10, Train Loss: 0.00017862\n",
      "Iteraion 20, Train Loss: 0.00015578\n",
      "Iteraion 30, Train Loss: 0.00015469\n",
      "Epoch: 62, RMSE: 14.5506, MAE: 5.9274, MAPE: 54.4293,Train Loss: 0.00012851\n",
      "Epoch: 62, RMSE: 16.0681, MAE: 6.5308, MAPE: 50.2060,Test Loss: 0.00015533\n",
      "=========epoch 63=========\n",
      "Iteraion 10, Train Loss: 0.00013504\n",
      "Iteraion 20, Train Loss: 0.00013509\n",
      "Iteraion 30, Train Loss: 0.00017864\n",
      "Epoch: 63, RMSE: 14.4586, MAE: 5.9402, MAPE: 55.6744,Train Loss: 0.00012690\n",
      "Epoch: 63, RMSE: 15.5775, MAE: 6.3959, MAPE: 50.7464,Test Loss: 0.00014604\n",
      "=========epoch 64=========\n",
      "Iteraion 10, Train Loss: 0.00018538\n",
      "Iteraion 20, Train Loss: 0.00015649\n",
      "Iteraion 30, Train Loss: 0.00015538\n",
      "Epoch: 64, RMSE: 14.4764, MAE: 5.7824, MAPE: 50.7389,Train Loss: 0.00012721\n",
      "Epoch: 64, RMSE: 16.1722, MAE: 6.4397, MAPE: 46.3932,Test Loss: 0.00015725\n",
      "=========epoch 65=========\n",
      "Iteraion 10, Train Loss: 0.00014898\n",
      "Iteraion 20, Train Loss: 0.00016239\n",
      "Iteraion 30, Train Loss: 0.00013252\n",
      "Epoch: 65, RMSE: 15.0774, MAE: 6.2047, MAPE: 56.9921,Train Loss: 0.00013800\n",
      "Epoch: 65, RMSE: 16.1804, MAE: 6.6168, MAPE: 51.2985,Test Loss: 0.00015761\n",
      "=========epoch 66=========\n",
      "Iteraion 10, Train Loss: 0.00013594\n",
      "Iteraion 20, Train Loss: 0.00019452\n",
      "Iteraion 30, Train Loss: 0.00016370\n",
      "Epoch: 66, RMSE: 14.3390, MAE: 5.7798, MAPE: 51.1818,Train Loss: 0.00012482\n",
      "Epoch: 66, RMSE: 15.7639, MAE: 6.3309, MAPE: 46.5908,Test Loss: 0.00014948\n",
      "=========epoch 67=========\n",
      "Iteraion 10, Train Loss: 0.00015310\n",
      "Iteraion 20, Train Loss: 0.00017207\n",
      "Iteraion 30, Train Loss: 0.00015749\n",
      "Epoch: 67, RMSE: 14.3013, MAE: 5.7795, MAPE: 51.6525,Train Loss: 0.00012415\n",
      "Epoch: 67, RMSE: 15.5504, MAE: 6.3040, MAPE: 47.5715,Test Loss: 0.00014553\n",
      "=========epoch 68=========\n",
      "Iteraion 10, Train Loss: 0.00015101\n",
      "Iteraion 20, Train Loss: 0.00015175\n",
      "Iteraion 30, Train Loss: 0.00011857\n",
      "Epoch: 68, RMSE: 14.2790, MAE: 5.7023, MAPE: 49.2068,Train Loss: 0.00012376\n",
      "Epoch: 68, RMSE: 15.8435, MAE: 6.3256, MAPE: 45.1449,Test Loss: 0.00015101\n",
      "=========epoch 69=========\n",
      "Iteraion 10, Train Loss: 0.00015605\n",
      "Iteraion 20, Train Loss: 0.00015981\n",
      "Iteraion 30, Train Loss: 0.00014788\n",
      "Epoch: 69, RMSE: 13.9620, MAE: 5.6661, MAPE: 50.9977,Train Loss: 0.00011833\n",
      "Epoch: 69, RMSE: 15.2750, MAE: 6.1908, MAPE: 46.5800,Test Loss: 0.00014039\n",
      "=========epoch 70=========\n",
      "Iteraion 10, Train Loss: 0.00016026\n",
      "Iteraion 20, Train Loss: 0.00017965\n",
      "Iteraion 30, Train Loss: 0.00018872\n",
      "Epoch: 70, RMSE: 14.1629, MAE: 5.7722, MAPE: 53.0351,Train Loss: 0.00012179\n",
      "Epoch: 70, RMSE: 15.6996, MAE: 6.3283, MAPE: 48.6484,Test Loss: 0.00014827\n",
      "=========epoch 71=========\n",
      "Iteraion 10, Train Loss: 0.00016820\n",
      "Iteraion 20, Train Loss: 0.00016052\n",
      "Iteraion 30, Train Loss: 0.00023325\n",
      "Epoch: 71, RMSE: 14.0034, MAE: 5.7017, MAPE: 51.4024,Train Loss: 0.00011905\n",
      "Epoch: 71, RMSE: 15.5838, MAE: 6.3114, MAPE: 46.9629,Test Loss: 0.00014609\n",
      "=========epoch 72=========\n",
      "Iteraion 10, Train Loss: 0.00015678\n",
      "Iteraion 20, Train Loss: 0.00015778\n",
      "Iteraion 30, Train Loss: 0.00014401\n",
      "Epoch: 72, RMSE: 14.0380, MAE: 5.6116, MAPE: 49.8204,Train Loss: 0.00011965\n",
      "Epoch: 72, RMSE: 15.7300, MAE: 6.2562, MAPE: 46.3502,Test Loss: 0.00014880\n",
      "=========epoch 73=========\n",
      "Iteraion 10, Train Loss: 0.00019098\n",
      "Iteraion 20, Train Loss: 0.00014325\n",
      "Iteraion 30, Train Loss: 0.00016992\n",
      "Epoch: 73, RMSE: 13.7775, MAE: 5.6309, MAPE: 51.7567,Train Loss: 0.00011523\n",
      "Epoch: 73, RMSE: 15.3782, MAE: 6.2223, MAPE: 47.8736,Test Loss: 0.00014230\n",
      "=========epoch 74=========\n",
      "Iteraion 10, Train Loss: 0.00014396\n",
      "Iteraion 20, Train Loss: 0.00017076\n",
      "Iteraion 30, Train Loss: 0.00015613\n",
      "Epoch: 74, RMSE: 13.9755, MAE: 5.6441, MAPE: 50.3292,Train Loss: 0.00011856\n",
      "Epoch: 74, RMSE: 15.2674, MAE: 6.1693, MAPE: 46.5274,Test Loss: 0.00014025\n",
      "=========epoch 75=========\n",
      "Iteraion 10, Train Loss: 0.00013033\n",
      "Iteraion 20, Train Loss: 0.00014435\n",
      "Iteraion 30, Train Loss: 0.00013136\n",
      "Epoch: 75, RMSE: 13.8658, MAE: 5.5626, MAPE: 49.2389,Train Loss: 0.00011672\n",
      "Epoch: 75, RMSE: 15.5991, MAE: 6.2141, MAPE: 45.2537,Test Loss: 0.00014633\n",
      "=========epoch 76=========\n",
      "Iteraion 10, Train Loss: 0.00016406\n",
      "Iteraion 20, Train Loss: 0.00018338\n",
      "Iteraion 30, Train Loss: 0.00015211\n",
      "Epoch: 76, RMSE: 14.3252, MAE: 5.6806, MAPE: 47.0672,Train Loss: 0.00012458\n",
      "Epoch: 76, RMSE: 16.1805, MAE: 6.3961, MAPE: 43.6345,Test Loss: 0.00015748\n",
      "=========epoch 77=========\n",
      "Iteraion 10, Train Loss: 0.00014253\n",
      "Iteraion 20, Train Loss: 0.00015895\n",
      "Iteraion 30, Train Loss: 0.00014432\n",
      "Epoch: 77, RMSE: 13.7597, MAE: 5.5437, MAPE: 49.3928,Train Loss: 0.00011494\n",
      "Epoch: 77, RMSE: 15.3176, MAE: 6.1430, MAPE: 45.1983,Test Loss: 0.00014117\n",
      "=========epoch 78=========\n",
      "Iteraion 10, Train Loss: 0.00016528\n",
      "Iteraion 20, Train Loss: 0.00014502\n",
      "Iteraion 30, Train Loss: 0.00014076\n",
      "Epoch: 78, RMSE: 14.2100, MAE: 5.6066, MAPE: 46.1123,Train Loss: 0.00012259\n",
      "Epoch: 78, RMSE: 16.0737, MAE: 6.3302, MAPE: 43.0023,Test Loss: 0.00015536\n",
      "=========epoch 79=========\n",
      "Iteraion 10, Train Loss: 0.00013811\n",
      "Iteraion 20, Train Loss: 0.00013399\n",
      "Iteraion 30, Train Loss: 0.00017452\n",
      "Epoch: 79, RMSE: 13.6797, MAE: 5.5739, MAPE: 50.4753,Train Loss: 0.00011360\n",
      "Epoch: 79, RMSE: 15.0231, MAE: 6.0783, MAPE: 46.3489,Test Loss: 0.00013582\n",
      "=========epoch 80=========\n",
      "Iteraion 10, Train Loss: 0.00014833\n",
      "Iteraion 20, Train Loss: 0.00015316\n",
      "Iteraion 30, Train Loss: 0.00020098\n",
      "Epoch: 80, RMSE: 13.7344, MAE: 5.5570, MAPE: 50.8250,Train Loss: 0.00011450\n",
      "Epoch: 80, RMSE: 15.1092, MAE: 6.0697, MAPE: 46.9056,Test Loss: 0.00013737\n",
      "=========epoch 81=========\n",
      "Iteraion 10, Train Loss: 0.00013924\n",
      "Iteraion 20, Train Loss: 0.00016286\n",
      "Iteraion 30, Train Loss: 0.00016923\n",
      "Epoch: 81, RMSE: 13.5374, MAE: 5.3956, MAPE: 46.4552,Train Loss: 0.00011126\n",
      "Epoch: 81, RMSE: 15.2565, MAE: 6.0418, MAPE: 43.2263,Test Loss: 0.00014000\n",
      "=========epoch 82=========\n",
      "Iteraion 10, Train Loss: 0.00015271\n",
      "Iteraion 20, Train Loss: 0.00014302\n",
      "Iteraion 30, Train Loss: 0.00015055\n",
      "Epoch: 82, RMSE: 14.1796, MAE: 5.8486, MAPE: 52.3140,Train Loss: 0.00012205\n",
      "Epoch: 82, RMSE: 15.2974, MAE: 6.2850, MAPE: 47.7818,Test Loss: 0.00014088\n",
      "=========epoch 83=========\n",
      "Iteraion 10, Train Loss: 0.00016745\n",
      "Iteraion 20, Train Loss: 0.00014709\n",
      "Iteraion 30, Train Loss: 0.00016015\n",
      "Epoch: 83, RMSE: 13.4946, MAE: 5.3770, MAPE: 46.7312,Train Loss: 0.00011055\n",
      "Epoch: 83, RMSE: 15.2182, MAE: 6.0221, MAPE: 43.3911,Test Loss: 0.00013930\n",
      "=========epoch 84=========\n",
      "Iteraion 10, Train Loss: 0.00014743\n",
      "Iteraion 20, Train Loss: 0.00013546\n",
      "Iteraion 30, Train Loss: 0.00016144\n",
      "Epoch: 84, RMSE: 13.4657, MAE: 5.3892, MAPE: 47.4146,Train Loss: 0.00011007\n",
      "Epoch: 84, RMSE: 15.1733, MAE: 6.0118, MAPE: 43.8488,Test Loss: 0.00013848\n",
      "=========epoch 85=========\n",
      "Iteraion 10, Train Loss: 0.00012308\n",
      "Iteraion 20, Train Loss: 0.00016310\n",
      "Iteraion 30, Train Loss: 0.00013238\n",
      "Epoch: 85, RMSE: 13.3844, MAE: 5.4023, MAPE: 48.2731,Train Loss: 0.00010876\n",
      "Epoch: 85, RMSE: 15.0968, MAE: 6.0244, MAPE: 44.6118,Test Loss: 0.00013714\n",
      "=========epoch 86=========\n",
      "Iteraion 10, Train Loss: 0.00017328\n",
      "Iteraion 20, Train Loss: 0.00012887\n",
      "Iteraion 30, Train Loss: 0.00012958\n",
      "Epoch: 86, RMSE: 13.9563, MAE: 5.4632, MAPE: 45.4927,Train Loss: 0.00011825\n",
      "Epoch: 86, RMSE: 15.6340, MAE: 6.1332, MAPE: 42.1292,Test Loss: 0.00014700\n",
      "=========epoch 87=========\n",
      "Iteraion 10, Train Loss: 0.00013993\n",
      "Iteraion 20, Train Loss: 0.00013125\n",
      "Iteraion 30, Train Loss: 0.00014373\n",
      "Epoch: 87, RMSE: 13.2723, MAE: 5.3053, MAPE: 46.6417,Train Loss: 0.00010694\n",
      "Epoch: 87, RMSE: 14.9639, MAE: 5.9352, MAPE: 43.4055,Test Loss: 0.00013471\n",
      "=========epoch 88=========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraion 10, Train Loss: 0.00015149\n",
      "Iteraion 20, Train Loss: 0.00013540\n",
      "Iteraion 30, Train Loss: 0.00014653\n",
      "Epoch: 88, RMSE: 13.8340, MAE: 5.4762, MAPE: 46.2788,Train Loss: 0.00011618\n",
      "Epoch: 88, RMSE: 15.5523, MAE: 6.1518, MAPE: 42.5406,Test Loss: 0.00014546\n",
      "=========epoch 89=========\n",
      "Iteraion 10, Train Loss: 0.00016183\n",
      "Iteraion 20, Train Loss: 0.00017166\n",
      "Iteraion 30, Train Loss: 0.00015017\n",
      "Epoch: 89, RMSE: 13.4407, MAE: 5.5102, MAPE: 50.3946,Train Loss: 0.00010967\n",
      "Epoch: 89, RMSE: 15.0947, MAE: 6.0989, MAPE: 46.8804,Test Loss: 0.00013708\n",
      "=========epoch 90=========\n",
      "Iteraion 10, Train Loss: 0.00013455\n",
      "Iteraion 20, Train Loss: 0.00013761\n",
      "Iteraion 30, Train Loss: 0.00013687\n",
      "Epoch: 90, RMSE: 14.5640, MAE: 5.5974, MAPE: 44.4937,Train Loss: 0.00012877\n",
      "Epoch: 90, RMSE: 16.4438, MAE: 6.3432, MAPE: 41.3676,Test Loss: 0.00016257\n",
      "=========epoch 91=========\n",
      "Iteraion 10, Train Loss: 0.00015411\n",
      "Iteraion 20, Train Loss: 0.00013304\n",
      "Iteraion 30, Train Loss: 0.00016927\n",
      "Epoch: 91, RMSE: 13.5965, MAE: 5.5268, MAPE: 49.9677,Train Loss: 0.00011223\n",
      "Epoch: 91, RMSE: 14.9414, MAE: 6.0132, MAPE: 46.0542,Test Loss: 0.00013441\n",
      "=========epoch 92=========\n",
      "Iteraion 10, Train Loss: 0.00013791\n",
      "Iteraion 20, Train Loss: 0.00016705\n",
      "Iteraion 30, Train Loss: 0.00013345\n",
      "Epoch: 92, RMSE: 13.1598, MAE: 5.2896, MAPE: 47.6715,Train Loss: 0.00010513\n",
      "Epoch: 92, RMSE: 14.6017, MAE: 5.8438, MAPE: 44.1584,Test Loss: 0.00012827\n",
      "=========epoch 93=========\n",
      "Iteraion 10, Train Loss: 0.00013130\n",
      "Iteraion 20, Train Loss: 0.00013872\n",
      "Iteraion 30, Train Loss: 0.00014015\n",
      "Epoch: 93, RMSE: 13.8495, MAE: 5.4162, MAPE: 45.2272,Train Loss: 0.00011645\n",
      "Epoch: 93, RMSE: 15.3852, MAE: 6.0507, MAPE: 41.8149,Test Loss: 0.00014236\n",
      "=========epoch 94=========\n",
      "Iteraion 10, Train Loss: 0.00015908\n",
      "Iteraion 20, Train Loss: 0.00014599\n",
      "Iteraion 30, Train Loss: 0.00013169\n",
      "Epoch: 94, RMSE: 13.1435, MAE: 5.2634, MAPE: 46.9726,Train Loss: 0.00010486\n",
      "Epoch: 94, RMSE: 14.7594, MAE: 5.8704, MAPE: 43.4552,Test Loss: 0.00013106\n",
      "=========epoch 95=========\n",
      "Iteraion 10, Train Loss: 0.00013817\n",
      "Iteraion 20, Train Loss: 0.00013925\n",
      "Iteraion 30, Train Loss: 0.00013317\n",
      "Epoch: 95, RMSE: 14.6027, MAE: 5.6313, MAPE: 42.5406,Train Loss: 0.00012946\n",
      "Epoch: 95, RMSE: 16.5110, MAE: 6.3903, MAPE: 39.9098,Test Loss: 0.00016397\n",
      "=========epoch 96=========\n",
      "Iteraion 10, Train Loss: 0.00011980\n",
      "Iteraion 20, Train Loss: 0.00012027\n",
      "Iteraion 30, Train Loss: 0.00014454\n",
      "Epoch: 96, RMSE: 13.0564, MAE: 5.2814, MAPE: 46.9860,Train Loss: 0.00010349\n",
      "Epoch: 96, RMSE: 14.5870, MAE: 5.8429, MAPE: 43.5056,Test Loss: 0.00012806\n",
      "=========epoch 97=========\n",
      "Iteraion 10, Train Loss: 0.00012377\n",
      "Iteraion 20, Train Loss: 0.00010960\n",
      "Iteraion 30, Train Loss: 0.00012502\n",
      "Epoch: 97, RMSE: 13.1363, MAE: 5.2611, MAPE: 45.9081,Train Loss: 0.00010474\n",
      "Epoch: 97, RMSE: 14.8603, MAE: 5.9022, MAPE: 42.8209,Test Loss: 0.00013286\n",
      "=========epoch 98=========\n",
      "Iteraion 10, Train Loss: 0.00013909\n",
      "Iteraion 20, Train Loss: 0.00013882\n",
      "Iteraion 30, Train Loss: 0.00013501\n",
      "Epoch: 98, RMSE: 13.0708, MAE: 5.2263, MAPE: 45.3066,Train Loss: 0.00010373\n",
      "Epoch: 98, RMSE: 14.5882, MAE: 5.7945, MAPE: 41.9115,Test Loss: 0.00012802\n",
      "=========epoch 99=========\n",
      "Iteraion 10, Train Loss: 0.00016088\n",
      "Iteraion 20, Train Loss: 0.00011903\n",
      "Iteraion 30, Train Loss: 0.00011831\n",
      "Epoch: 99, RMSE: 13.3872, MAE: 5.2119, MAPE: 42.4351,Train Loss: 0.00010881\n",
      "Epoch: 99, RMSE: 15.2581, MAE: 5.9152, MAPE: 39.7767,Test Loss: 0.00014002\n",
      "=========epoch 100=========\n",
      "Iteraion 10, Train Loss: 0.00015215\n",
      "Iteraion 20, Train Loss: 0.00015071\n",
      "Iteraion 30, Train Loss: 0.00013341\n",
      "Epoch: 100, RMSE: 13.2754, MAE: 5.2438, MAPE: 43.5562,Train Loss: 0.00010699\n",
      "Epoch: 100, RMSE: 15.0459, MAE: 5.9189, MAPE: 40.6151,Test Loss: 0.00013616\n",
      "=========epoch 101=========\n",
      "Iteraion 10, Train Loss: 0.00011729\n",
      "Iteraion 20, Train Loss: 0.00014997\n",
      "Iteraion 30, Train Loss: 0.00010536\n",
      "Epoch: 101, RMSE: 13.0612, MAE: 5.1700, MAPE: 44.0486,Train Loss: 0.00010358\n",
      "Epoch: 101, RMSE: 14.9680, MAE: 5.8861, MAPE: 41.0854,Test Loss: 0.00013477\n",
      "=========epoch 102=========\n",
      "Iteraion 10, Train Loss: 0.00015393\n",
      "Iteraion 20, Train Loss: 0.00016044\n",
      "Iteraion 30, Train Loss: 0.00013430\n",
      "Epoch: 102, RMSE: 13.2351, MAE: 5.2159, MAPE: 42.8205,Train Loss: 0.00010636\n",
      "Epoch: 102, RMSE: 14.9677, MAE: 5.8847, MAPE: 40.1277,Test Loss: 0.00013474\n",
      "=========epoch 103=========\n",
      "Iteraion 10, Train Loss: 0.00013280\n",
      "Iteraion 20, Train Loss: 0.00012317\n",
      "Iteraion 30, Train Loss: 0.00014189\n",
      "Epoch: 103, RMSE: 14.5612, MAE: 5.6309, MAPE: 41.4643,Train Loss: 0.00012876\n",
      "Epoch: 103, RMSE: 16.4861, MAE: 6.4052, MAPE: 39.1730,Test Loss: 0.00016340\n",
      "=========epoch 104=========\n",
      "Iteraion 10, Train Loss: 0.00013202\n",
      "Iteraion 20, Train Loss: 0.00015140\n",
      "Iteraion 30, Train Loss: 0.00013128\n",
      "Epoch: 104, RMSE: 12.7713, MAE: 5.0965, MAPE: 44.2386,Train Loss: 0.00009902\n",
      "Epoch: 104, RMSE: 14.6333, MAE: 5.7402, MAPE: 41.2913,Test Loss: 0.00012883\n",
      "=========epoch 105=========\n",
      "Iteraion 10, Train Loss: 0.00012323\n",
      "Iteraion 20, Train Loss: 0.00015746\n",
      "Iteraion 30, Train Loss: 0.00012402\n",
      "Epoch: 105, RMSE: 12.7529, MAE: 5.0818, MAPE: 44.2840,Train Loss: 0.00009874\n",
      "Epoch: 105, RMSE: 14.6089, MAE: 5.7615, MAPE: 41.3386,Test Loss: 0.00012835\n",
      "=========epoch 106=========\n",
      "Iteraion 10, Train Loss: 0.00012850\n",
      "Iteraion 20, Train Loss: 0.00013879\n",
      "Iteraion 30, Train Loss: 0.00015408\n",
      "Epoch: 106, RMSE: 13.0407, MAE: 5.0936, MAPE: 42.0170,Train Loss: 0.00010324\n",
      "Epoch: 106, RMSE: 14.8871, MAE: 5.7817, MAPE: 39.1888,Test Loss: 0.00013330\n",
      "=========epoch 107=========\n",
      "Iteraion 10, Train Loss: 0.00013854\n",
      "Iteraion 20, Train Loss: 0.00013885\n",
      "Iteraion 30, Train Loss: 0.00013677\n",
      "Epoch: 107, RMSE: 12.6330, MAE: 5.0369, MAPE: 43.0599,Train Loss: 0.00009689\n",
      "Epoch: 107, RMSE: 14.4616, MAE: 5.7093, MAPE: 40.3011,Test Loss: 0.00012581\n",
      "=========epoch 108=========\n",
      "Iteraion 10, Train Loss: 0.00012705\n",
      "Iteraion 20, Train Loss: 0.00012729\n",
      "Iteraion 30, Train Loss: 0.00013508\n",
      "Epoch: 108, RMSE: 13.9768, MAE: 5.5552, MAPE: 47.7255,Train Loss: 0.00011860\n",
      "Epoch: 108, RMSE: 15.5842, MAE: 6.1103, MAPE: 44.2601,Test Loss: 0.00014623\n",
      "=========epoch 109=========\n",
      "Iteraion 10, Train Loss: 0.00012716\n",
      "Iteraion 20, Train Loss: 0.00015039\n",
      "Iteraion 30, Train Loss: 0.00018290\n",
      "Epoch: 109, RMSE: 13.3226, MAE: 5.2032, MAPE: 41.3186,Train Loss: 0.00010777\n",
      "Epoch: 109, RMSE: 15.4553, MAE: 5.9999, MAPE: 38.6632,Test Loss: 0.00014363\n",
      "=========epoch 110=========\n",
      "Iteraion 10, Train Loss: 0.00012409\n",
      "Iteraion 20, Train Loss: 0.00012877\n",
      "Iteraion 30, Train Loss: 0.00016201\n",
      "Epoch: 110, RMSE: 13.0265, MAE: 5.1336, MAPE: 42.8513,Train Loss: 0.00010302\n",
      "Epoch: 110, RMSE: 15.1194, MAE: 5.8713, MAPE: 39.8448,Test Loss: 0.00013749\n",
      "=========epoch 111=========\n",
      "Iteraion 10, Train Loss: 0.00014401\n",
      "Iteraion 20, Train Loss: 0.00013991\n",
      "Iteraion 30, Train Loss: 0.00012312\n",
      "Epoch: 111, RMSE: 12.7711, MAE: 5.0544, MAPE: 43.3237,Train Loss: 0.00009903\n",
      "Epoch: 111, RMSE: 14.4207, MAE: 5.6574, MAPE: 40.4686,Test Loss: 0.00012514\n",
      "=========epoch 112=========\n",
      "Iteraion 10, Train Loss: 0.00014606\n",
      "Iteraion 20, Train Loss: 0.00012548\n",
      "Iteraion 30, Train Loss: 0.00012111\n",
      "Epoch: 112, RMSE: 14.2251, MAE: 5.4998, MAPE: 41.0899,Train Loss: 0.00012286\n",
      "Epoch: 112, RMSE: 16.1037, MAE: 6.2472, MAPE: 38.7394,Test Loss: 0.00015594\n",
      "=========epoch 113=========\n",
      "Iteraion 10, Train Loss: 0.00011374\n",
      "Iteraion 20, Train Loss: 0.00014954\n",
      "Iteraion 30, Train Loss: 0.00014956\n",
      "Epoch: 113, RMSE: 13.1514, MAE: 5.1374, MAPE: 43.4339,Train Loss: 0.00010503\n",
      "Epoch: 113, RMSE: 15.1951, MAE: 5.8758, MAPE: 40.7751,Test Loss: 0.00013886\n",
      "=========epoch 114=========\n",
      "Iteraion 10, Train Loss: 0.00015240\n",
      "Iteraion 20, Train Loss: 0.00013627\n",
      "Iteraion 30, Train Loss: 0.00014229\n",
      "Epoch: 114, RMSE: 12.6568, MAE: 5.0269, MAPE: 43.3889,Train Loss: 0.00009726\n",
      "Epoch: 114, RMSE: 14.7862, MAE: 5.7826, MAPE: 40.8178,Test Loss: 0.00013148\n",
      "=========epoch 115=========\n",
      "Iteraion 10, Train Loss: 0.00013862\n",
      "Iteraion 20, Train Loss: 0.00012423\n",
      "Iteraion 30, Train Loss: 0.00013558\n",
      "Epoch: 115, RMSE: 12.4276, MAE: 4.9616, MAPE: 43.4226,Train Loss: 0.00009378\n",
      "Epoch: 115, RMSE: 14.2213, MAE: 5.6057, MAPE: 40.5508,Test Loss: 0.00012167\n",
      "=========epoch 116=========\n",
      "Iteraion 10, Train Loss: 0.00013878\n",
      "Iteraion 20, Train Loss: 0.00012465\n",
      "Iteraion 30, Train Loss: 0.00010830\n",
      "Epoch: 116, RMSE: 12.6051, MAE: 4.9895, MAPE: 42.1914,Train Loss: 0.00009647\n",
      "Epoch: 116, RMSE: 14.5910, MAE: 5.7163, MAPE: 39.6270,Test Loss: 0.00012804\n",
      "=========epoch 117=========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraion 10, Train Loss: 0.00011515\n",
      "Iteraion 20, Train Loss: 0.00013518\n",
      "Iteraion 30, Train Loss: 0.00012115\n",
      "Epoch: 117, RMSE: 12.5995, MAE: 5.1101, MAPE: 46.4151,Train Loss: 0.00009639\n",
      "Epoch: 117, RMSE: 14.4989, MAE: 5.7225, MAPE: 43.1103,Test Loss: 0.00012652\n",
      "=========epoch 118=========\n",
      "Iteraion 10, Train Loss: 0.00013888\n",
      "Iteraion 20, Train Loss: 0.00011494\n",
      "Iteraion 30, Train Loss: 0.00013293\n",
      "Epoch: 118, RMSE: 12.4743, MAE: 4.9204, MAPE: 41.8008,Train Loss: 0.00009447\n",
      "Epoch: 118, RMSE: 14.4045, MAE: 5.6195, MAPE: 39.3290,Test Loss: 0.00012482\n",
      "=========epoch 119=========\n",
      "Iteraion 10, Train Loss: 0.00013789\n",
      "Iteraion 20, Train Loss: 0.00012650\n",
      "Iteraion 30, Train Loss: 0.00013169\n",
      "Epoch: 119, RMSE: 12.3456, MAE: 4.9474, MAPE: 43.5532,Train Loss: 0.00009253\n",
      "Epoch: 119, RMSE: 14.2106, MAE: 5.6191, MAPE: 40.4990,Test Loss: 0.00012150\n",
      "=========epoch 120=========\n",
      "Iteraion 10, Train Loss: 0.00011588\n",
      "Iteraion 20, Train Loss: 0.00014651\n",
      "Iteraion 30, Train Loss: 0.00012693\n",
      "Epoch: 120, RMSE: 12.6733, MAE: 5.1232, MAPE: 44.9208,Train Loss: 0.00009751\n",
      "Epoch: 120, RMSE: 14.4057, MAE: 5.7249, MAPE: 41.9800,Test Loss: 0.00012492\n",
      "=========epoch 121=========\n",
      "Iteraion 10, Train Loss: 0.00012404\n",
      "Iteraion 20, Train Loss: 0.00013018\n",
      "Iteraion 30, Train Loss: 0.00012723\n",
      "Epoch: 121, RMSE: 12.4018, MAE: 4.9111, MAPE: 42.5067,Train Loss: 0.00009339\n",
      "Epoch: 121, RMSE: 14.5325, MAE: 5.6508, MAPE: 39.9663,Test Loss: 0.00012706\n",
      "=========epoch 122=========\n",
      "Iteraion 10, Train Loss: 0.00012023\n",
      "Iteraion 20, Train Loss: 0.00013341\n",
      "Iteraion 30, Train Loss: 0.00011748\n",
      "Epoch: 122, RMSE: 12.0952, MAE: 4.8430, MAPE: 43.1408,Train Loss: 0.00008883\n",
      "Epoch: 122, RMSE: 13.9181, MAE: 5.4666, MAPE: 40.1373,Test Loss: 0.00011656\n",
      "=========epoch 123=========\n",
      "Iteraion 10, Train Loss: 0.00010487\n",
      "Iteraion 20, Train Loss: 0.00013071\n",
      "Iteraion 30, Train Loss: 0.00011146\n",
      "Epoch: 123, RMSE: 12.1723, MAE: 4.8192, MAPE: 42.4272,Train Loss: 0.00008996\n",
      "Epoch: 123, RMSE: 14.0943, MAE: 5.4992, MAPE: 39.6045,Test Loss: 0.00011950\n",
      "=========epoch 124=========\n",
      "Iteraion 10, Train Loss: 0.00012832\n",
      "Iteraion 20, Train Loss: 0.00010784\n",
      "Iteraion 30, Train Loss: 0.00013984\n",
      "Epoch: 124, RMSE: 12.3251, MAE: 4.8752, MAPE: 41.6847,Train Loss: 0.00009222\n",
      "Epoch: 124, RMSE: 14.1847, MAE: 5.5423, MAPE: 38.9274,Test Loss: 0.00012106\n",
      "=========epoch 125=========\n",
      "Iteraion 10, Train Loss: 0.00012468\n",
      "Iteraion 20, Train Loss: 0.00014174\n",
      "Iteraion 30, Train Loss: 0.00009993\n",
      "Epoch: 125, RMSE: 12.5003, MAE: 4.9308, MAPE: 41.3036,Train Loss: 0.00009487\n",
      "Epoch: 125, RMSE: 14.3117, MAE: 5.5927, MAPE: 38.9215,Test Loss: 0.00012322\n",
      "=========epoch 126=========\n",
      "Iteraion 10, Train Loss: 0.00012297\n",
      "Iteraion 20, Train Loss: 0.00013641\n",
      "Iteraion 30, Train Loss: 0.00013151\n",
      "Epoch: 126, RMSE: 12.3355, MAE: 4.8440, MAPE: 40.7216,Train Loss: 0.00009238\n",
      "Epoch: 126, RMSE: 14.2895, MAE: 5.5680, MAPE: 38.2221,Test Loss: 0.00012282\n",
      "=========epoch 127=========\n",
      "Iteraion 10, Train Loss: 0.00012185\n",
      "Iteraion 20, Train Loss: 0.00012778\n",
      "Iteraion 30, Train Loss: 0.00011285\n",
      "Epoch: 127, RMSE: 12.0803, MAE: 4.7972, MAPE: 40.8741,Train Loss: 0.00008860\n",
      "Epoch: 127, RMSE: 13.9998, MAE: 5.5035, MAPE: 38.5065,Test Loss: 0.00011794\n",
      "=========epoch 128=========\n",
      "Iteraion 10, Train Loss: 0.00013524\n",
      "Iteraion 20, Train Loss: 0.00015607\n",
      "Iteraion 30, Train Loss: 0.00013174\n",
      "Epoch: 128, RMSE: 12.8888, MAE: 4.9941, MAPE: 40.1730,Train Loss: 0.00010086\n",
      "Epoch: 128, RMSE: 14.8763, MAE: 5.7500, MAPE: 37.9124,Test Loss: 0.00013312\n",
      "=========epoch 129=========\n",
      "Iteraion 10, Train Loss: 0.00012911\n",
      "Iteraion 20, Train Loss: 0.00012818\n",
      "Iteraion 30, Train Loss: 0.00013890\n",
      "Epoch: 129, RMSE: 12.1631, MAE: 4.8311, MAPE: 41.3907,Train Loss: 0.00008983\n",
      "Epoch: 129, RMSE: 14.1185, MAE: 5.5530, MAPE: 38.7441,Test Loss: 0.00011990\n",
      "=========epoch 130=========\n",
      "Iteraion 10, Train Loss: 0.00011133\n",
      "Iteraion 20, Train Loss: 0.00012708\n",
      "Iteraion 30, Train Loss: 0.00010388\n",
      "Epoch: 130, RMSE: 12.4875, MAE: 4.9321, MAPE: 42.5167,Train Loss: 0.00009467\n",
      "Epoch: 130, RMSE: 14.4031, MAE: 5.6099, MAPE: 39.8094,Test Loss: 0.00012480\n",
      "=========epoch 131=========\n",
      "Iteraion 10, Train Loss: 0.00013102\n",
      "Iteraion 20, Train Loss: 0.00011248\n",
      "Iteraion 30, Train Loss: 0.00010725\n",
      "Epoch: 131, RMSE: 12.4834, MAE: 4.8750, MAPE: 40.7957,Train Loss: 0.00009462\n",
      "Epoch: 131, RMSE: 14.4977, MAE: 5.6186, MAPE: 38.4742,Test Loss: 0.00012640\n",
      "=========epoch 132=========\n",
      "Iteraion 10, Train Loss: 0.00011698\n",
      "Iteraion 20, Train Loss: 0.00012957\n",
      "Iteraion 30, Train Loss: 0.00011997\n",
      "Epoch: 132, RMSE: 12.0375, MAE: 4.7943, MAPE: 42.0730,Train Loss: 0.00008798\n",
      "Epoch: 132, RMSE: 13.9596, MAE: 5.4679, MAPE: 39.5492,Test Loss: 0.00011725\n",
      "=========epoch 133=========\n",
      "Iteraion 10, Train Loss: 0.00010378\n",
      "Iteraion 20, Train Loss: 0.00012013\n",
      "Iteraion 30, Train Loss: 0.00015552\n",
      "Epoch: 133, RMSE: 12.2180, MAE: 4.9160, MAPE: 43.5655,Train Loss: 0.00009063\n",
      "Epoch: 133, RMSE: 14.1994, MAE: 5.5900, MAPE: 40.6717,Test Loss: 0.00012135\n",
      "=========epoch 134=========\n",
      "Iteraion 10, Train Loss: 0.00012341\n",
      "Iteraion 20, Train Loss: 0.00011906\n",
      "Iteraion 30, Train Loss: 0.00014233\n",
      "Epoch: 134, RMSE: 12.2655, MAE: 4.8574, MAPE: 41.2253,Train Loss: 0.00009136\n",
      "Epoch: 134, RMSE: 14.1753, MAE: 5.5557, MAPE: 38.7073,Test Loss: 0.00012087\n",
      "=========epoch 135=========\n",
      "Iteraion 10, Train Loss: 0.00012192\n",
      "Iteraion 20, Train Loss: 0.00012982\n",
      "Iteraion 30, Train Loss: 0.00010432\n",
      "Epoch: 135, RMSE: 12.1144, MAE: 4.7615, MAPE: 39.8469,Train Loss: 0.00008911\n",
      "Epoch: 135, RMSE: 14.2437, MAE: 5.5305, MAPE: 37.5338,Test Loss: 0.00012202\n",
      "=========epoch 136=========\n",
      "Iteraion 10, Train Loss: 0.00014027\n",
      "Iteraion 20, Train Loss: 0.00012708\n",
      "Iteraion 30, Train Loss: 0.00012434\n",
      "Epoch: 136, RMSE: 11.9560, MAE: 4.7523, MAPE: 41.6492,Train Loss: 0.00008679\n",
      "Epoch: 136, RMSE: 14.0051, MAE: 5.4673, MAPE: 38.9798,Test Loss: 0.00011804\n",
      "=========epoch 137=========\n",
      "Iteraion 10, Train Loss: 0.00012273\n",
      "Iteraion 20, Train Loss: 0.00012268\n",
      "Iteraion 30, Train Loss: 0.00010999\n",
      "Epoch: 137, RMSE: 12.4013, MAE: 4.9341, MAPE: 42.4723,Train Loss: 0.00009337\n",
      "Epoch: 137, RMSE: 14.3306, MAE: 5.5940, MAPE: 39.5440,Test Loss: 0.00012362\n",
      "=========epoch 138=========\n",
      "Iteraion 10, Train Loss: 0.00011347\n",
      "Iteraion 20, Train Loss: 0.00011303\n",
      "Iteraion 30, Train Loss: 0.00011299\n",
      "Epoch: 138, RMSE: 12.7689, MAE: 5.1407, MAPE: 44.8406,Train Loss: 0.00009899\n",
      "Epoch: 138, RMSE: 14.7250, MAE: 5.8029, MAPE: 41.9731,Test Loss: 0.00013049\n",
      "=========epoch 139=========\n",
      "Iteraion 10, Train Loss: 0.00011475\n",
      "Iteraion 20, Train Loss: 0.00012270\n",
      "Iteraion 30, Train Loss: 0.00010860\n",
      "Epoch: 139, RMSE: 12.2274, MAE: 4.8763, MAPE: 42.7963,Train Loss: 0.00009077\n",
      "Epoch: 139, RMSE: 14.1674, MAE: 5.5493, MAPE: 39.8105,Test Loss: 0.00012076\n",
      "=========epoch 140=========\n",
      "Iteraion 10, Train Loss: 0.00013470\n",
      "Iteraion 20, Train Loss: 0.00011419\n",
      "Iteraion 30, Train Loss: 0.00010742\n",
      "Epoch: 140, RMSE: 11.9033, MAE: 4.7775, MAPE: 43.0559,Train Loss: 0.00008603\n",
      "Epoch: 140, RMSE: 13.9053, MAE: 5.4683, MAPE: 40.2579,Test Loss: 0.00011635\n",
      "=========epoch 141=========\n",
      "Iteraion 10, Train Loss: 0.00011852\n",
      "Iteraion 20, Train Loss: 0.00012501\n",
      "Iteraion 30, Train Loss: 0.00011739\n",
      "Epoch: 141, RMSE: 12.2512, MAE: 4.9058, MAPE: 43.3241,Train Loss: 0.00009113\n",
      "Epoch: 141, RMSE: 14.1687, MAE: 5.5580, MAPE: 40.7229,Test Loss: 0.00012081\n",
      "=========epoch 142=========\n",
      "Iteraion 10, Train Loss: 0.00011162\n",
      "Iteraion 20, Train Loss: 0.00014350\n",
      "Iteraion 30, Train Loss: 0.00012477\n",
      "Epoch: 142, RMSE: 13.2627, MAE: 5.3388, MAPE: 45.9155,Train Loss: 0.00010680\n",
      "Epoch: 142, RMSE: 15.0441, MAE: 5.9273, MAPE: 42.2940,Test Loss: 0.00013631\n",
      "=========epoch 143=========\n",
      "Iteraion 10, Train Loss: 0.00011569\n",
      "Iteraion 20, Train Loss: 0.00013026\n",
      "Iteraion 30, Train Loss: 0.00013707\n",
      "Epoch: 143, RMSE: 11.9557, MAE: 4.7958, MAPE: 43.5316,Train Loss: 0.00008680\n",
      "Epoch: 143, RMSE: 14.2421, MAE: 5.5689, MAPE: 40.5396,Test Loss: 0.00012202\n",
      "=========epoch 144=========\n",
      "Iteraion 10, Train Loss: 0.00013583\n",
      "Iteraion 20, Train Loss: 0.00014441\n",
      "Iteraion 30, Train Loss: 0.00012114\n",
      "Epoch: 144, RMSE: 12.0655, MAE: 4.7526, MAPE: 40.5712,Train Loss: 0.00008839\n",
      "Epoch: 144, RMSE: 14.1662, MAE: 5.5026, MAPE: 38.3150,Test Loss: 0.00012067\n",
      "=========epoch 145=========\n",
      "Iteraion 10, Train Loss: 0.00012458\n",
      "Iteraion 20, Train Loss: 0.00011282\n",
      "Iteraion 30, Train Loss: 0.00012444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 145, RMSE: 12.0672, MAE: 4.7968, MAPE: 40.0066,Train Loss: 0.00008842\n",
      "Epoch: 145, RMSE: 14.2311, MAE: 5.5846, MAPE: 37.7844,Test Loss: 0.00012181\n",
      "=========epoch 146=========\n",
      "Iteraion 10, Train Loss: 0.00012444\n",
      "Iteraion 20, Train Loss: 0.00012499\n",
      "Iteraion 30, Train Loss: 0.00010290\n",
      "Epoch: 146, RMSE: 11.8238, MAE: 4.6866, MAPE: 41.2194,Train Loss: 0.00008490\n",
      "Epoch: 146, RMSE: 13.9178, MAE: 5.4230, MAPE: 38.7626,Test Loss: 0.00011650\n",
      "=========epoch 147=========\n",
      "Iteraion 10, Train Loss: 0.00009938\n",
      "Iteraion 20, Train Loss: 0.00012515\n",
      "Iteraion 30, Train Loss: 0.00011409\n",
      "Epoch: 147, RMSE: 11.6401, MAE: 4.6353, MAPE: 41.0124,Train Loss: 0.00008227\n",
      "Epoch: 147, RMSE: 13.6072, MAE: 5.3240, MAPE: 38.4545,Test Loss: 0.00011140\n",
      "=========epoch 148=========\n",
      "Iteraion 10, Train Loss: 0.00011062\n",
      "Iteraion 20, Train Loss: 0.00010962\n",
      "Iteraion 30, Train Loss: 0.00010463\n",
      "Epoch: 148, RMSE: 12.2095, MAE: 4.8161, MAPE: 40.9381,Train Loss: 0.00009050\n",
      "Epoch: 148, RMSE: 14.2826, MAE: 5.5533, MAPE: 38.1576,Test Loss: 0.00012270\n",
      "=========epoch 149=========\n",
      "Iteraion 10, Train Loss: 0.00012016\n",
      "Iteraion 20, Train Loss: 0.00010747\n",
      "Iteraion 30, Train Loss: 0.00011278\n",
      "Epoch: 149, RMSE: 12.0859, MAE: 4.7277, MAPE: 39.3475,Train Loss: 0.00008867\n",
      "Epoch: 149, RMSE: 14.1088, MAE: 5.4513, MAPE: 36.8555,Test Loss: 0.00011972\n",
      "=========epoch 150=========\n",
      "Iteraion 10, Train Loss: 0.00010578\n",
      "Iteraion 20, Train Loss: 0.00011091\n",
      "Iteraion 30, Train Loss: 0.00011163\n",
      "Epoch: 150, RMSE: 11.6437, MAE: 4.6330, MAPE: 40.5889,Train Loss: 0.00008232\n",
      "Epoch: 150, RMSE: 13.7880, MAE: 5.3715, MAPE: 38.1192,Test Loss: 0.00011440\n",
      "=========epoch 151=========\n",
      "Iteraion 10, Train Loss: 0.00009731\n",
      "Iteraion 20, Train Loss: 0.00011730\n",
      "Iteraion 30, Train Loss: 0.00012927\n",
      "Epoch: 151, RMSE: 11.9131, MAE: 4.7190, MAPE: 41.8641,Train Loss: 0.00008617\n",
      "Epoch: 151, RMSE: 13.8474, MAE: 5.3750, MAPE: 39.0570,Test Loss: 0.00011537\n",
      "=========epoch 152=========\n",
      "Iteraion 10, Train Loss: 0.00012635\n",
      "Iteraion 20, Train Loss: 0.00010600\n",
      "Iteraion 30, Train Loss: 0.00011461\n",
      "Epoch: 152, RMSE: 11.6184, MAE: 4.6645, MAPE: 41.0527,Train Loss: 0.00008195\n",
      "Epoch: 152, RMSE: 13.5970, MAE: 5.3306, MAPE: 38.3183,Test Loss: 0.00011127\n",
      "=========epoch 153=========\n",
      "Iteraion 10, Train Loss: 0.00011145\n",
      "Iteraion 20, Train Loss: 0.00010639\n",
      "Iteraion 30, Train Loss: 0.00010291\n",
      "Epoch: 153, RMSE: 11.7775, MAE: 4.6407, MAPE: 38.0238,Train Loss: 0.00008422\n",
      "Epoch: 153, RMSE: 13.8605, MAE: 5.3883, MAPE: 36.0120,Test Loss: 0.00011559\n",
      "=========epoch 154=========\n",
      "Iteraion 10, Train Loss: 0.00012727\n",
      "Iteraion 20, Train Loss: 0.00011815\n",
      "Iteraion 30, Train Loss: 0.00010827\n",
      "Epoch: 154, RMSE: 11.5823, MAE: 4.5878, MAPE: 40.2564,Train Loss: 0.00008145\n",
      "Epoch: 154, RMSE: 13.6828, MAE: 5.3093, MAPE: 37.7815,Test Loss: 0.00011264\n",
      "=========epoch 155=========\n",
      "Iteraion 10, Train Loss: 0.00010759\n",
      "Iteraion 20, Train Loss: 0.00012020\n",
      "Iteraion 30, Train Loss: 0.00011985\n",
      "Epoch: 155, RMSE: 13.0292, MAE: 5.2460, MAPE: 45.1542,Train Loss: 0.00010307\n",
      "Epoch: 155, RMSE: 14.9445, MAE: 5.8649, MAPE: 41.7322,Test Loss: 0.00013446\n",
      "=========epoch 156=========\n",
      "Iteraion 10, Train Loss: 0.00010830\n",
      "Iteraion 20, Train Loss: 0.00011083\n",
      "Iteraion 30, Train Loss: 0.00008436\n",
      "Epoch: 156, RMSE: 11.6607, MAE: 4.5878, MAPE: 38.3912,Train Loss: 0.00008256\n",
      "Epoch: 156, RMSE: 13.7798, MAE: 5.3466, MAPE: 36.4047,Test Loss: 0.00011425\n",
      "=========epoch 157=========\n",
      "Iteraion 10, Train Loss: 0.00010658\n",
      "Iteraion 20, Train Loss: 0.00010054\n",
      "Iteraion 30, Train Loss: 0.00013737\n",
      "Epoch: 157, RMSE: 11.5811, MAE: 4.5832, MAPE: 39.9614,Train Loss: 0.00008144\n",
      "Epoch: 157, RMSE: 13.6384, MAE: 5.2667, MAPE: 37.5013,Test Loss: 0.00011191\n",
      "=========epoch 158=========\n",
      "Iteraion 10, Train Loss: 0.00012055\n",
      "Iteraion 20, Train Loss: 0.00009697\n",
      "Iteraion 30, Train Loss: 0.00012806\n",
      "Epoch: 158, RMSE: 12.0805, MAE: 4.7002, MAPE: 37.6260,Train Loss: 0.00008862\n",
      "Epoch: 158, RMSE: 14.0563, MAE: 5.4396, MAPE: 35.6625,Test Loss: 0.00011886\n",
      "=========epoch 159=========\n",
      "Iteraion 10, Train Loss: 0.00011002\n",
      "Iteraion 20, Train Loss: 0.00010882\n",
      "Iteraion 30, Train Loss: 0.00010742\n",
      "Epoch: 159, RMSE: 11.6069, MAE: 4.5437, MAPE: 37.8270,Train Loss: 0.00008181\n",
      "Epoch: 159, RMSE: 13.8352, MAE: 5.3361, MAPE: 35.9309,Test Loss: 0.00011512\n",
      "=========epoch 160=========\n",
      "Iteraion 10, Train Loss: 0.00010200\n",
      "Iteraion 20, Train Loss: 0.00011287\n",
      "Iteraion 30, Train Loss: 0.00010935\n",
      "Epoch: 160, RMSE: 11.3689, MAE: 4.5140, MAPE: 39.2205,Train Loss: 0.00007848\n",
      "Epoch: 160, RMSE: 13.4925, MAE: 5.2597, MAPE: 36.6834,Test Loss: 0.00010951\n",
      "=========epoch 161=========\n",
      "Iteraion 10, Train Loss: 0.00009480\n",
      "Iteraion 20, Train Loss: 0.00009846\n",
      "Iteraion 30, Train Loss: 0.00010861\n",
      "Epoch: 161, RMSE: 11.4076, MAE: 4.5549, MAPE: 41.0586,Train Loss: 0.00007902\n",
      "Epoch: 161, RMSE: 13.5339, MAE: 5.2618, MAPE: 38.4300,Test Loss: 0.00011021\n",
      "=========epoch 162=========\n",
      "Iteraion 10, Train Loss: 0.00012531\n",
      "Iteraion 20, Train Loss: 0.00012023\n",
      "Iteraion 30, Train Loss: 0.00011297\n",
      "Epoch: 162, RMSE: 11.6403, MAE: 4.5765, MAPE: 39.1945,Train Loss: 0.00008228\n",
      "Epoch: 162, RMSE: 13.8338, MAE: 5.3492, MAPE: 36.8590,Test Loss: 0.00011510\n",
      "=========epoch 163=========\n",
      "Iteraion 10, Train Loss: 0.00008855\n",
      "Iteraion 20, Train Loss: 0.00010367\n",
      "Iteraion 30, Train Loss: 0.00009575\n",
      "Epoch: 163, RMSE: 11.4352, MAE: 4.5463, MAPE: 40.2064,Train Loss: 0.00007940\n",
      "Epoch: 163, RMSE: 13.4327, MAE: 5.2275, MAPE: 37.7408,Test Loss: 0.00010857\n",
      "=========epoch 164=========\n",
      "Iteraion 10, Train Loss: 0.00010494\n",
      "Iteraion 20, Train Loss: 0.00008872\n",
      "Iteraion 30, Train Loss: 0.00010676\n",
      "Epoch: 164, RMSE: 11.6756, MAE: 4.5667, MAPE: 38.2684,Train Loss: 0.00008277\n",
      "Epoch: 164, RMSE: 13.7458, MAE: 5.2880, MAPE: 36.1177,Test Loss: 0.00011370\n",
      "=========epoch 165=========\n",
      "Iteraion 10, Train Loss: 0.00010612\n",
      "Iteraion 20, Train Loss: 0.00012674\n",
      "Iteraion 30, Train Loss: 0.00012951\n",
      "Epoch: 165, RMSE: 11.8385, MAE: 4.6234, MAPE: 38.9184,Train Loss: 0.00008512\n",
      "Epoch: 165, RMSE: 14.1091, MAE: 5.4054, MAPE: 36.7954,Test Loss: 0.00011970\n",
      "=========epoch 166=========\n",
      "Iteraion 10, Train Loss: 0.00012118\n",
      "Iteraion 20, Train Loss: 0.00010958\n",
      "Iteraion 30, Train Loss: 0.00014865\n",
      "Epoch: 166, RMSE: 11.4404, MAE: 4.5216, MAPE: 39.1125,Train Loss: 0.00007947\n",
      "Epoch: 166, RMSE: 13.6709, MAE: 5.2845, MAPE: 36.8546,Test Loss: 0.00011245\n",
      "=========epoch 167=========\n",
      "Iteraion 10, Train Loss: 0.00016417\n",
      "Iteraion 20, Train Loss: 0.00012926\n",
      "Iteraion 30, Train Loss: 0.00008792\n",
      "Epoch: 167, RMSE: 11.3439, MAE: 4.4905, MAPE: 39.3300,Train Loss: 0.00007814\n",
      "Epoch: 167, RMSE: 13.4979, MAE: 5.2284, MAPE: 36.9431,Test Loss: 0.00010961\n",
      "=========epoch 168=========\n",
      "Iteraion 10, Train Loss: 0.00011844\n",
      "Iteraion 20, Train Loss: 0.00012674\n",
      "Iteraion 30, Train Loss: 0.00013835\n",
      "Epoch: 168, RMSE: 11.3867, MAE: 4.5289, MAPE: 39.4089,Train Loss: 0.00007873\n",
      "Epoch: 168, RMSE: 13.6486, MAE: 5.2915, MAPE: 37.1817,Test Loss: 0.00011209\n",
      "=========epoch 169=========\n",
      "Iteraion 10, Train Loss: 0.00011810\n",
      "Iteraion 20, Train Loss: 0.00010098\n",
      "Iteraion 30, Train Loss: 0.00010571\n",
      "Epoch: 169, RMSE: 11.5311, MAE: 4.5216, MAPE: 37.8649,Train Loss: 0.00008073\n",
      "Epoch: 169, RMSE: 13.7013, MAE: 5.2889, MAPE: 35.8210,Test Loss: 0.00011296\n",
      "=========epoch 170=========\n",
      "Iteraion 10, Train Loss: 0.00011321\n",
      "Iteraion 20, Train Loss: 0.00009586\n",
      "Iteraion 30, Train Loss: 0.00011417\n",
      "Epoch: 170, RMSE: 11.6065, MAE: 4.6259, MAPE: 40.4647,Train Loss: 0.00008178\n",
      "Epoch: 170, RMSE: 13.6071, MAE: 5.2896, MAPE: 37.7246,Test Loss: 0.00011146\n",
      "=========epoch 171=========\n",
      "Iteraion 10, Train Loss: 0.00009339\n",
      "Iteraion 20, Train Loss: 0.00010899\n",
      "Iteraion 30, Train Loss: 0.00009937\n",
      "Epoch: 171, RMSE: 11.4728, MAE: 4.5631, MAPE: 40.1910,Train Loss: 0.00007993\n",
      "Epoch: 171, RMSE: 13.6654, MAE: 5.2977, MAPE: 37.9054,Test Loss: 0.00011237\n",
      "=========epoch 172=========\n",
      "Iteraion 10, Train Loss: 0.00009787\n",
      "Iteraion 20, Train Loss: 0.00011600\n",
      "Iteraion 30, Train Loss: 0.00010418\n",
      "Epoch: 172, RMSE: 11.4439, MAE: 4.5718, MAPE: 40.8279,Train Loss: 0.00007952\n",
      "Epoch: 172, RMSE: 13.5030, MAE: 5.2564, MAPE: 38.1491,Test Loss: 0.00010970\n",
      "=========epoch 173=========\n",
      "Iteraion 10, Train Loss: 0.00010133\n",
      "Iteraion 20, Train Loss: 0.00013172\n",
      "Iteraion 30, Train Loss: 0.00009938\n",
      "Epoch: 173, RMSE: 11.3213, MAE: 4.4628, MAPE: 38.0808,Train Loss: 0.00007783\n",
      "Epoch: 173, RMSE: 13.4660, MAE: 5.2109, MAPE: 35.9586,Test Loss: 0.00010907\n",
      "=========epoch 174=========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraion 10, Train Loss: 0.00011415\n",
      "Iteraion 20, Train Loss: 0.00010634\n",
      "Iteraion 30, Train Loss: 0.00010233\n",
      "Epoch: 174, RMSE: 11.6197, MAE: 4.5541, MAPE: 37.9796,Train Loss: 0.00008200\n",
      "Epoch: 174, RMSE: 13.8047, MAE: 5.3218, MAPE: 35.8439,Test Loss: 0.00011461\n",
      "=========epoch 175=========\n",
      "Iteraion 10, Train Loss: 0.00010276\n",
      "Iteraion 20, Train Loss: 0.00012055\n",
      "Iteraion 30, Train Loss: 0.00010735\n",
      "Epoch: 175, RMSE: 11.2835, MAE: 4.4626, MAPE: 37.4247,Train Loss: 0.00007731\n",
      "Epoch: 175, RMSE: 13.4494, MAE: 5.2228, MAPE: 35.4180,Test Loss: 0.00010886\n",
      "=========epoch 176=========\n",
      "Iteraion 10, Train Loss: 0.00012969\n",
      "Iteraion 20, Train Loss: 0.00009414\n",
      "Iteraion 30, Train Loss: 0.00008637\n",
      "Epoch: 176, RMSE: 11.2519, MAE: 4.4850, MAPE: 39.3789,Train Loss: 0.00007687\n",
      "Epoch: 176, RMSE: 13.3462, MAE: 5.1817, MAPE: 36.9644,Test Loss: 0.00010722\n",
      "=========epoch 177=========\n",
      "Iteraion 10, Train Loss: 0.00009841\n",
      "Iteraion 20, Train Loss: 0.00010878\n",
      "Iteraion 30, Train Loss: 0.00010792\n",
      "Epoch: 177, RMSE: 11.7793, MAE: 4.5849, MAPE: 36.7290,Train Loss: 0.00008423\n",
      "Epoch: 177, RMSE: 14.0020, MAE: 5.3817, MAPE: 34.6809,Test Loss: 0.00011793\n",
      "=========epoch 178=========\n",
      "Iteraion 10, Train Loss: 0.00010785\n",
      "Iteraion 20, Train Loss: 0.00011649\n",
      "Iteraion 30, Train Loss: 0.00012130\n",
      "Epoch: 178, RMSE: 11.4839, MAE: 4.5414, MAPE: 40.0828,Train Loss: 0.00008008\n",
      "Epoch: 178, RMSE: 13.6460, MAE: 5.2625, MAPE: 37.4224,Test Loss: 0.00011205\n",
      "=========epoch 179=========\n",
      "Iteraion 10, Train Loss: 0.00010143\n",
      "Iteraion 20, Train Loss: 0.00009380\n",
      "Iteraion 30, Train Loss: 0.00010074\n",
      "Epoch: 179, RMSE: 11.6016, MAE: 4.4988, MAPE: 37.7386,Train Loss: 0.00008174\n",
      "Epoch: 179, RMSE: 13.7296, MAE: 5.2389, MAPE: 35.5520,Test Loss: 0.00011341\n",
      "=========epoch 180=========\n",
      "Iteraion 10, Train Loss: 0.00012146\n",
      "Iteraion 20, Train Loss: 0.00010574\n",
      "Iteraion 30, Train Loss: 0.00008334\n",
      "Epoch: 180, RMSE: 11.2962, MAE: 4.5260, MAPE: 39.6183,Train Loss: 0.00007747\n",
      "Epoch: 180, RMSE: 13.3536, MAE: 5.2275, MAPE: 37.2479,Test Loss: 0.00010732\n",
      "=========epoch 181=========\n",
      "Iteraion 10, Train Loss: 0.00011465\n",
      "Iteraion 20, Train Loss: 0.00010141\n",
      "Iteraion 30, Train Loss: 0.00010354\n",
      "Epoch: 181, RMSE: 11.4308, MAE: 4.5087, MAPE: 36.4801,Train Loss: 0.00007934\n",
      "Epoch: 181, RMSE: 13.6039, MAE: 5.2935, MAPE: 34.4255,Test Loss: 0.00011132\n",
      "=========epoch 182=========\n",
      "Iteraion 10, Train Loss: 0.00009809\n",
      "Iteraion 20, Train Loss: 0.00010010\n",
      "Iteraion 30, Train Loss: 0.00010329\n",
      "Epoch: 182, RMSE: 11.2361, MAE: 4.4427, MAPE: 38.1556,Train Loss: 0.00007666\n",
      "Epoch: 182, RMSE: 13.5358, MAE: 5.2188, MAPE: 35.9099,Test Loss: 0.00011025\n",
      "=========epoch 183=========\n",
      "Iteraion 10, Train Loss: 0.00010522\n",
      "Iteraion 20, Train Loss: 0.00010675\n",
      "Iteraion 30, Train Loss: 0.00010680\n",
      "Epoch: 183, RMSE: 11.2554, MAE: 4.4094, MAPE: 37.5188,Train Loss: 0.00007693\n",
      "Epoch: 183, RMSE: 13.5234, MAE: 5.1878, MAPE: 35.4920,Test Loss: 0.00011003\n",
      "=========epoch 184=========\n",
      "Iteraion 10, Train Loss: 0.00011934\n",
      "Iteraion 20, Train Loss: 0.00015211\n",
      "Iteraion 30, Train Loss: 0.00011142\n",
      "Epoch: 184, RMSE: 11.2126, MAE: 4.4115, MAPE: 38.9121,Train Loss: 0.00007635\n",
      "Epoch: 184, RMSE: 13.4576, MAE: 5.1737, MAPE: 36.5942,Test Loss: 0.00010895\n",
      "=========epoch 185=========\n",
      "Iteraion 10, Train Loss: 0.00011154\n",
      "Iteraion 20, Train Loss: 0.00010340\n",
      "Iteraion 30, Train Loss: 0.00012537\n",
      "Epoch: 185, RMSE: 11.7284, MAE: 4.5125, MAPE: 36.1331,Train Loss: 0.00008353\n",
      "Epoch: 185, RMSE: 13.9452, MAE: 5.3234, MAPE: 34.4910,Test Loss: 0.00011697\n",
      "=========epoch 186=========\n",
      "Iteraion 10, Train Loss: 0.00011029\n",
      "Iteraion 20, Train Loss: 0.00009914\n",
      "Iteraion 30, Train Loss: 0.00011452\n",
      "Epoch: 186, RMSE: 10.9818, MAE: 4.3456, MAPE: 38.0538,Train Loss: 0.00007323\n",
      "Epoch: 186, RMSE: 13.2382, MAE: 5.0991, MAPE: 35.8027,Test Loss: 0.00010544\n",
      "=========epoch 187=========\n",
      "Iteraion 10, Train Loss: 0.00009821\n",
      "Iteraion 20, Train Loss: 0.00009757\n",
      "Iteraion 30, Train Loss: 0.00010413\n",
      "Epoch: 187, RMSE: 11.0068, MAE: 4.3545, MAPE: 37.6319,Train Loss: 0.00007357\n",
      "Epoch: 187, RMSE: 13.2724, MAE: 5.1196, MAPE: 35.5358,Test Loss: 0.00010597\n",
      "=========epoch 188=========\n",
      "Iteraion 10, Train Loss: 0.00009318\n",
      "Iteraion 20, Train Loss: 0.00010301\n",
      "Iteraion 30, Train Loss: 0.00011680\n",
      "Epoch: 188, RMSE: 11.3047, MAE: 4.4137, MAPE: 36.8373,Train Loss: 0.00007761\n",
      "Epoch: 188, RMSE: 13.5899, MAE: 5.2122, MAPE: 34.8442,Test Loss: 0.00011109\n",
      "=========epoch 189=========\n",
      "Iteraion 10, Train Loss: 0.00011235\n",
      "Iteraion 20, Train Loss: 0.00009899\n",
      "Iteraion 30, Train Loss: 0.00010759\n",
      "Epoch: 189, RMSE: 11.0466, MAE: 4.4085, MAPE: 38.6868,Train Loss: 0.00007410\n",
      "Epoch: 189, RMSE: 13.2108, MAE: 5.1319, MAPE: 36.3285,Test Loss: 0.00010501\n",
      "=========epoch 190=========\n",
      "Iteraion 10, Train Loss: 0.00010242\n",
      "Iteraion 20, Train Loss: 0.00009926\n",
      "Iteraion 30, Train Loss: 0.00011617\n",
      "Epoch: 190, RMSE: 11.5750, MAE: 4.6516, MAPE: 40.4785,Train Loss: 0.00008135\n",
      "Epoch: 190, RMSE: 13.7385, MAE: 5.3526, MAPE: 37.7977,Test Loss: 0.00011360\n",
      "=========epoch 191=========\n",
      "Iteraion 10, Train Loss: 0.00010861\n",
      "Iteraion 20, Train Loss: 0.00011046\n",
      "Iteraion 30, Train Loss: 0.00009892\n",
      "Epoch: 191, RMSE: 11.0203, MAE: 4.3770, MAPE: 38.3646,Train Loss: 0.00007374\n",
      "Epoch: 191, RMSE: 13.2451, MAE: 5.1174, MAPE: 36.0005,Test Loss: 0.00010558\n",
      "=========epoch 192=========\n",
      "Iteraion 10, Train Loss: 0.00010435\n",
      "Iteraion 20, Train Loss: 0.00010003\n",
      "Iteraion 30, Train Loss: 0.00010069\n",
      "Epoch: 192, RMSE: 10.9252, MAE: 4.3194, MAPE: 37.8235,Train Loss: 0.00007248\n",
      "Epoch: 192, RMSE: 13.2462, MAE: 5.1038, MAPE: 35.7028,Test Loss: 0.00010556\n",
      "=========epoch 193=========\n",
      "Iteraion 10, Train Loss: 0.00010617\n",
      "Iteraion 20, Train Loss: 0.00011972\n",
      "Iteraion 30, Train Loss: 0.00011412\n",
      "Epoch: 193, RMSE: 10.9451, MAE: 4.3446, MAPE: 37.8176,Train Loss: 0.00007274\n",
      "Epoch: 193, RMSE: 13.2456, MAE: 5.1050, MAPE: 35.6073,Test Loss: 0.00010558\n",
      "=========epoch 194=========\n",
      "Iteraion 10, Train Loss: 0.00010913\n",
      "Iteraion 20, Train Loss: 0.00011238\n",
      "Iteraion 30, Train Loss: 0.00014835\n",
      "Epoch: 194, RMSE: 11.1784, MAE: 4.4282, MAPE: 38.2607,Train Loss: 0.00007587\n",
      "Epoch: 194, RMSE: 13.3588, MAE: 5.1437, MAPE: 35.8269,Test Loss: 0.00010742\n",
      "=========epoch 195=========\n",
      "Iteraion 10, Train Loss: 0.00013017\n",
      "Iteraion 20, Train Loss: 0.00011600\n",
      "Iteraion 30, Train Loss: 0.00008697\n",
      "Epoch: 195, RMSE: 11.2340, MAE: 4.4196, MAPE: 37.2158,Train Loss: 0.00007664\n",
      "Epoch: 195, RMSE: 13.7154, MAE: 5.2603, MAPE: 35.3588,Test Loss: 0.00011319\n",
      "=========epoch 196=========\n",
      "Iteraion 10, Train Loss: 0.00010963\n",
      "Iteraion 20, Train Loss: 0.00009037\n",
      "Iteraion 30, Train Loss: 0.00009646\n",
      "Epoch: 196, RMSE: 11.1563, MAE: 4.3527, MAPE: 37.0386,Train Loss: 0.00007559\n",
      "Epoch: 196, RMSE: 13.5446, MAE: 5.1726, MAPE: 35.0766,Test Loss: 0.00011035\n",
      "=========epoch 197=========\n",
      "Iteraion 10, Train Loss: 0.00009759\n",
      "Iteraion 20, Train Loss: 0.00010191\n",
      "Iteraion 30, Train Loss: 0.00010499\n",
      "Epoch: 197, RMSE: 11.0379, MAE: 4.3301, MAPE: 37.0978,Train Loss: 0.00007399\n",
      "Epoch: 197, RMSE: 13.3282, MAE: 5.0818, MAPE: 35.2317,Test Loss: 0.00010688\n",
      "=========epoch 198=========\n",
      "Iteraion 10, Train Loss: 0.00010035\n",
      "Iteraion 20, Train Loss: 0.00009396\n",
      "Iteraion 30, Train Loss: 0.00009984\n",
      "Epoch: 198, RMSE: 10.9613, MAE: 4.3081, MAPE: 37.1233,Train Loss: 0.00007297\n",
      "Epoch: 198, RMSE: 13.4204, MAE: 5.1336, MAPE: 35.1935,Test Loss: 0.00010834\n",
      "=========epoch 199=========\n",
      "Iteraion 10, Train Loss: 0.00009927\n",
      "Iteraion 20, Train Loss: 0.00010171\n",
      "Iteraion 30, Train Loss: 0.00011138\n",
      "Epoch: 199, RMSE: 11.2518, MAE: 4.5579, MAPE: 41.3013,Train Loss: 0.00007688\n",
      "Epoch: 199, RMSE: 13.5142, MAE: 5.2790, MAPE: 38.5554,Test Loss: 0.00010992\n",
      "=========epoch 200=========\n",
      "Iteraion 10, Train Loss: 0.00008661\n",
      "Iteraion 20, Train Loss: 0.00009595\n",
      "Iteraion 30, Train Loss: 0.00013168\n",
      "Epoch: 200, RMSE: 12.0599, MAE: 4.8346, MAPE: 41.4300,Train Loss: 0.00008830\n",
      "Epoch: 200, RMSE: 14.2239, MAE: 5.5126, MAPE: 38.7827,Test Loss: 0.00012179\n",
      "=========epoch 201=========\n",
      "Iteraion 10, Train Loss: 0.00009725\n",
      "Iteraion 20, Train Loss: 0.00011927\n",
      "Iteraion 30, Train Loss: 0.00010066\n",
      "Epoch: 201, RMSE: 10.9347, MAE: 4.3496, MAPE: 38.2839,Train Loss: 0.00007260\n",
      "Epoch: 201, RMSE: 13.3304, MAE: 5.1205, MAPE: 35.9228,Test Loss: 0.00010695\n",
      "=========epoch 202=========\n",
      "Iteraion 10, Train Loss: 0.00010766\n",
      "Iteraion 20, Train Loss: 0.00010909\n",
      "Iteraion 30, Train Loss: 0.00009832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 202, RMSE: 11.0516, MAE: 4.4513, MAPE: 40.5344,Train Loss: 0.00007416\n",
      "Epoch: 202, RMSE: 13.1938, MAE: 5.1218, MAPE: 37.4155,Test Loss: 0.00010475\n",
      "=========epoch 203=========\n",
      "Iteraion 10, Train Loss: 0.00010080\n",
      "Iteraion 20, Train Loss: 0.00011212\n",
      "Iteraion 30, Train Loss: 0.00012041\n",
      "Epoch: 203, RMSE: 11.5533, MAE: 4.6341, MAPE: 39.5664,Train Loss: 0.00008104\n",
      "Epoch: 203, RMSE: 13.7331, MAE: 5.3300, MAPE: 37.0182,Test Loss: 0.00011355\n",
      "=========epoch 204=========\n",
      "Iteraion 10, Train Loss: 0.00011081\n",
      "Iteraion 20, Train Loss: 0.00010627\n",
      "Iteraion 30, Train Loss: 0.00010820\n",
      "Epoch: 204, RMSE: 11.3860, MAE: 4.4576, MAPE: 38.1267,Train Loss: 0.00007871\n",
      "Epoch: 204, RMSE: 13.6614, MAE: 5.1957, MAPE: 35.9121,Test Loss: 0.00011235\n",
      "=========epoch 205=========\n",
      "Iteraion 10, Train Loss: 0.00010286\n",
      "Iteraion 20, Train Loss: 0.00009099\n",
      "Iteraion 30, Train Loss: 0.00009727\n",
      "Epoch: 205, RMSE: 10.8476, MAE: 4.2921, MAPE: 36.6260,Train Loss: 0.00007145\n",
      "Epoch: 205, RMSE: 13.2075, MAE: 5.0861, MAPE: 34.6238,Test Loss: 0.00010497\n",
      "=========epoch 206=========\n",
      "Iteraion 10, Train Loss: 0.00009201\n",
      "Iteraion 20, Train Loss: 0.00009558\n",
      "Iteraion 30, Train Loss: 0.00010337\n",
      "Epoch: 206, RMSE: 10.7863, MAE: 4.2599, MAPE: 37.3359,Train Loss: 0.00007064\n",
      "Epoch: 206, RMSE: 13.1189, MAE: 5.0308, MAPE: 35.2230,Test Loss: 0.00010356\n",
      "=========epoch 207=========\n",
      "Iteraion 10, Train Loss: 0.00010760\n",
      "Iteraion 20, Train Loss: 0.00010917\n",
      "Iteraion 30, Train Loss: 0.00011032\n",
      "Epoch: 207, RMSE: 10.8807, MAE: 4.3121, MAPE: 37.8085,Train Loss: 0.00007189\n",
      "Epoch: 207, RMSE: 13.1601, MAE: 5.0655, MAPE: 35.3463,Test Loss: 0.00010423\n",
      "=========epoch 208=========\n",
      "Iteraion 10, Train Loss: 0.00010294\n",
      "Iteraion 20, Train Loss: 0.00010042\n",
      "Iteraion 30, Train Loss: 0.00010488\n",
      "Epoch: 208, RMSE: 11.0043, MAE: 4.2898, MAPE: 36.0332,Train Loss: 0.00007353\n",
      "Epoch: 208, RMSE: 13.4339, MAE: 5.1172, MAPE: 34.2166,Test Loss: 0.00010859\n",
      "=========epoch 209=========\n",
      "Iteraion 10, Train Loss: 0.00009108\n",
      "Iteraion 20, Train Loss: 0.00008709\n",
      "Iteraion 30, Train Loss: 0.00008389\n",
      "Epoch: 209, RMSE: 11.0077, MAE: 4.2765, MAPE: 37.4656,Train Loss: 0.00007358\n",
      "Epoch: 209, RMSE: 13.3801, MAE: 5.0479, MAPE: 35.2278,Test Loss: 0.00010770\n",
      "=========epoch 210=========\n",
      "Iteraion 10, Train Loss: 0.00011033\n",
      "Iteraion 20, Train Loss: 0.00010282\n",
      "Iteraion 30, Train Loss: 0.00011695\n",
      "Epoch: 210, RMSE: 11.0326, MAE: 4.4095, MAPE: 39.0632,Train Loss: 0.00007392\n",
      "Epoch: 210, RMSE: 13.2496, MAE: 5.1331, MAPE: 36.5532,Test Loss: 0.00010564\n",
      "=========epoch 211=========\n",
      "Iteraion 10, Train Loss: 0.00009750\n",
      "Iteraion 20, Train Loss: 0.00010424\n",
      "Iteraion 30, Train Loss: 0.00012028\n",
      "Epoch: 211, RMSE: 10.9756, MAE: 4.2940, MAPE: 35.6759,Train Loss: 0.00007315\n",
      "Epoch: 211, RMSE: 13.3517, MAE: 5.1164, MAPE: 34.0390,Test Loss: 0.00010730\n",
      "=========epoch 212=========\n",
      "Iteraion 10, Train Loss: 0.00009121\n",
      "Iteraion 20, Train Loss: 0.00009707\n",
      "Iteraion 30, Train Loss: 0.00010546\n",
      "Epoch: 212, RMSE: 10.9994, MAE: 4.3027, MAPE: 35.3789,Train Loss: 0.00007347\n",
      "Epoch: 212, RMSE: 13.3592, MAE: 5.1255, MAPE: 33.6864,Test Loss: 0.00010738\n",
      "=========epoch 213=========\n",
      "Iteraion 10, Train Loss: 0.00008760\n",
      "Iteraion 20, Train Loss: 0.00009885\n",
      "Iteraion 30, Train Loss: 0.00009108\n",
      "Epoch: 213, RMSE: 10.6585, MAE: 4.2147, MAPE: 36.6566,Train Loss: 0.00006899\n",
      "Epoch: 213, RMSE: 13.1194, MAE: 5.0228, MAPE: 34.5864,Test Loss: 0.00010358\n",
      "=========epoch 214=========\n",
      "Iteraion 10, Train Loss: 0.00011095\n",
      "Iteraion 20, Train Loss: 0.00009650\n",
      "Iteraion 30, Train Loss: 0.00009598\n",
      "Epoch: 214, RMSE: 10.7953, MAE: 4.2749, MAPE: 37.1625,Train Loss: 0.00007076\n",
      "Epoch: 214, RMSE: 13.1500, MAE: 5.0556, MAPE: 35.0204,Test Loss: 0.00010408\n",
      "=========epoch 215=========\n",
      "Iteraion 10, Train Loss: 0.00009436\n",
      "Iteraion 20, Train Loss: 0.00013021\n",
      "Iteraion 30, Train Loss: 0.00010521\n",
      "Epoch: 215, RMSE: 10.9794, MAE: 4.2785, MAPE: 35.9493,Train Loss: 0.00007321\n",
      "Epoch: 215, RMSE: 13.4640, MAE: 5.1132, MAPE: 33.9881,Test Loss: 0.00010904\n",
      "=========epoch 216=========\n",
      "Iteraion 10, Train Loss: 0.00010170\n",
      "Iteraion 20, Train Loss: 0.00009940\n",
      "Iteraion 30, Train Loss: 0.00010214\n",
      "Epoch: 216, RMSE: 10.9280, MAE: 4.2426, MAPE: 35.6699,Train Loss: 0.00007253\n",
      "Epoch: 216, RMSE: 13.3204, MAE: 5.0590, MAPE: 34.0840,Test Loss: 0.00010673\n",
      "=========epoch 217=========\n",
      "Iteraion 10, Train Loss: 0.00009222\n",
      "Iteraion 20, Train Loss: 0.00010105\n",
      "Iteraion 30, Train Loss: 0.00009654\n",
      "Epoch: 217, RMSE: 11.1592, MAE: 4.3589, MAPE: 34.9156,Train Loss: 0.00007561\n",
      "Epoch: 217, RMSE: 13.5372, MAE: 5.1799, MAPE: 33.4990,Test Loss: 0.00011023\n",
      "=========epoch 218=========\n",
      "Iteraion 10, Train Loss: 0.00008708\n",
      "Iteraion 20, Train Loss: 0.00010429\n",
      "Iteraion 30, Train Loss: 0.00009811\n",
      "Epoch: 218, RMSE: 11.1514, MAE: 4.4304, MAPE: 37.7076,Train Loss: 0.00007551\n",
      "Epoch: 218, RMSE: 13.4411, MAE: 5.1730, MAPE: 35.2308,Test Loss: 0.00010873\n",
      "=========epoch 219=========\n",
      "Iteraion 10, Train Loss: 0.00008373\n",
      "Iteraion 20, Train Loss: 0.00009961\n",
      "Iteraion 30, Train Loss: 0.00012006\n",
      "Epoch: 219, RMSE: 11.5640, MAE: 4.5811, MAPE: 38.8950,Train Loss: 0.00008119\n",
      "Epoch: 219, RMSE: 13.8775, MAE: 5.2908, MAPE: 36.5428,Test Loss: 0.00011595\n",
      "=========epoch 220=========\n",
      "Iteraion 10, Train Loss: 0.00009683\n",
      "Iteraion 20, Train Loss: 0.00008505\n",
      "Iteraion 30, Train Loss: 0.00009678\n",
      "Epoch: 220, RMSE: 10.6744, MAE: 4.2173, MAPE: 37.0057,Train Loss: 0.00006919\n",
      "Epoch: 220, RMSE: 13.0480, MAE: 4.9863, MAPE: 34.7572,Test Loss: 0.00010244\n",
      "=========epoch 221=========\n",
      "Iteraion 10, Train Loss: 0.00011079\n",
      "Iteraion 20, Train Loss: 0.00012154\n",
      "Iteraion 30, Train Loss: 0.00009621\n",
      "Epoch: 221, RMSE: 10.6379, MAE: 4.1880, MAPE: 36.2379,Train Loss: 0.00006871\n",
      "Epoch: 221, RMSE: 13.0646, MAE: 4.9799, MAPE: 34.0539,Test Loss: 0.00010268\n",
      "=========epoch 222=========\n",
      "Iteraion 10, Train Loss: 0.00012270\n",
      "Iteraion 20, Train Loss: 0.00010798\n",
      "Iteraion 30, Train Loss: 0.00009327\n",
      "Epoch: 222, RMSE: 10.6109, MAE: 4.1983, MAPE: 36.5328,Train Loss: 0.00006837\n",
      "Epoch: 222, RMSE: 12.9677, MAE: 4.9659, MAPE: 34.5030,Test Loss: 0.00010121\n",
      "=========epoch 223=========\n",
      "Iteraion 10, Train Loss: 0.00009960\n",
      "Iteraion 20, Train Loss: 0.00008353\n",
      "Iteraion 30, Train Loss: 0.00010303\n",
      "Epoch: 223, RMSE: 10.8530, MAE: 4.2927, MAPE: 36.9767,Train Loss: 0.00007151\n",
      "Epoch: 223, RMSE: 13.1961, MAE: 5.0614, MAPE: 34.8069,Test Loss: 0.00010482\n",
      "=========epoch 224=========\n",
      "Iteraion 10, Train Loss: 0.00009236\n",
      "Iteraion 20, Train Loss: 0.00009760\n",
      "Iteraion 30, Train Loss: 0.00010719\n",
      "Epoch: 224, RMSE: 11.6849, MAE: 4.5635, MAPE: 34.6840,Train Loss: 0.00008291\n",
      "Epoch: 224, RMSE: 14.0981, MAE: 5.4521, MAPE: 33.3379,Test Loss: 0.00011956\n",
      "=========epoch 225=========\n",
      "Iteraion 10, Train Loss: 0.00009718\n",
      "Iteraion 20, Train Loss: 0.00010121\n",
      "Iteraion 30, Train Loss: 0.00009960\n",
      "Epoch: 225, RMSE: 10.8787, MAE: 4.2608, MAPE: 35.2004,Train Loss: 0.00007186\n",
      "Epoch: 225, RMSE: 13.2670, MAE: 5.0867, MAPE: 33.4659,Test Loss: 0.00010591\n",
      "=========epoch 226=========\n",
      "Iteraion 10, Train Loss: 0.00010017\n",
      "Iteraion 20, Train Loss: 0.00011072\n",
      "Iteraion 30, Train Loss: 0.00011032\n",
      "Epoch: 226, RMSE: 10.7760, MAE: 4.2338, MAPE: 35.7322,Train Loss: 0.00007051\n",
      "Epoch: 226, RMSE: 13.1172, MAE: 5.0253, MAPE: 33.8118,Test Loss: 0.00010353\n",
      "=========epoch 227=========\n",
      "Iteraion 10, Train Loss: 0.00010660\n",
      "Iteraion 20, Train Loss: 0.00009415\n",
      "Iteraion 30, Train Loss: 0.00011958\n",
      "Epoch: 227, RMSE: 10.8308, MAE: 4.2612, MAPE: 36.3555,Train Loss: 0.00007124\n",
      "Epoch: 227, RMSE: 13.2351, MAE: 5.0469, MAPE: 34.2561,Test Loss: 0.00010542\n",
      "=========epoch 228=========\n",
      "Iteraion 10, Train Loss: 0.00010419\n",
      "Iteraion 20, Train Loss: 0.00009804\n",
      "Iteraion 30, Train Loss: 0.00010569\n",
      "Epoch: 228, RMSE: 11.3217, MAE: 4.3305, MAPE: 35.1211,Train Loss: 0.00007784\n",
      "Epoch: 228, RMSE: 13.8940, MAE: 5.2140, MAPE: 33.6011,Test Loss: 0.00011612\n",
      "=========epoch 229=========\n",
      "Iteraion 10, Train Loss: 0.00010196\n",
      "Iteraion 20, Train Loss: 0.00011993\n",
      "Iteraion 30, Train Loss: 0.00010836\n",
      "Epoch: 229, RMSE: 10.5277, MAE: 4.1558, MAPE: 35.1163,Train Loss: 0.00006731\n",
      "Epoch: 229, RMSE: 13.0662, MAE: 5.0035, MAPE: 33.5475,Test Loss: 0.00010272\n",
      "=========epoch 230=========\n",
      "Iteraion 10, Train Loss: 0.00009483\n",
      "Iteraion 20, Train Loss: 0.00008602\n",
      "Iteraion 30, Train Loss: 0.00009950\n",
      "Epoch: 230, RMSE: 10.6754, MAE: 4.1991, MAPE: 36.2636,Train Loss: 0.00006921\n",
      "Epoch: 230, RMSE: 13.1402, MAE: 5.0034, MAPE: 34.4840,Test Loss: 0.00010390\n",
      "=========epoch 231=========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraion 10, Train Loss: 0.00010377\n",
      "Iteraion 20, Train Loss: 0.00009667\n",
      "Iteraion 30, Train Loss: 0.00010823\n",
      "Epoch: 231, RMSE: 11.0422, MAE: 4.2895, MAPE: 34.6820,Train Loss: 0.00007404\n",
      "Epoch: 231, RMSE: 13.4890, MAE: 5.1348, MAPE: 33.2132,Test Loss: 0.00010946\n",
      "=========epoch 232=========\n",
      "Iteraion 10, Train Loss: 0.00009756\n",
      "Iteraion 20, Train Loss: 0.00011831\n",
      "Iteraion 30, Train Loss: 0.00009652\n",
      "Epoch: 232, RMSE: 10.8625, MAE: 4.2618, MAPE: 34.5803,Train Loss: 0.00007165\n",
      "Epoch: 232, RMSE: 13.2008, MAE: 5.0857, MAPE: 33.0509,Test Loss: 0.00010486\n",
      "=========epoch 233=========\n",
      "Iteraion 10, Train Loss: 0.00008704\n",
      "Iteraion 20, Train Loss: 0.00009992\n",
      "Iteraion 30, Train Loss: 0.00008960\n",
      "Epoch: 233, RMSE: 10.6319, MAE: 4.1818, MAPE: 35.2755,Train Loss: 0.00006864\n",
      "Epoch: 233, RMSE: 13.0810, MAE: 4.9890, MAPE: 33.6086,Test Loss: 0.00010296\n",
      "=========epoch 234=========\n",
      "Iteraion 10, Train Loss: 0.00008363\n",
      "Iteraion 20, Train Loss: 0.00009639\n",
      "Iteraion 30, Train Loss: 0.00009604\n",
      "Epoch: 234, RMSE: 10.4537, MAE: 4.1270, MAPE: 36.5011,Train Loss: 0.00006637\n",
      "Epoch: 234, RMSE: 13.0391, MAE: 4.9464, MAPE: 34.4349,Test Loss: 0.00010231\n",
      "=========epoch 235=========\n",
      "Iteraion 10, Train Loss: 0.00009084\n",
      "Iteraion 20, Train Loss: 0.00010745\n",
      "Iteraion 30, Train Loss: 0.00007643\n",
      "Epoch: 235, RMSE: 10.8793, MAE: 4.2022, MAPE: 34.3982,Train Loss: 0.00007188\n",
      "Epoch: 235, RMSE: 13.4801, MAE: 5.0816, MAPE: 32.9742,Test Loss: 0.00010933\n",
      "=========epoch 236=========\n",
      "Iteraion 10, Train Loss: 0.00010005\n",
      "Iteraion 20, Train Loss: 0.00009247\n",
      "Iteraion 30, Train Loss: 0.00008875\n",
      "Epoch: 236, RMSE: 10.5839, MAE: 4.1467, MAPE: 34.6783,Train Loss: 0.00006802\n",
      "Epoch: 236, RMSE: 13.1329, MAE: 5.0050, MAPE: 33.2110,Test Loss: 0.00010379\n",
      "=========epoch 237=========\n",
      "Iteraion 10, Train Loss: 0.00009200\n",
      "Iteraion 20, Train Loss: 0.00011141\n",
      "Iteraion 30, Train Loss: 0.00009601\n",
      "Epoch: 237, RMSE: 10.5622, MAE: 4.1660, MAPE: 36.3243,Train Loss: 0.00006774\n",
      "Epoch: 237, RMSE: 12.8507, MAE: 4.9021, MAPE: 34.2972,Test Loss: 0.00009938\n",
      "=========epoch 238=========\n",
      "Iteraion 10, Train Loss: 0.00008771\n",
      "Iteraion 20, Train Loss: 0.00011112\n",
      "Iteraion 30, Train Loss: 0.00009281\n",
      "Epoch: 238, RMSE: 10.5789, MAE: 4.1536, MAPE: 35.2195,Train Loss: 0.00006795\n",
      "Epoch: 238, RMSE: 13.1380, MAE: 4.9966, MAPE: 33.5736,Test Loss: 0.00010387\n",
      "=========epoch 239=========\n",
      "Iteraion 10, Train Loss: 0.00009073\n",
      "Iteraion 20, Train Loss: 0.00009070\n",
      "Iteraion 30, Train Loss: 0.00010526\n",
      "Epoch: 239, RMSE: 10.9396, MAE: 4.4055, MAPE: 38.6220,Train Loss: 0.00007267\n",
      "Epoch: 239, RMSE: 13.3990, MAE: 5.1597, MAPE: 36.1205,Test Loss: 0.00010810\n",
      "=========epoch 240=========\n",
      "Iteraion 10, Train Loss: 0.00009724\n",
      "Iteraion 20, Train Loss: 0.00009846\n",
      "Iteraion 30, Train Loss: 0.00008094\n",
      "Epoch: 240, RMSE: 10.6112, MAE: 4.1719, MAPE: 35.6921,Train Loss: 0.00006838\n",
      "Epoch: 240, RMSE: 13.0818, MAE: 4.9785, MAPE: 33.7720,Test Loss: 0.00010298\n",
      "=========epoch 241=========\n",
      "Iteraion 10, Train Loss: 0.00010444\n",
      "Iteraion 20, Train Loss: 0.00009377\n",
      "Iteraion 30, Train Loss: 0.00009063\n",
      "Epoch: 241, RMSE: 10.4652, MAE: 4.1296, MAPE: 35.6804,Train Loss: 0.00006651\n",
      "Epoch: 241, RMSE: 12.9893, MAE: 4.9458, MAPE: 33.9253,Test Loss: 0.00010154\n",
      "=========epoch 242=========\n",
      "Iteraion 10, Train Loss: 0.00010463\n",
      "Iteraion 20, Train Loss: 0.00008697\n",
      "Iteraion 30, Train Loss: 0.00010339\n",
      "Epoch: 242, RMSE: 10.3783, MAE: 4.1217, MAPE: 36.8476,Train Loss: 0.00006540\n",
      "Epoch: 242, RMSE: 12.8690, MAE: 4.9165, MAPE: 34.7978,Test Loss: 0.00009968\n",
      "=========epoch 243=========\n",
      "Iteraion 10, Train Loss: 0.00010511\n",
      "Iteraion 20, Train Loss: 0.00009932\n",
      "Iteraion 30, Train Loss: 0.00010190\n",
      "Epoch: 243, RMSE: 11.3860, MAE: 4.4050, MAPE: 34.3841,Train Loss: 0.00007875\n",
      "Epoch: 243, RMSE: 13.7922, MAE: 5.2411, MAPE: 33.0325,Test Loss: 0.00011447\n",
      "=========epoch 244=========\n",
      "Iteraion 10, Train Loss: 0.00011084\n",
      "Iteraion 20, Train Loss: 0.00010668\n",
      "Iteraion 30, Train Loss: 0.00009995\n",
      "Epoch: 244, RMSE: 10.6182, MAE: 4.1398, MAPE: 35.3897,Train Loss: 0.00006848\n",
      "Epoch: 244, RMSE: 13.2611, MAE: 4.9929, MAPE: 33.4756,Test Loss: 0.00010584\n",
      "=========epoch 245=========\n",
      "Iteraion 10, Train Loss: 0.00010812\n",
      "Iteraion 20, Train Loss: 0.00010091\n",
      "Iteraion 30, Train Loss: 0.00010798\n",
      "Epoch: 245, RMSE: 10.9058, MAE: 4.2274, MAPE: 35.0990,Train Loss: 0.00007222\n",
      "Epoch: 245, RMSE: 13.3509, MAE: 5.0542, MAPE: 33.5789,Test Loss: 0.00010722\n",
      "=========epoch 246=========\n",
      "Iteraion 10, Train Loss: 0.00012945\n",
      "Iteraion 20, Train Loss: 0.00011348\n",
      "Iteraion 30, Train Loss: 0.00009631\n",
      "Epoch: 246, RMSE: 10.6931, MAE: 4.2011, MAPE: 34.9830,Train Loss: 0.00006944\n",
      "Epoch: 246, RMSE: 13.1813, MAE: 5.0450, MAPE: 33.7070,Test Loss: 0.00010458\n",
      "=========epoch 247=========\n",
      "Iteraion 10, Train Loss: 0.00010643\n",
      "Iteraion 20, Train Loss: 0.00008335\n",
      "Iteraion 30, Train Loss: 0.00008012\n",
      "Epoch: 247, RMSE: 10.5151, MAE: 4.1762, MAPE: 36.0230,Train Loss: 0.00006714\n",
      "Epoch: 247, RMSE: 13.0940, MAE: 5.0092, MAPE: 34.4079,Test Loss: 0.00010318\n",
      "=========epoch 248=========\n",
      "Iteraion 10, Train Loss: 0.00009630\n",
      "Iteraion 20, Train Loss: 0.00008084\n",
      "Iteraion 30, Train Loss: 0.00011648\n",
      "Epoch: 248, RMSE: 10.4527, MAE: 4.1311, MAPE: 36.0247,Train Loss: 0.00006634\n",
      "Epoch: 248, RMSE: 13.0176, MAE: 4.9546, MAPE: 34.1650,Test Loss: 0.00010197\n",
      "=========epoch 249=========\n",
      "Iteraion 10, Train Loss: 0.00010471\n",
      "Iteraion 20, Train Loss: 0.00009432\n",
      "Iteraion 30, Train Loss: 0.00010200\n",
      "Epoch: 249, RMSE: 10.5490, MAE: 4.1160, MAPE: 35.2334,Train Loss: 0.00006758\n",
      "Epoch: 249, RMSE: 13.2242, MAE: 4.9732, MAPE: 33.5711,Test Loss: 0.00010522\n",
      "=========epoch 250=========\n",
      "Iteraion 10, Train Loss: 0.00009954\n",
      "Iteraion 20, Train Loss: 0.00009533\n",
      "Iteraion 30, Train Loss: 0.00008193\n",
      "Epoch: 250, RMSE: 10.9471, MAE: 4.2884, MAPE: 33.6837,Train Loss: 0.00007277\n",
      "Epoch: 250, RMSE: 13.4829, MAE: 5.1564, MAPE: 32.5956,Test Loss: 0.00010935\n",
      "=========epoch 251=========\n",
      "Iteraion 10, Train Loss: 0.00009605\n",
      "Iteraion 20, Train Loss: 0.00009426\n",
      "Iteraion 30, Train Loss: 0.00009672\n",
      "Epoch: 251, RMSE: 10.3833, MAE: 4.0674, MAPE: 35.4437,Train Loss: 0.00006547\n",
      "Epoch: 251, RMSE: 13.0038, MAE: 4.9131, MAPE: 33.7827,Test Loss: 0.00010175\n",
      "=========epoch 252=========\n",
      "Iteraion 10, Train Loss: 0.00009199\n",
      "Iteraion 20, Train Loss: 0.00010339\n",
      "Iteraion 30, Train Loss: 0.00009090\n",
      "Epoch: 252, RMSE: 10.3140, MAE: 4.0650, MAPE: 35.3716,Train Loss: 0.00006460\n",
      "Epoch: 252, RMSE: 13.0262, MAE: 4.9313, MAPE: 33.7317,Test Loss: 0.00010211\n",
      "=========epoch 253=========\n",
      "Iteraion 10, Train Loss: 0.00008619\n",
      "Iteraion 20, Train Loss: 0.00009692\n",
      "Iteraion 30, Train Loss: 0.00008363\n",
      "Epoch: 253, RMSE: 10.2506, MAE: 4.0235, MAPE: 35.3300,Train Loss: 0.00006381\n",
      "Epoch: 253, RMSE: 12.8307, MAE: 4.8531, MAPE: 33.6756,Test Loss: 0.00009906\n",
      "=========epoch 254=========\n",
      "Iteraion 10, Train Loss: 0.00009540\n",
      "Iteraion 20, Train Loss: 0.00008019\n",
      "Iteraion 30, Train Loss: 0.00009703\n",
      "Epoch: 254, RMSE: 10.7851, MAE: 4.1580, MAPE: 34.7208,Train Loss: 0.00007063\n",
      "Epoch: 254, RMSE: 13.4176, MAE: 5.0390, MAPE: 33.3206,Test Loss: 0.00010830\n",
      "=========epoch 255=========\n",
      "Iteraion 10, Train Loss: 0.00010270\n",
      "Iteraion 20, Train Loss: 0.00009011\n",
      "Iteraion 30, Train Loss: 0.00010004\n",
      "Epoch: 255, RMSE: 10.6589, MAE: 4.1978, MAPE: 35.6283,Train Loss: 0.00006899\n",
      "Epoch: 255, RMSE: 13.1306, MAE: 4.9969, MAPE: 33.9880,Test Loss: 0.00010377\n",
      "=========epoch 256=========\n",
      "Iteraion 10, Train Loss: 0.00009791\n",
      "Iteraion 20, Train Loss: 0.00008890\n",
      "Iteraion 30, Train Loss: 0.00009469\n",
      "Epoch: 256, RMSE: 11.2445, MAE: 4.4529, MAPE: 37.9369,Train Loss: 0.00007679\n",
      "Epoch: 256, RMSE: 13.6696, MAE: 5.2133, MAPE: 35.6964,Test Loss: 0.00011252\n",
      "=========epoch 257=========\n",
      "Iteraion 10, Train Loss: 0.00009535\n",
      "Iteraion 20, Train Loss: 0.00008472\n",
      "Iteraion 30, Train Loss: 0.00007035\n",
      "Epoch: 257, RMSE: 10.2672, MAE: 4.0521, MAPE: 35.9913,Train Loss: 0.00006402\n",
      "Epoch: 257, RMSE: 12.8126, MAE: 4.8545, MAPE: 33.8896,Test Loss: 0.00009879\n",
      "=========epoch 258=========\n",
      "Iteraion 10, Train Loss: 0.00009178\n",
      "Iteraion 20, Train Loss: 0.00008341\n",
      "Iteraion 30, Train Loss: 0.00009505\n",
      "Epoch: 258, RMSE: 10.3681, MAE: 4.0576, MAPE: 33.9938,Train Loss: 0.00006528\n",
      "Epoch: 258, RMSE: 12.9695, MAE: 4.9337, MAPE: 32.7466,Test Loss: 0.00010122\n",
      "=========epoch 259=========\n",
      "Iteraion 10, Train Loss: 0.00008862\n",
      "Iteraion 20, Train Loss: 0.00010117\n",
      "Iteraion 30, Train Loss: 0.00008723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 259, RMSE: 10.6844, MAE: 4.1356, MAPE: 33.9688,Train Loss: 0.00006932\n",
      "Epoch: 259, RMSE: 13.2791, MAE: 4.9949, MAPE: 32.8070,Test Loss: 0.00010609\n",
      "=========epoch 260=========\n",
      "Iteraion 10, Train Loss: 0.00010137\n",
      "Iteraion 20, Train Loss: 0.00009663\n",
      "Iteraion 30, Train Loss: 0.00009266\n",
      "Epoch: 260, RMSE: 10.3359, MAE: 4.0780, MAPE: 36.2337,Train Loss: 0.00006488\n",
      "Epoch: 260, RMSE: 12.9809, MAE: 4.9105, MAPE: 34.2945,Test Loss: 0.00010138\n",
      "=========epoch 261=========\n",
      "Iteraion 10, Train Loss: 0.00007869\n",
      "Iteraion 20, Train Loss: 0.00009605\n",
      "Iteraion 30, Train Loss: 0.00008261\n",
      "Epoch: 261, RMSE: 10.2419, MAE: 4.0274, MAPE: 34.2463,Train Loss: 0.00006370\n",
      "Epoch: 261, RMSE: 12.8120, MAE: 4.8676, MAPE: 32.8806,Test Loss: 0.00009879\n",
      "=========epoch 262=========\n",
      "Iteraion 10, Train Loss: 0.00008305\n",
      "Iteraion 20, Train Loss: 0.00007321\n",
      "Iteraion 30, Train Loss: 0.00008093\n",
      "Epoch: 262, RMSE: 10.2499, MAE: 4.0295, MAPE: 34.5738,Train Loss: 0.00006380\n",
      "Epoch: 262, RMSE: 13.0764, MAE: 4.9372, MAPE: 33.1057,Test Loss: 0.00010289\n",
      "=========epoch 263=========\n",
      "Iteraion 10, Train Loss: 0.00008476\n",
      "Iteraion 20, Train Loss: 0.00008733\n",
      "Iteraion 30, Train Loss: 0.00007599\n",
      "Epoch: 263, RMSE: 10.4163, MAE: 4.0451, MAPE: 35.6056,Train Loss: 0.00006589\n",
      "Epoch: 263, RMSE: 13.0758, MAE: 4.8973, MAPE: 33.8143,Test Loss: 0.00010287\n",
      "=========epoch 264=========\n",
      "Iteraion 10, Train Loss: 0.00009607\n",
      "Iteraion 20, Train Loss: 0.00007524\n",
      "Iteraion 30, Train Loss: 0.00009993\n",
      "Epoch: 264, RMSE: 10.5205, MAE: 4.1159, MAPE: 34.4543,Train Loss: 0.00006721\n",
      "Epoch: 264, RMSE: 13.1166, MAE: 4.9822, MAPE: 32.9787,Test Loss: 0.00010352\n",
      "=========epoch 265=========\n",
      "Iteraion 10, Train Loss: 0.00008818\n",
      "Iteraion 20, Train Loss: 0.00009304\n",
      "Iteraion 30, Train Loss: 0.00008595\n",
      "Epoch: 265, RMSE: 10.2559, MAE: 4.0233, MAPE: 34.4360,Train Loss: 0.00006387\n",
      "Epoch: 265, RMSE: 12.9614, MAE: 4.9007, MAPE: 32.8790,Test Loss: 0.00010107\n",
      "=========epoch 266=========\n",
      "Iteraion 10, Train Loss: 0.00008821\n",
      "Iteraion 20, Train Loss: 0.00009138\n",
      "Iteraion 30, Train Loss: 0.00009510\n",
      "Epoch: 266, RMSE: 10.1795, MAE: 4.0318, MAPE: 35.2951,Train Loss: 0.00006292\n",
      "Epoch: 266, RMSE: 12.8634, MAE: 4.8698, MAPE: 33.6367,Test Loss: 0.00009956\n",
      "=========epoch 267=========\n",
      "Iteraion 10, Train Loss: 0.00008027\n",
      "Iteraion 20, Train Loss: 0.00009021\n",
      "Iteraion 30, Train Loss: 0.00010126\n",
      "Epoch: 267, RMSE: 10.6285, MAE: 4.2804, MAPE: 38.4945,Train Loss: 0.00006859\n",
      "Epoch: 267, RMSE: 13.2596, MAE: 5.0596, MAPE: 36.0419,Test Loss: 0.00010586\n",
      "=========epoch 268=========\n",
      "Iteraion 10, Train Loss: 0.00008087\n",
      "Iteraion 20, Train Loss: 0.00010008\n",
      "Iteraion 30, Train Loss: 0.00009471\n",
      "Epoch: 268, RMSE: 10.1445, MAE: 3.9948, MAPE: 34.5784,Train Loss: 0.00006249\n",
      "Epoch: 268, RMSE: 12.9109, MAE: 4.8730, MAPE: 33.1103,Test Loss: 0.00010031\n",
      "=========epoch 269=========\n",
      "Iteraion 10, Train Loss: 0.00010049\n",
      "Iteraion 20, Train Loss: 0.00009519\n",
      "Iteraion 30, Train Loss: 0.00008030\n",
      "Epoch: 269, RMSE: 10.0984, MAE: 3.9799, MAPE: 34.4789,Train Loss: 0.00006193\n",
      "Epoch: 269, RMSE: 12.7835, MAE: 4.8412, MAPE: 33.0717,Test Loss: 0.00009834\n",
      "=========epoch 270=========\n",
      "Iteraion 10, Train Loss: 0.00008300\n",
      "Iteraion 20, Train Loss: 0.00009123\n",
      "Iteraion 30, Train Loss: 0.00008375\n",
      "Epoch: 270, RMSE: 10.1647, MAE: 4.0241, MAPE: 35.4208,Train Loss: 0.00006274\n",
      "Epoch: 270, RMSE: 12.7748, MAE: 4.8341, MAPE: 33.7106,Test Loss: 0.00009823\n",
      "=========epoch 271=========\n",
      "Iteraion 10, Train Loss: 0.00009259\n",
      "Iteraion 20, Train Loss: 0.00008858\n",
      "Iteraion 30, Train Loss: 0.00010190\n",
      "Epoch: 271, RMSE: 10.2910, MAE: 4.0097, MAPE: 34.4280,Train Loss: 0.00006431\n",
      "Epoch: 271, RMSE: 12.9309, MAE: 4.8693, MAPE: 33.0060,Test Loss: 0.00010060\n",
      "=========epoch 272=========\n",
      "Iteraion 10, Train Loss: 0.00008039\n",
      "Iteraion 20, Train Loss: 0.00008033\n",
      "Iteraion 30, Train Loss: 0.00007314\n",
      "Epoch: 272, RMSE: 10.2539, MAE: 4.0802, MAPE: 36.0881,Train Loss: 0.00006385\n",
      "Epoch: 272, RMSE: 12.7320, MAE: 4.8580, MAPE: 33.8954,Test Loss: 0.00009760\n",
      "=========epoch 273=========\n",
      "Iteraion 10, Train Loss: 0.00009173\n",
      "Iteraion 20, Train Loss: 0.00010967\n",
      "Iteraion 30, Train Loss: 0.00009190\n",
      "Epoch: 273, RMSE: 10.1521, MAE: 4.0450, MAPE: 35.4332,Train Loss: 0.00006259\n",
      "Epoch: 273, RMSE: 12.7615, MAE: 4.8722, MAPE: 33.8364,Test Loss: 0.00009802\n",
      "=========epoch 274=========\n",
      "Iteraion 10, Train Loss: 0.00009467\n",
      "Iteraion 20, Train Loss: 0.00008883\n",
      "Iteraion 30, Train Loss: 0.00008672\n",
      "Epoch: 274, RMSE: 10.2638, MAE: 4.1136, MAPE: 37.6470,Train Loss: 0.00006398\n",
      "Epoch: 274, RMSE: 12.9405, MAE: 4.9144, MAPE: 35.3498,Test Loss: 0.00010083\n",
      "=========epoch 275=========\n",
      "Iteraion 10, Train Loss: 0.00008998\n",
      "Iteraion 20, Train Loss: 0.00008504\n",
      "Iteraion 30, Train Loss: 0.00010397\n",
      "Epoch: 275, RMSE: 10.0026, MAE: 3.9479, MAPE: 34.5519,Train Loss: 0.00006076\n",
      "Epoch: 275, RMSE: 12.7436, MAE: 4.8375, MAPE: 33.1785,Test Loss: 0.00009771\n",
      "=========epoch 276=========\n",
      "Iteraion 10, Train Loss: 0.00008849\n",
      "Iteraion 20, Train Loss: 0.00007829\n",
      "Iteraion 30, Train Loss: 0.00008563\n",
      "Epoch: 276, RMSE: 10.2015, MAE: 3.9960, MAPE: 33.8721,Train Loss: 0.00006320\n",
      "Epoch: 276, RMSE: 13.0077, MAE: 4.9031, MAPE: 32.7405,Test Loss: 0.00010179\n",
      "=========epoch 277=========\n",
      "Iteraion 10, Train Loss: 0.00008139\n",
      "Iteraion 20, Train Loss: 0.00009075\n",
      "Iteraion 30, Train Loss: 0.00010270\n",
      "Epoch: 277, RMSE: 9.9495, MAE: 3.9510, MAPE: 35.2794,Train Loss: 0.00006012\n",
      "Epoch: 277, RMSE: 12.6937, MAE: 4.8219, MAPE: 33.5692,Test Loss: 0.00009695\n",
      "=========epoch 278=========\n",
      "Iteraion 10, Train Loss: 0.00009898\n",
      "Iteraion 20, Train Loss: 0.00010308\n",
      "Iteraion 30, Train Loss: 0.00009918\n",
      "Epoch: 278, RMSE: 10.1378, MAE: 4.0399, MAPE: 35.6164,Train Loss: 0.00006241\n",
      "Epoch: 278, RMSE: 12.8881, MAE: 4.8770, MAPE: 33.5743,Test Loss: 0.00009999\n",
      "=========epoch 279=========\n",
      "Iteraion 10, Train Loss: 0.00008933\n",
      "Iteraion 20, Train Loss: 0.00008614\n",
      "Iteraion 30, Train Loss: 0.00007402\n",
      "Epoch: 279, RMSE: 10.5649, MAE: 4.3139, MAPE: 38.9072,Train Loss: 0.00006778\n",
      "Epoch: 279, RMSE: 13.1073, MAE: 5.0634, MAPE: 36.4072,Test Loss: 0.00010346\n",
      "=========epoch 280=========\n",
      "Iteraion 10, Train Loss: 0.00008170\n",
      "Iteraion 20, Train Loss: 0.00007793\n",
      "Iteraion 30, Train Loss: 0.00009924\n",
      "Epoch: 280, RMSE: 9.9605, MAE: 3.9378, MAPE: 34.7266,Train Loss: 0.00006025\n",
      "Epoch: 280, RMSE: 12.7396, MAE: 4.8249, MAPE: 33.1931,Test Loss: 0.00009767\n",
      "=========epoch 281=========\n",
      "Iteraion 10, Train Loss: 0.00007969\n",
      "Iteraion 20, Train Loss: 0.00008810\n",
      "Iteraion 30, Train Loss: 0.00007268\n",
      "Epoch: 281, RMSE: 10.0296, MAE: 3.9517, MAPE: 34.2660,Train Loss: 0.00006108\n",
      "Epoch: 281, RMSE: 12.7498, MAE: 4.8368, MAPE: 32.9287,Test Loss: 0.00009781\n",
      "=========epoch 282=========\n",
      "Iteraion 10, Train Loss: 0.00008480\n",
      "Iteraion 20, Train Loss: 0.00007848\n",
      "Iteraion 30, Train Loss: 0.00008789\n",
      "Epoch: 282, RMSE: 10.6390, MAE: 4.0851, MAPE: 33.6019,Train Loss: 0.00006874\n",
      "Epoch: 282, RMSE: 13.3960, MAE: 4.9938, MAPE: 32.6348,Test Loss: 0.00010798\n",
      "=========epoch 283=========\n",
      "Iteraion 10, Train Loss: 0.00008552\n",
      "Iteraion 20, Train Loss: 0.00008484\n",
      "Iteraion 30, Train Loss: 0.00009397\n",
      "Epoch: 283, RMSE: 9.8603, MAE: 3.9168, MAPE: 35.0713,Train Loss: 0.00005904\n",
      "Epoch: 283, RMSE: 12.6051, MAE: 4.7862, MAPE: 33.5572,Test Loss: 0.00009563\n",
      "=========epoch 284=========\n",
      "Iteraion 10, Train Loss: 0.00008368\n",
      "Iteraion 20, Train Loss: 0.00010082\n",
      "Iteraion 30, Train Loss: 0.00008123\n",
      "Epoch: 284, RMSE: 9.9735, MAE: 3.9685, MAPE: 35.4037,Train Loss: 0.00006040\n",
      "Epoch: 284, RMSE: 12.7507, MAE: 4.8354, MAPE: 33.8474,Test Loss: 0.00009783\n",
      "=========epoch 285=========\n",
      "Iteraion 10, Train Loss: 0.00008310\n",
      "Iteraion 20, Train Loss: 0.00008191\n",
      "Iteraion 30, Train Loss: 0.00009950\n",
      "Epoch: 285, RMSE: 9.9874, MAE: 4.0077, MAPE: 36.2642,Train Loss: 0.00006057\n",
      "Epoch: 285, RMSE: 12.8260, MAE: 4.8524, MAPE: 34.4015,Test Loss: 0.00009900\n",
      "=========epoch 286=========\n",
      "Iteraion 10, Train Loss: 0.00008445\n",
      "Iteraion 20, Train Loss: 0.00008606\n",
      "Iteraion 30, Train Loss: 0.00009737\n",
      "Epoch: 286, RMSE: 10.5510, MAE: 4.1258, MAPE: 33.3160,Train Loss: 0.00006760\n",
      "Epoch: 286, RMSE: 13.4460, MAE: 5.0793, MAPE: 32.4070,Test Loss: 0.00010877\n",
      "=========epoch 287=========\n",
      "Iteraion 10, Train Loss: 0.00008772\n",
      "Iteraion 20, Train Loss: 0.00007954\n",
      "Iteraion 30, Train Loss: 0.00010071\n",
      "Epoch: 287, RMSE: 10.8922, MAE: 4.2774, MAPE: 36.7309,Train Loss: 0.00007204\n",
      "Epoch: 287, RMSE: 13.4870, MAE: 5.0788, MAPE: 35.0006,Test Loss: 0.00010946\n",
      "=========epoch 288=========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraion 10, Train Loss: 0.00008970\n",
      "Iteraion 20, Train Loss: 0.00007830\n",
      "Iteraion 30, Train Loss: 0.00008240\n",
      "Epoch: 288, RMSE: 9.9436, MAE: 3.9340, MAPE: 35.0018,Train Loss: 0.00006005\n",
      "Epoch: 288, RMSE: 12.7128, MAE: 4.7842, MAPE: 33.5104,Test Loss: 0.00009728\n",
      "=========epoch 289=========\n",
      "Iteraion 10, Train Loss: 0.00008355\n",
      "Iteraion 20, Train Loss: 0.00007538\n",
      "Iteraion 30, Train Loss: 0.00009559\n",
      "Epoch: 289, RMSE: 9.8660, MAE: 3.9121, MAPE: 35.0386,Train Loss: 0.00005911\n",
      "Epoch: 289, RMSE: 12.5661, MAE: 4.7608, MAPE: 33.4986,Test Loss: 0.00009502\n",
      "=========epoch 290=========\n",
      "Iteraion 10, Train Loss: 0.00010877\n",
      "Iteraion 20, Train Loss: 0.00009094\n",
      "Iteraion 30, Train Loss: 0.00008168\n",
      "Epoch: 290, RMSE: 9.8199, MAE: 3.8776, MAPE: 34.2470,Train Loss: 0.00005856\n",
      "Epoch: 290, RMSE: 12.6660, MAE: 4.7700, MAPE: 32.8945,Test Loss: 0.00009653\n",
      "=========epoch 291=========\n",
      "Iteraion 10, Train Loss: 0.00007535\n",
      "Iteraion 20, Train Loss: 0.00010871\n",
      "Iteraion 30, Train Loss: 0.00008737\n",
      "Epoch: 291, RMSE: 9.9822, MAE: 3.9825, MAPE: 35.6955,Train Loss: 0.00006051\n",
      "Epoch: 291, RMSE: 12.6969, MAE: 4.8108, MAPE: 33.9117,Test Loss: 0.00009704\n",
      "=========epoch 292=========\n",
      "Iteraion 10, Train Loss: 0.00008751\n",
      "Iteraion 20, Train Loss: 0.00009349\n",
      "Iteraion 30, Train Loss: 0.00008410\n",
      "Epoch: 292, RMSE: 10.0461, MAE: 3.9446, MAPE: 34.5969,Train Loss: 0.00006129\n",
      "Epoch: 292, RMSE: 12.7342, MAE: 4.8092, MAPE: 33.1885,Test Loss: 0.00009756\n",
      "=========epoch 293=========\n",
      "Iteraion 10, Train Loss: 0.00006972\n",
      "Iteraion 20, Train Loss: 0.00009136\n",
      "Iteraion 30, Train Loss: 0.00009767\n",
      "Epoch: 293, RMSE: 10.0110, MAE: 3.9080, MAPE: 34.4003,Train Loss: 0.00006087\n",
      "Epoch: 293, RMSE: 12.8146, MAE: 4.7976, MAPE: 33.0779,Test Loss: 0.00009886\n",
      "=========epoch 294=========\n",
      "Iteraion 10, Train Loss: 0.00009029\n",
      "Iteraion 20, Train Loss: 0.00010515\n",
      "Iteraion 30, Train Loss: 0.00007578\n",
      "Epoch: 294, RMSE: 10.0487, MAE: 3.9685, MAPE: 35.5620,Train Loss: 0.00006132\n",
      "Epoch: 294, RMSE: 12.8142, MAE: 4.7988, MAPE: 33.7416,Test Loss: 0.00009883\n",
      "=========epoch 295=========\n",
      "Iteraion 10, Train Loss: 0.00009530\n",
      "Iteraion 20, Train Loss: 0.00009552\n",
      "Iteraion 30, Train Loss: 0.00007658\n",
      "Epoch: 295, RMSE: 11.2779, MAE: 4.5048, MAPE: 38.4129,Train Loss: 0.00007723\n",
      "Epoch: 295, RMSE: 13.9572, MAE: 5.3190, MAPE: 36.2476,Test Loss: 0.00011732\n",
      "=========epoch 296=========\n",
      "Iteraion 10, Train Loss: 0.00009376\n",
      "Iteraion 20, Train Loss: 0.00009554\n",
      "Iteraion 30, Train Loss: 0.00010861\n",
      "Epoch: 296, RMSE: 9.9318, MAE: 3.9373, MAPE: 34.4879,Train Loss: 0.00005991\n",
      "Epoch: 296, RMSE: 12.7143, MAE: 4.8187, MAPE: 33.1804,Test Loss: 0.00009733\n",
      "=========epoch 297=========\n",
      "Iteraion 10, Train Loss: 0.00009692\n",
      "Iteraion 20, Train Loss: 0.00007964\n",
      "Iteraion 30, Train Loss: 0.00009183\n",
      "Epoch: 297, RMSE: 10.1055, MAE: 4.0121, MAPE: 35.0996,Train Loss: 0.00006201\n",
      "Epoch: 297, RMSE: 12.8007, MAE: 4.8569, MAPE: 33.5780,Test Loss: 0.00009865\n",
      "=========epoch 298=========\n",
      "Iteraion 10, Train Loss: 0.00007601\n",
      "Iteraion 20, Train Loss: 0.00007717\n",
      "Iteraion 30, Train Loss: 0.00008053\n",
      "Epoch: 298, RMSE: 10.2744, MAE: 4.0011, MAPE: 33.7802,Train Loss: 0.00006412\n",
      "Epoch: 298, RMSE: 13.2298, MAE: 4.9417, MAPE: 32.8143,Test Loss: 0.00010539\n",
      "=========epoch 299=========\n",
      "Iteraion 10, Train Loss: 0.00007490\n",
      "Iteraion 20, Train Loss: 0.00009042\n",
      "Iteraion 30, Train Loss: 0.00008196\n",
      "Epoch: 299, RMSE: 10.0909, MAE: 3.9743, MAPE: 33.3507,Train Loss: 0.00006183\n",
      "Epoch: 299, RMSE: 12.9818, MAE: 4.9299, MAPE: 32.6098,Test Loss: 0.00010138\n",
      "=========epoch 300=========\n",
      "Iteraion 10, Train Loss: 0.00007941\n",
      "Iteraion 20, Train Loss: 0.00008017\n",
      "Iteraion 30, Train Loss: 0.00008409\n",
      "Epoch: 300, RMSE: 10.2571, MAE: 3.9738, MAPE: 33.9598,Train Loss: 0.00006389\n",
      "Epoch: 300, RMSE: 13.1887, MAE: 4.9051, MAPE: 32.8962,Test Loss: 0.00010467\n",
      "=========epoch 301=========\n",
      "Iteraion 10, Train Loss: 0.00007309\n",
      "Iteraion 20, Train Loss: 0.00008812\n",
      "Iteraion 30, Train Loss: 0.00008265\n",
      "Epoch: 301, RMSE: 9.8409, MAE: 3.8655, MAPE: 33.8783,Train Loss: 0.00005882\n",
      "Epoch: 301, RMSE: 12.8221, MAE: 4.7973, MAPE: 32.5624,Test Loss: 0.00009892\n",
      "=========epoch 302=========\n",
      "Iteraion 10, Train Loss: 0.00008379\n",
      "Iteraion 20, Train Loss: 0.00008239\n",
      "Iteraion 30, Train Loss: 0.00008306\n",
      "Epoch: 302, RMSE: 9.8686, MAE: 3.9070, MAPE: 33.6570,Train Loss: 0.00005914\n",
      "Epoch: 302, RMSE: 12.8531, MAE: 4.8729, MAPE: 32.8433,Test Loss: 0.00009940\n",
      "=========epoch 303=========\n",
      "Iteraion 10, Train Loss: 0.00007688\n",
      "Iteraion 20, Train Loss: 0.00008051\n",
      "Iteraion 30, Train Loss: 0.00007781\n",
      "Epoch: 303, RMSE: 10.9324, MAE: 4.1825, MAPE: 33.5165,Train Loss: 0.00007258\n",
      "Epoch: 303, RMSE: 13.8750, MAE: 5.1596, MAPE: 32.5356,Test Loss: 0.00011580\n",
      "=========epoch 304=========\n",
      "Iteraion 10, Train Loss: 0.00008098\n",
      "Iteraion 20, Train Loss: 0.00008692\n",
      "Iteraion 30, Train Loss: 0.00008032\n",
      "Epoch: 304, RMSE: 9.9963, MAE: 3.9317, MAPE: 33.1720,Train Loss: 0.00006068\n",
      "Epoch: 304, RMSE: 12.9839, MAE: 4.8940, MAPE: 32.3046,Test Loss: 0.00010142\n",
      "=========epoch 305=========\n",
      "Iteraion 10, Train Loss: 0.00009274\n",
      "Iteraion 20, Train Loss: 0.00008528\n",
      "Iteraion 30, Train Loss: 0.00007682\n",
      "Epoch: 305, RMSE: 9.6661, MAE: 3.8190, MAPE: 33.4213,Train Loss: 0.00005674\n",
      "Epoch: 305, RMSE: 12.5915, MAE: 4.7380, MAPE: 32.4059,Test Loss: 0.00009542\n",
      "=========epoch 306=========\n",
      "Iteraion 10, Train Loss: 0.00007807\n",
      "Iteraion 20, Train Loss: 0.00007939\n",
      "Iteraion 30, Train Loss: 0.00008927\n",
      "Epoch: 306, RMSE: 9.6907, MAE: 3.8574, MAPE: 35.0721,Train Loss: 0.00005704\n",
      "Epoch: 306, RMSE: 12.6452, MAE: 4.7460, MAPE: 33.5777,Test Loss: 0.00009622\n",
      "=========epoch 307=========\n",
      "Iteraion 10, Train Loss: 0.00008431\n",
      "Iteraion 20, Train Loss: 0.00010042\n",
      "Iteraion 30, Train Loss: 0.00009268\n",
      "Epoch: 307, RMSE: 9.9568, MAE: 3.9205, MAPE: 33.7035,Train Loss: 0.00006020\n",
      "Epoch: 307, RMSE: 12.8268, MAE: 4.8347, MAPE: 32.5121,Test Loss: 0.00009899\n",
      "=========epoch 308=========\n",
      "Iteraion 10, Train Loss: 0.00007697\n",
      "Iteraion 20, Train Loss: 0.00008058\n",
      "Iteraion 30, Train Loss: 0.00009115\n",
      "Epoch: 308, RMSE: 9.6569, MAE: 3.8462, MAPE: 35.1361,Train Loss: 0.00005663\n",
      "Epoch: 308, RMSE: 12.6327, MAE: 4.7513, MAPE: 33.6798,Test Loss: 0.00009604\n",
      "=========epoch 309=========\n",
      "Iteraion 10, Train Loss: 0.00007344\n",
      "Iteraion 20, Train Loss: 0.00008165\n",
      "Iteraion 30, Train Loss: 0.00008181\n",
      "Epoch: 309, RMSE: 9.7625, MAE: 3.8751, MAPE: 34.5595,Train Loss: 0.00005788\n",
      "Epoch: 309, RMSE: 12.5484, MAE: 4.7539, MAPE: 33.1592,Test Loss: 0.00009478\n",
      "=========epoch 310=========\n",
      "Iteraion 10, Train Loss: 0.00008061\n",
      "Iteraion 20, Train Loss: 0.00009039\n",
      "Iteraion 30, Train Loss: 0.00008766\n",
      "Epoch: 310, RMSE: 10.8022, MAE: 4.1521, MAPE: 33.1158,Train Loss: 0.00007087\n",
      "Epoch: 310, RMSE: 13.7176, MAE: 5.1129, MAPE: 32.3552,Test Loss: 0.00011320\n",
      "=========epoch 311=========\n",
      "Iteraion 10, Train Loss: 0.00008520\n",
      "Iteraion 20, Train Loss: 0.00007901\n",
      "Iteraion 30, Train Loss: 0.00009055\n",
      "Epoch: 311, RMSE: 9.7766, MAE: 3.8509, MAPE: 33.9568,Train Loss: 0.00005804\n",
      "Epoch: 311, RMSE: 12.6701, MAE: 4.7582, MAPE: 32.6230,Test Loss: 0.00009657\n",
      "=========epoch 312=========\n",
      "Iteraion 10, Train Loss: 0.00008492\n",
      "Iteraion 20, Train Loss: 0.00008389\n",
      "Iteraion 30, Train Loss: 0.00008825\n",
      "Epoch: 312, RMSE: 10.2003, MAE: 4.0226, MAPE: 34.9233,Train Loss: 0.00006318\n",
      "Epoch: 312, RMSE: 13.0427, MAE: 4.8971, MAPE: 33.5554,Test Loss: 0.00010240\n",
      "=========epoch 313=========\n",
      "Iteraion 10, Train Loss: 0.00009938\n",
      "Iteraion 20, Train Loss: 0.00008123\n",
      "Iteraion 30, Train Loss: 0.00010196\n",
      "Epoch: 313, RMSE: 9.9873, MAE: 3.9777, MAPE: 35.3616,Train Loss: 0.00006057\n",
      "Epoch: 313, RMSE: 12.9216, MAE: 4.8499, MAPE: 33.8434,Test Loss: 0.00010050\n",
      "=========epoch 314=========\n",
      "Iteraion 10, Train Loss: 0.00007997\n",
      "Iteraion 20, Train Loss: 0.00008053\n",
      "Iteraion 30, Train Loss: 0.00007259\n",
      "Epoch: 314, RMSE: 9.6926, MAE: 3.8603, MAPE: 34.6440,Train Loss: 0.00005706\n",
      "Epoch: 314, RMSE: 12.7662, MAE: 4.7983, MAPE: 33.4264,Test Loss: 0.00009807\n",
      "=========epoch 315=========\n",
      "Iteraion 10, Train Loss: 0.00008116\n",
      "Iteraion 20, Train Loss: 0.00007616\n",
      "Iteraion 30, Train Loss: 0.00009487\n",
      "Epoch: 315, RMSE: 10.0938, MAE: 3.9172, MAPE: 33.6804,Train Loss: 0.00006188\n",
      "Epoch: 315, RMSE: 13.1078, MAE: 4.8736, MAPE: 32.7048,Test Loss: 0.00010340\n",
      "=========epoch 316=========\n",
      "Iteraion 10, Train Loss: 0.00008331\n",
      "Iteraion 20, Train Loss: 0.00007643\n",
      "Iteraion 30, Train Loss: 0.00008632\n",
      "Epoch: 316, RMSE: 10.0850, MAE: 4.0001, MAPE: 35.2333,Train Loss: 0.00006176\n",
      "Epoch: 316, RMSE: 13.0593, MAE: 4.9108, MAPE: 33.9118,Test Loss: 0.00010273\n",
      "=========epoch 317=========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraion 10, Train Loss: 0.00009362\n",
      "Iteraion 20, Train Loss: 0.00007678\n",
      "Iteraion 30, Train Loss: 0.00006695\n",
      "Epoch: 317, RMSE: 9.8997, MAE: 3.8829, MAPE: 33.9631,Train Loss: 0.00005952\n",
      "Epoch: 317, RMSE: 12.9169, MAE: 4.8381, MAPE: 32.8473,Test Loss: 0.00010040\n",
      "=========epoch 318=========\n",
      "Iteraion 10, Train Loss: 0.00008666\n",
      "Iteraion 20, Train Loss: 0.00009908\n",
      "Iteraion 30, Train Loss: 0.00007784\n",
      "Epoch: 318, RMSE: 9.5753, MAE: 3.8199, MAPE: 34.4991,Train Loss: 0.00005568\n",
      "Epoch: 318, RMSE: 12.5858, MAE: 4.7545, MAPE: 33.3809,Test Loss: 0.00009531\n",
      "=========epoch 319=========\n",
      "Iteraion 10, Train Loss: 0.00008273\n",
      "Iteraion 20, Train Loss: 0.00008698\n",
      "Iteraion 30, Train Loss: 0.00007251\n",
      "Epoch: 319, RMSE: 9.7125, MAE: 3.8280, MAPE: 33.3886,Train Loss: 0.00005729\n",
      "Epoch: 319, RMSE: 12.7004, MAE: 4.7705, MAPE: 32.3974,Test Loss: 0.00009708\n",
      "=========epoch 320=========\n",
      "Iteraion 10, Train Loss: 0.00008727\n",
      "Iteraion 20, Train Loss: 0.00007575\n",
      "Iteraion 30, Train Loss: 0.00009303\n",
      "Epoch: 320, RMSE: 9.4824, MAE: 3.7743, MAPE: 34.2425,Train Loss: 0.00005460\n",
      "Epoch: 320, RMSE: 12.5414, MAE: 4.7138, MAPE: 33.1580,Test Loss: 0.00009463\n",
      "=========epoch 321=========\n",
      "Iteraion 10, Train Loss: 0.00008946\n",
      "Iteraion 20, Train Loss: 0.00007808\n",
      "Iteraion 30, Train Loss: 0.00008159\n",
      "Epoch: 321, RMSE: 9.7497, MAE: 3.8610, MAPE: 33.1583,Train Loss: 0.00005773\n",
      "Epoch: 321, RMSE: 12.8759, MAE: 4.8533, MAPE: 32.5745,Test Loss: 0.00009978\n",
      "=========epoch 322=========\n",
      "Iteraion 10, Train Loss: 0.00008005\n",
      "Iteraion 20, Train Loss: 0.00008463\n",
      "Iteraion 30, Train Loss: 0.00008452\n",
      "Epoch: 322, RMSE: 9.6070, MAE: 3.8183, MAPE: 33.8855,Train Loss: 0.00005605\n",
      "Epoch: 322, RMSE: 12.7685, MAE: 4.7833, MAPE: 32.8767,Test Loss: 0.00009814\n",
      "=========epoch 323=========\n",
      "Iteraion 10, Train Loss: 0.00008051\n",
      "Iteraion 20, Train Loss: 0.00009371\n",
      "Iteraion 30, Train Loss: 0.00009243\n",
      "Epoch: 323, RMSE: 9.5592, MAE: 3.7879, MAPE: 33.8617,Train Loss: 0.00005549\n",
      "Epoch: 323, RMSE: 12.7360, MAE: 4.7802, MAPE: 32.9254,Test Loss: 0.00009762\n",
      "=========epoch 324=========\n",
      "Iteraion 10, Train Loss: 0.00007908\n",
      "Iteraion 20, Train Loss: 0.00005922\n",
      "Iteraion 30, Train Loss: 0.00007559\n",
      "Epoch: 324, RMSE: 9.5774, MAE: 3.7953, MAPE: 34.4453,Train Loss: 0.00005571\n",
      "Epoch: 324, RMSE: 12.6038, MAE: 4.7191, MAPE: 33.1944,Test Loss: 0.00009560\n",
      "=========epoch 325=========\n",
      "Iteraion 10, Train Loss: 0.00007897\n",
      "Iteraion 20, Train Loss: 0.00007963\n",
      "Iteraion 30, Train Loss: 0.00008568\n",
      "Epoch: 325, RMSE: 9.9594, MAE: 3.9480, MAPE: 33.0530,Train Loss: 0.00006024\n",
      "Epoch: 325, RMSE: 12.9436, MAE: 4.8952, MAPE: 32.4517,Test Loss: 0.00010083\n",
      "=========epoch 326=========\n",
      "Iteraion 10, Train Loss: 0.00008813\n",
      "Iteraion 20, Train Loss: 0.00008219\n",
      "Iteraion 30, Train Loss: 0.00008978\n",
      "Epoch: 326, RMSE: 9.6174, MAE: 3.8116, MAPE: 33.0505,Train Loss: 0.00005617\n",
      "Epoch: 326, RMSE: 12.8228, MAE: 4.8112, MAPE: 32.3559,Test Loss: 0.00009891\n",
      "=========epoch 327=========\n",
      "Iteraion 10, Train Loss: 0.00007967\n",
      "Iteraion 20, Train Loss: 0.00007622\n",
      "Iteraion 30, Train Loss: 0.00007674\n",
      "Epoch: 327, RMSE: 9.5310, MAE: 3.8055, MAPE: 33.8854,Train Loss: 0.00005516\n",
      "Epoch: 327, RMSE: 12.5097, MAE: 4.7263, MAPE: 32.8547,Test Loss: 0.00009417\n",
      "=========epoch 328=========\n",
      "Iteraion 10, Train Loss: 0.00007373\n",
      "Iteraion 20, Train Loss: 0.00008811\n",
      "Iteraion 30, Train Loss: 0.00008254\n",
      "Epoch: 328, RMSE: 9.4591, MAE: 3.7539, MAPE: 32.7265,Train Loss: 0.00005434\n",
      "Epoch: 328, RMSE: 12.6221, MAE: 4.7406, MAPE: 32.1176,Test Loss: 0.00009589\n",
      "=========epoch 329=========\n",
      "Iteraion 10, Train Loss: 0.00007605\n",
      "Iteraion 20, Train Loss: 0.00007705\n",
      "Iteraion 30, Train Loss: 0.00008101\n",
      "Epoch: 329, RMSE: 9.3224, MAE: 3.7016, MAPE: 33.4552,Train Loss: 0.00005278\n",
      "Epoch: 329, RMSE: 12.6152, MAE: 4.7055, MAPE: 32.5830,Test Loss: 0.00009580\n",
      "=========epoch 330=========\n",
      "Iteraion 10, Train Loss: 0.00007390\n",
      "Iteraion 20, Train Loss: 0.00008869\n",
      "Iteraion 30, Train Loss: 0.00008362\n",
      "Epoch: 330, RMSE: 9.3097, MAE: 3.7035, MAPE: 33.7154,Train Loss: 0.00005263\n",
      "Epoch: 330, RMSE: 12.4503, MAE: 4.6732, MAPE: 32.6498,Test Loss: 0.00009328\n",
      "=========epoch 331=========\n",
      "Iteraion 10, Train Loss: 0.00007320\n",
      "Iteraion 20, Train Loss: 0.00008405\n",
      "Iteraion 30, Train Loss: 0.00008863\n",
      "Epoch: 331, RMSE: 9.3327, MAE: 3.7151, MAPE: 33.6260,Train Loss: 0.00005290\n",
      "Epoch: 331, RMSE: 12.4438, MAE: 4.6653, MAPE: 32.7672,Test Loss: 0.00009317\n",
      "=========epoch 332=========\n",
      "Iteraion 10, Train Loss: 0.00008532\n",
      "Iteraion 20, Train Loss: 0.00008852\n",
      "Iteraion 30, Train Loss: 0.00007769\n",
      "Epoch: 332, RMSE: 9.9303, MAE: 3.8652, MAPE: 33.1428,Train Loss: 0.00005988\n",
      "Epoch: 332, RMSE: 13.0632, MAE: 4.8662, MAPE: 32.4518,Test Loss: 0.00010269\n",
      "=========epoch 333=========\n",
      "Iteraion 10, Train Loss: 0.00007788\n",
      "Iteraion 20, Train Loss: 0.00007378\n",
      "Iteraion 30, Train Loss: 0.00008469\n",
      "Epoch: 333, RMSE: 9.7708, MAE: 3.8243, MAPE: 33.4094,Train Loss: 0.00005797\n",
      "Epoch: 333, RMSE: 13.0002, MAE: 4.8413, MAPE: 32.5276,Test Loss: 0.00010170\n",
      "=========epoch 334=========\n",
      "Iteraion 10, Train Loss: 0.00008541\n",
      "Iteraion 20, Train Loss: 0.00007375\n",
      "Iteraion 30, Train Loss: 0.00008444\n",
      "Epoch: 334, RMSE: 9.4026, MAE: 3.7610, MAPE: 34.4013,Train Loss: 0.00005369\n",
      "Epoch: 334, RMSE: 12.4784, MAE: 4.6936, MAPE: 33.0898,Test Loss: 0.00009372\n",
      "=========epoch 335=========\n",
      "Iteraion 10, Train Loss: 0.00006808\n",
      "Iteraion 20, Train Loss: 0.00007691\n",
      "Iteraion 30, Train Loss: 0.00007385\n",
      "Epoch: 335, RMSE: 9.3953, MAE: 3.7404, MAPE: 33.9450,Train Loss: 0.00005361\n",
      "Epoch: 335, RMSE: 12.5807, MAE: 4.7324, MAPE: 33.0784,Test Loss: 0.00009524\n",
      "=========epoch 336=========\n",
      "Iteraion 10, Train Loss: 0.00007418\n",
      "Iteraion 20, Train Loss: 0.00007632\n",
      "Iteraion 30, Train Loss: 0.00008235\n",
      "Epoch: 336, RMSE: 9.3132, MAE: 3.7052, MAPE: 33.5289,Train Loss: 0.00005268\n",
      "Epoch: 336, RMSE: 12.5144, MAE: 4.6857, MAPE: 32.6675,Test Loss: 0.00009425\n",
      "=========epoch 337=========\n",
      "Iteraion 10, Train Loss: 0.00007556\n",
      "Iteraion 20, Train Loss: 0.00006235\n",
      "Iteraion 30, Train Loss: 0.00008165\n",
      "Epoch: 337, RMSE: 9.6705, MAE: 3.8066, MAPE: 33.0217,Train Loss: 0.00005679\n",
      "Epoch: 337, RMSE: 12.8160, MAE: 4.7882, MAPE: 32.3923,Test Loss: 0.00009883\n",
      "=========epoch 338=========\n",
      "Iteraion 10, Train Loss: 0.00007563\n",
      "Iteraion 20, Train Loss: 0.00007208\n",
      "Iteraion 30, Train Loss: 0.00008241\n",
      "Epoch: 338, RMSE: 9.5564, MAE: 3.7575, MAPE: 33.5244,Train Loss: 0.00005546\n",
      "Epoch: 338, RMSE: 12.7329, MAE: 4.7203, MAPE: 32.5822,Test Loss: 0.00009758\n",
      "=========epoch 339=========\n",
      "Iteraion 10, Train Loss: 0.00007264\n",
      "Iteraion 20, Train Loss: 0.00008335\n",
      "Iteraion 30, Train Loss: 0.00008075\n",
      "Epoch: 339, RMSE: 9.4211, MAE: 3.8080, MAPE: 35.6816,Train Loss: 0.00005391\n",
      "Epoch: 339, RMSE: 12.5410, MAE: 4.7395, MAPE: 34.2510,Test Loss: 0.00009465\n",
      "=========epoch 340=========\n",
      "Iteraion 10, Train Loss: 0.00007798\n",
      "Iteraion 20, Train Loss: 0.00009196\n",
      "Iteraion 30, Train Loss: 0.00009466\n",
      "Epoch: 340, RMSE: 9.4272, MAE: 3.7673, MAPE: 34.3346,Train Loss: 0.00005397\n",
      "Epoch: 340, RMSE: 12.4991, MAE: 4.6964, MAPE: 33.1085,Test Loss: 0.00009403\n",
      "=========epoch 341=========\n",
      "Iteraion 10, Train Loss: 0.00009313\n",
      "Iteraion 20, Train Loss: 0.00007751\n",
      "Iteraion 30, Train Loss: 0.00008537\n",
      "Epoch: 341, RMSE: 9.7379, MAE: 3.9285, MAPE: 36.4224,Train Loss: 0.00005759\n",
      "Epoch: 341, RMSE: 12.8297, MAE: 4.8350, MAPE: 34.7414,Test Loss: 0.00009909\n",
      "=========epoch 342=========\n",
      "Iteraion 10, Train Loss: 0.00009059\n",
      "Iteraion 20, Train Loss: 0.00009707\n",
      "Iteraion 30, Train Loss: 0.00007704\n",
      "Epoch: 342, RMSE: 10.1776, MAE: 4.0203, MAPE: 32.7385,Train Loss: 0.00006290\n",
      "Epoch: 342, RMSE: 13.3529, MAE: 5.0595, MAPE: 32.6401,Test Loss: 0.00010726\n",
      "=========epoch 343=========\n",
      "Iteraion 10, Train Loss: 0.00009277\n",
      "Iteraion 20, Train Loss: 0.00007195\n",
      "Iteraion 30, Train Loss: 0.00007710\n",
      "Epoch: 343, RMSE: 9.4236, MAE: 3.8020, MAPE: 35.2561,Train Loss: 0.00005393\n",
      "Epoch: 343, RMSE: 12.6738, MAE: 4.7591, MAPE: 33.8584,Test Loss: 0.00009671\n",
      "=========epoch 344=========\n",
      "Iteraion 10, Train Loss: 0.00009246\n",
      "Iteraion 20, Train Loss: 0.00008471\n",
      "Iteraion 30, Train Loss: 0.00007118\n",
      "Epoch: 344, RMSE: 10.3868, MAE: 4.0048, MAPE: 33.1153,Train Loss: 0.00006552\n",
      "Epoch: 344, RMSE: 13.5246, MAE: 5.0203, MAPE: 32.5703,Test Loss: 0.00011007\n",
      "=========epoch 345=========\n",
      "Iteraion 10, Train Loss: 0.00009498\n",
      "Iteraion 20, Train Loss: 0.00008402\n",
      "Iteraion 30, Train Loss: 0.00007564\n",
      "Epoch: 345, RMSE: 9.8181, MAE: 3.9860, MAPE: 37.1891,Train Loss: 0.00005855\n",
      "Epoch: 345, RMSE: 12.7724, MAE: 4.8371, MAPE: 35.1849,Test Loss: 0.00009823\n",
      "=========epoch 346=========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraion 10, Train Loss: 0.00007554\n",
      "Iteraion 20, Train Loss: 0.00008669\n",
      "Iteraion 30, Train Loss: 0.00008770\n",
      "Epoch: 346, RMSE: 9.5005, MAE: 3.7840, MAPE: 32.4669,Train Loss: 0.00005482\n",
      "Epoch: 346, RMSE: 12.7888, MAE: 4.8151, MAPE: 32.1182,Test Loss: 0.00009839\n",
      "=========epoch 347=========\n",
      "Iteraion 10, Train Loss: 0.00008476\n",
      "Iteraion 20, Train Loss: 0.00008016\n",
      "Iteraion 30, Train Loss: 0.00008043\n",
      "Epoch: 347, RMSE: 9.6780, MAE: 3.8668, MAPE: 34.5976,Train Loss: 0.00005688\n",
      "Epoch: 347, RMSE: 12.8226, MAE: 4.7905, MAPE: 33.3942,Test Loss: 0.00009900\n",
      "=========epoch 348=========\n",
      "Iteraion 10, Train Loss: 0.00007724\n",
      "Iteraion 20, Train Loss: 0.00008418\n",
      "Iteraion 30, Train Loss: 0.00008544\n",
      "Epoch: 348, RMSE: 9.3898, MAE: 3.7341, MAPE: 34.0929,Train Loss: 0.00005355\n",
      "Epoch: 348, RMSE: 12.6697, MAE: 4.7132, MAPE: 33.0986,Test Loss: 0.00009664\n",
      "=========epoch 349=========\n",
      "Iteraion 10, Train Loss: 0.00008564\n",
      "Iteraion 20, Train Loss: 0.00007342\n",
      "Iteraion 30, Train Loss: 0.00006535\n",
      "Epoch: 349, RMSE: 9.2133, MAE: 3.6896, MAPE: 33.0752,Train Loss: 0.00005155\n",
      "Epoch: 349, RMSE: 12.5192, MAE: 4.6908, MAPE: 32.4455,Test Loss: 0.00009434\n",
      "=========epoch 350=========\n",
      "Iteraion 10, Train Loss: 0.00008846\n",
      "Iteraion 20, Train Loss: 0.00007692\n",
      "Iteraion 30, Train Loss: 0.00007409\n",
      "Epoch: 350, RMSE: 9.3958, MAE: 3.7107, MAPE: 34.4362,Train Loss: 0.00005363\n",
      "Epoch: 350, RMSE: 12.6570, MAE: 4.6974, MAPE: 33.4346,Test Loss: 0.00009643\n",
      "=========epoch 351=========\n",
      "Iteraion 10, Train Loss: 0.00007305\n",
      "Iteraion 20, Train Loss: 0.00007538\n",
      "Iteraion 30, Train Loss: 0.00007655\n",
      "Epoch: 351, RMSE: 9.2273, MAE: 3.6894, MAPE: 33.9704,Train Loss: 0.00005171\n",
      "Epoch: 351, RMSE: 12.5077, MAE: 4.6707, MAPE: 32.9844,Test Loss: 0.00009414\n",
      "=========epoch 352=========\n",
      "Iteraion 10, Train Loss: 0.00008734\n",
      "Iteraion 20, Train Loss: 0.00009234\n",
      "Iteraion 30, Train Loss: 0.00007882\n",
      "Epoch: 352, RMSE: 9.6173, MAE: 3.9101, MAPE: 36.3792,Train Loss: 0.00005618\n",
      "Epoch: 352, RMSE: 12.7412, MAE: 4.8338, MAPE: 34.8785,Test Loss: 0.00009772\n",
      "=========epoch 353=========\n",
      "Iteraion 10, Train Loss: 0.00007982\n",
      "Iteraion 20, Train Loss: 0.00006880\n",
      "Iteraion 30, Train Loss: 0.00007897\n",
      "Epoch: 353, RMSE: 9.1654, MAE: 3.6636, MAPE: 33.3308,Train Loss: 0.00005101\n",
      "Epoch: 353, RMSE: 12.3713, MAE: 4.6484, MAPE: 32.6413,Test Loss: 0.00009211\n",
      "=========epoch 354=========\n",
      "Iteraion 10, Train Loss: 0.00007760\n",
      "Iteraion 20, Train Loss: 0.00008117\n",
      "Iteraion 30, Train Loss: 0.00006931\n",
      "Epoch: 354, RMSE: 9.3432, MAE: 3.7256, MAPE: 33.9962,Train Loss: 0.00005302\n",
      "Epoch: 354, RMSE: 12.6365, MAE: 4.7157, MAPE: 33.1207,Test Loss: 0.00009609\n",
      "=========epoch 355=========\n",
      "Iteraion 10, Train Loss: 0.00008581\n",
      "Iteraion 20, Train Loss: 0.00007722\n",
      "Iteraion 30, Train Loss: 0.00007250\n",
      "Epoch: 355, RMSE: 9.2831, MAE: 3.7432, MAPE: 35.3620,Train Loss: 0.00005233\n",
      "Epoch: 355, RMSE: 12.4192, MAE: 4.6865, MAPE: 34.1739,Test Loss: 0.00009284\n",
      "=========epoch 356=========\n",
      "Iteraion 10, Train Loss: 0.00008877\n",
      "Iteraion 20, Train Loss: 0.00007493\n",
      "Iteraion 30, Train Loss: 0.00007781\n",
      "Epoch: 356, RMSE: 9.3805, MAE: 3.7236, MAPE: 33.8956,Train Loss: 0.00005344\n",
      "Epoch: 356, RMSE: 12.6427, MAE: 4.7194, MAPE: 33.1783,Test Loss: 0.00009621\n",
      "=========epoch 357=========\n",
      "Iteraion 10, Train Loss: 0.00007997\n",
      "Iteraion 20, Train Loss: 0.00007683\n",
      "Iteraion 30, Train Loss: 0.00007979\n",
      "Epoch: 357, RMSE: 9.3318, MAE: 3.7276, MAPE: 32.8390,Train Loss: 0.00005289\n",
      "Epoch: 357, RMSE: 12.6792, MAE: 4.7772, MAPE: 32.4123,Test Loss: 0.00009673\n",
      "=========epoch 358=========\n",
      "Iteraion 10, Train Loss: 0.00007738\n",
      "Iteraion 20, Train Loss: 0.00008644\n",
      "Iteraion 30, Train Loss: 0.00006267\n",
      "Epoch: 358, RMSE: 9.1004, MAE: 3.6339, MAPE: 33.2322,Train Loss: 0.00005030\n",
      "Epoch: 358, RMSE: 12.4441, MAE: 4.6583, MAPE: 32.5561,Test Loss: 0.00009320\n",
      "=========epoch 359=========\n",
      "Iteraion 10, Train Loss: 0.00008642\n",
      "Iteraion 20, Train Loss: 0.00007555\n",
      "Iteraion 30, Train Loss: 0.00008318\n",
      "Epoch: 359, RMSE: 9.3649, MAE: 3.8223, MAPE: 36.3955,Train Loss: 0.00005327\n",
      "Epoch: 359, RMSE: 12.6557, MAE: 4.8003, MAPE: 35.1147,Test Loss: 0.00009644\n",
      "=========epoch 360=========\n",
      "Iteraion 10, Train Loss: 0.00007368\n",
      "Iteraion 20, Train Loss: 0.00007085\n",
      "Iteraion 30, Train Loss: 0.00008211\n",
      "Epoch: 360, RMSE: 9.4670, MAE: 3.7252, MAPE: 33.7806,Train Loss: 0.00005444\n",
      "Epoch: 360, RMSE: 12.7642, MAE: 4.7555, MAPE: 33.0425,Test Loss: 0.00009808\n",
      "=========epoch 361=========\n",
      "Iteraion 10, Train Loss: 0.00006386\n",
      "Iteraion 20, Train Loss: 0.00008245\n",
      "Iteraion 30, Train Loss: 0.00009864\n",
      "Epoch: 361, RMSE: 9.5318, MAE: 3.8220, MAPE: 35.0705,Train Loss: 0.00005518\n",
      "Epoch: 361, RMSE: 12.9077, MAE: 4.8257, MAPE: 33.9555,Test Loss: 0.00010032\n",
      "=========epoch 362=========\n",
      "Iteraion 10, Train Loss: 0.00008027\n",
      "Iteraion 20, Train Loss: 0.00008626\n",
      "Iteraion 30, Train Loss: 0.00007621\n",
      "Epoch: 362, RMSE: 9.4562, MAE: 3.7185, MAPE: 33.2804,Train Loss: 0.00005431\n",
      "Epoch: 362, RMSE: 12.7915, MAE: 4.7371, MAPE: 32.6025,Test Loss: 0.00009846\n",
      "=========epoch 363=========\n",
      "Iteraion 10, Train Loss: 0.00009539\n",
      "Iteraion 20, Train Loss: 0.00006451\n",
      "Iteraion 30, Train Loss: 0.00007661\n",
      "Epoch: 363, RMSE: 9.3142, MAE: 3.7807, MAPE: 35.7897,Train Loss: 0.00005269\n",
      "Epoch: 363, RMSE: 12.4547, MAE: 4.7022, MAPE: 34.3066,Test Loss: 0.00009340\n",
      "=========epoch 364=========\n",
      "Iteraion 10, Train Loss: 0.00007998\n",
      "Iteraion 20, Train Loss: 0.00007527\n",
      "Iteraion 30, Train Loss: 0.00007348\n",
      "Epoch: 364, RMSE: 9.2831, MAE: 3.7696, MAPE: 35.7790,Train Loss: 0.00005234\n",
      "Epoch: 364, RMSE: 12.5482, MAE: 4.7295, MAPE: 34.4672,Test Loss: 0.00009479\n",
      "=========epoch 365=========\n",
      "Iteraion 10, Train Loss: 0.00006925\n",
      "Iteraion 20, Train Loss: 0.00006944\n",
      "Iteraion 30, Train Loss: 0.00007419\n",
      "Epoch: 365, RMSE: 9.3039, MAE: 3.7012, MAPE: 33.7936,Train Loss: 0.00005257\n",
      "Epoch: 365, RMSE: 12.7194, MAE: 4.7290, MAPE: 32.9461,Test Loss: 0.00009740\n",
      "=========epoch 366=========\n",
      "Iteraion 10, Train Loss: 0.00007626\n",
      "Iteraion 20, Train Loss: 0.00007540\n",
      "Iteraion 30, Train Loss: 0.00007289\n",
      "Epoch: 366, RMSE: 9.2319, MAE: 3.6869, MAPE: 32.8838,Train Loss: 0.00005176\n",
      "Epoch: 366, RMSE: 12.5404, MAE: 4.6981, MAPE: 32.3462,Test Loss: 0.00009464\n",
      "=========epoch 367=========\n",
      "Iteraion 10, Train Loss: 0.00007669\n",
      "Iteraion 20, Train Loss: 0.00007957\n",
      "Iteraion 30, Train Loss: 0.00008024\n",
      "Epoch: 367, RMSE: 9.0659, MAE: 3.6332, MAPE: 33.2668,Train Loss: 0.00004992\n",
      "Epoch: 367, RMSE: 12.4578, MAE: 4.6595, MAPE: 32.6349,Test Loss: 0.00009341\n",
      "=========epoch 368=========\n",
      "Iteraion 10, Train Loss: 0.00008502\n",
      "Iteraion 20, Train Loss: 0.00007718\n",
      "Iteraion 30, Train Loss: 0.00007527\n",
      "Epoch: 368, RMSE: 9.6771, MAE: 3.7749, MAPE: 33.1050,Train Loss: 0.00005688\n",
      "Epoch: 368, RMSE: 13.1402, MAE: 4.8249, MAPE: 32.4279,Test Loss: 0.00010397\n",
      "=========epoch 369=========\n",
      "Iteraion 10, Train Loss: 0.00007376\n",
      "Iteraion 20, Train Loss: 0.00007737\n",
      "Iteraion 30, Train Loss: 0.00007858\n",
      "Epoch: 369, RMSE: 9.1449, MAE: 3.6538, MAPE: 33.6298,Train Loss: 0.00005080\n",
      "Epoch: 369, RMSE: 12.5292, MAE: 4.6936, MAPE: 33.0246,Test Loss: 0.00009448\n",
      "=========epoch 370=========\n",
      "Iteraion 10, Train Loss: 0.00008457\n",
      "Iteraion 20, Train Loss: 0.00007847\n",
      "Iteraion 30, Train Loss: 0.00008192\n",
      "Epoch: 370, RMSE: 9.1327, MAE: 3.6433, MAPE: 33.1805,Train Loss: 0.00005066\n",
      "Epoch: 370, RMSE: 12.5610, MAE: 4.6963, MAPE: 32.7873,Test Loss: 0.00009495\n",
      "=========epoch 371=========\n",
      "Iteraion 10, Train Loss: 0.00007685\n",
      "Iteraion 20, Train Loss: 0.00007775\n",
      "Iteraion 30, Train Loss: 0.00007232\n",
      "Epoch: 371, RMSE: 9.1903, MAE: 3.6643, MAPE: 33.2599,Train Loss: 0.00005130\n",
      "Epoch: 371, RMSE: 12.6486, MAE: 4.7227, MAPE: 32.7063,Test Loss: 0.00009633\n",
      "=========epoch 372=========\n",
      "Iteraion 10, Train Loss: 0.00006481\n",
      "Iteraion 20, Train Loss: 0.00007266\n",
      "Iteraion 30, Train Loss: 0.00006343\n",
      "Epoch: 372, RMSE: 9.1340, MAE: 3.6546, MAPE: 33.1643,Train Loss: 0.00005067\n",
      "Epoch: 372, RMSE: 12.4649, MAE: 4.6697, MAPE: 32.5868,Test Loss: 0.00009353\n",
      "=========epoch 373=========\n",
      "Iteraion 10, Train Loss: 0.00008176\n",
      "Iteraion 20, Train Loss: 0.00007726\n",
      "Iteraion 30, Train Loss: 0.00008739\n",
      "Epoch: 373, RMSE: 8.9587, MAE: 3.5981, MAPE: 33.4540,Train Loss: 0.00004874\n",
      "Epoch: 373, RMSE: 12.3491, MAE: 4.6115, MAPE: 32.7712,Test Loss: 0.00009178\n",
      "=========epoch 374=========\n",
      "Iteraion 10, Train Loss: 0.00007936\n",
      "Iteraion 20, Train Loss: 0.00007934\n",
      "Iteraion 30, Train Loss: 0.00007176\n",
      "Epoch: 374, RMSE: 9.3221, MAE: 3.7108, MAPE: 33.2699,Train Loss: 0.00005277\n",
      "Epoch: 374, RMSE: 12.7114, MAE: 4.7526, MAPE: 32.6820,Test Loss: 0.00009723\n",
      "=========epoch 375=========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraion 10, Train Loss: 0.00007363\n",
      "Iteraion 20, Train Loss: 0.00008283\n",
      "Iteraion 30, Train Loss: 0.00007802\n",
      "Epoch: 375, RMSE: 9.0887, MAE: 3.6503, MAPE: 34.0605,Train Loss: 0.00005017\n",
      "Epoch: 375, RMSE: 12.6684, MAE: 4.7055, MAPE: 33.3657,Test Loss: 0.00009661\n",
      "=========epoch 376=========\n",
      "Iteraion 10, Train Loss: 0.00007777\n",
      "Iteraion 20, Train Loss: 0.00007822\n",
      "Iteraion 30, Train Loss: 0.00007247\n",
      "Epoch: 376, RMSE: 9.6760, MAE: 3.8823, MAPE: 35.2180,Train Loss: 0.00005686\n",
      "Epoch: 376, RMSE: 13.0286, MAE: 4.8548, MAPE: 34.0092,Test Loss: 0.00010221\n",
      "=========epoch 377=========\n",
      "Iteraion 10, Train Loss: 0.00007006\n",
      "Iteraion 20, Train Loss: 0.00007590\n",
      "Iteraion 30, Train Loss: 0.00007040\n",
      "Epoch: 377, RMSE: 9.5708, MAE: 3.7650, MAPE: 32.7421,Train Loss: 0.00005563\n",
      "Epoch: 377, RMSE: 12.9921, MAE: 4.8283, MAPE: 32.4330,Test Loss: 0.00010158\n",
      "=========epoch 378=========\n",
      "Iteraion 10, Train Loss: 0.00007676\n",
      "Iteraion 20, Train Loss: 0.00008483\n",
      "Iteraion 30, Train Loss: 0.00007464\n",
      "Epoch: 378, RMSE: 9.2173, MAE: 3.6724, MAPE: 33.1176,Train Loss: 0.00005160\n",
      "Epoch: 378, RMSE: 12.5740, MAE: 4.7163, MAPE: 32.9693,Test Loss: 0.00009520\n",
      "=========epoch 379=========\n",
      "Iteraion 10, Train Loss: 0.00007693\n",
      "Iteraion 20, Train Loss: 0.00007103\n",
      "Iteraion 30, Train Loss: 0.00006691\n",
      "Epoch: 379, RMSE: 9.0251, MAE: 3.6501, MAPE: 34.7452,Train Loss: 0.00004947\n",
      "Epoch: 379, RMSE: 12.5169, MAE: 4.6654, MAPE: 33.9744,Test Loss: 0.00009431\n",
      "=========epoch 380=========\n",
      "Iteraion 10, Train Loss: 0.00006353\n",
      "Iteraion 20, Train Loss: 0.00007489\n",
      "Iteraion 30, Train Loss: 0.00007977\n",
      "Epoch: 380, RMSE: 9.5170, MAE: 3.7285, MAPE: 33.7246,Train Loss: 0.00005501\n",
      "Epoch: 380, RMSE: 13.0342, MAE: 4.7942, MAPE: 33.0914,Test Loss: 0.00010225\n",
      "=========epoch 381=========\n",
      "Iteraion 10, Train Loss: 0.00007153\n",
      "Iteraion 20, Train Loss: 0.00007723\n",
      "Iteraion 30, Train Loss: 0.00007932\n",
      "Epoch: 381, RMSE: 9.2284, MAE: 3.7351, MAPE: 34.8126,Train Loss: 0.00005172\n",
      "Epoch: 381, RMSE: 12.7225, MAE: 4.7371, MAPE: 33.8792,Test Loss: 0.00009744\n",
      "=========epoch 382=========\n",
      "Iteraion 10, Train Loss: 0.00007833\n",
      "Iteraion 20, Train Loss: 0.00007168\n",
      "Iteraion 30, Train Loss: 0.00006714\n",
      "Epoch: 382, RMSE: 9.1015, MAE: 3.7154, MAPE: 35.7035,Train Loss: 0.00005031\n",
      "Epoch: 382, RMSE: 12.5560, MAE: 4.7223, MAPE: 34.6112,Test Loss: 0.00009492\n",
      "=========epoch 383=========\n",
      "Iteraion 10, Train Loss: 0.00008022\n",
      "Iteraion 20, Train Loss: 0.00007954\n",
      "Iteraion 30, Train Loss: 0.00007679\n",
      "Epoch: 383, RMSE: 9.2958, MAE: 3.6939, MAPE: 32.3417,Train Loss: 0.00005248\n",
      "Epoch: 383, RMSE: 12.8579, MAE: 4.7916, MAPE: 32.2671,Test Loss: 0.00009949\n",
      "=========epoch 384=========\n",
      "Iteraion 10, Train Loss: 0.00006514\n",
      "Iteraion 20, Train Loss: 0.00008083\n",
      "Iteraion 30, Train Loss: 0.00008674\n",
      "Epoch: 384, RMSE: 9.0228, MAE: 3.5995, MAPE: 32.6497,Train Loss: 0.00004945\n",
      "Epoch: 384, RMSE: 12.4153, MAE: 4.6328, MAPE: 32.3101,Test Loss: 0.00009282\n",
      "=========epoch 385=========\n",
      "Iteraion 10, Train Loss: 0.00007426\n",
      "Iteraion 20, Train Loss: 0.00007264\n",
      "Iteraion 30, Train Loss: 0.00007097\n",
      "Epoch: 385, RMSE: 8.8601, MAE: 3.5658, MAPE: 33.5811,Train Loss: 0.00004767\n",
      "Epoch: 385, RMSE: 12.2756, MAE: 4.5763, MAPE: 32.8518,Test Loss: 0.00009070\n",
      "=========epoch 386=========\n",
      "Iteraion 10, Train Loss: 0.00007239\n",
      "Iteraion 20, Train Loss: 0.00006449\n",
      "Iteraion 30, Train Loss: 0.00007578\n",
      "Epoch: 386, RMSE: 9.0127, MAE: 3.5844, MAPE: 33.1717,Train Loss: 0.00004933\n",
      "Epoch: 386, RMSE: 12.5444, MAE: 4.6311, MAPE: 32.5746,Test Loss: 0.00009470\n",
      "=========epoch 387=========\n",
      "Iteraion 10, Train Loss: 0.00008095\n",
      "Iteraion 20, Train Loss: 0.00007369\n",
      "Iteraion 30, Train Loss: 0.00006934\n",
      "Epoch: 387, RMSE: 9.1227, MAE: 3.6275, MAPE: 32.6996,Train Loss: 0.00005055\n",
      "Epoch: 387, RMSE: 12.5820, MAE: 4.6903, MAPE: 32.5263,Test Loss: 0.00009530\n",
      "=========epoch 388=========\n",
      "Iteraion 10, Train Loss: 0.00007254\n",
      "Iteraion 20, Train Loss: 0.00007553\n",
      "Iteraion 30, Train Loss: 0.00007702\n",
      "Epoch: 388, RMSE: 8.8886, MAE: 3.5666, MAPE: 32.8683,Train Loss: 0.00004798\n",
      "Epoch: 388, RMSE: 12.4635, MAE: 4.6345, MAPE: 32.4093,Test Loss: 0.00009350\n",
      "=========epoch 389=========\n",
      "Iteraion 10, Train Loss: 0.00006783\n",
      "Iteraion 20, Train Loss: 0.00006674\n",
      "Iteraion 30, Train Loss: 0.00007365\n",
      "Epoch: 389, RMSE: 9.0171, MAE: 3.6402, MAPE: 33.8634,Train Loss: 0.00004938\n",
      "Epoch: 389, RMSE: 12.4994, MAE: 4.6708, MAPE: 33.1659,Test Loss: 0.00009406\n",
      "=========epoch 390=========\n",
      "Iteraion 10, Train Loss: 0.00006856\n",
      "Iteraion 20, Train Loss: 0.00008535\n",
      "Iteraion 30, Train Loss: 0.00008243\n",
      "Epoch: 390, RMSE: 9.2631, MAE: 3.6633, MAPE: 32.9330,Train Loss: 0.00005211\n",
      "Epoch: 390, RMSE: 12.7629, MAE: 4.7303, MAPE: 32.5658,Test Loss: 0.00009805\n",
      "=========epoch 391=========\n",
      "Iteraion 10, Train Loss: 0.00009319\n",
      "Iteraion 20, Train Loss: 0.00007668\n",
      "Iteraion 30, Train Loss: 0.00006690\n",
      "Epoch: 391, RMSE: 9.0625, MAE: 3.6347, MAPE: 32.6667,Train Loss: 0.00004988\n",
      "Epoch: 391, RMSE: 12.5745, MAE: 4.6956, MAPE: 32.3557,Test Loss: 0.00009517\n",
      "=========epoch 392=========\n",
      "Iteraion 10, Train Loss: 0.00007431\n",
      "Iteraion 20, Train Loss: 0.00008134\n",
      "Iteraion 30, Train Loss: 0.00007842\n",
      "Epoch: 392, RMSE: 9.0470, MAE: 3.6341, MAPE: 33.9524,Train Loss: 0.00004971\n",
      "Epoch: 392, RMSE: 12.5327, MAE: 4.6556, MAPE: 33.2232,Test Loss: 0.00009454\n",
      "=========epoch 393=========\n",
      "Iteraion 10, Train Loss: 0.00007339\n",
      "Iteraion 20, Train Loss: 0.00007029\n",
      "Iteraion 30, Train Loss: 0.00007724\n",
      "Epoch: 393, RMSE: 9.0207, MAE: 3.5994, MAPE: 32.8374,Train Loss: 0.00004942\n",
      "Epoch: 393, RMSE: 12.6335, MAE: 4.6885, MAPE: 32.6587,Test Loss: 0.00009610\n",
      "=========epoch 394=========\n",
      "Iteraion 10, Train Loss: 0.00007519\n",
      "Iteraion 20, Train Loss: 0.00006777\n",
      "Iteraion 30, Train Loss: 0.00006957\n",
      "Epoch: 394, RMSE: 8.9201, MAE: 3.5763, MAPE: 32.7769,Train Loss: 0.00004832\n",
      "Epoch: 394, RMSE: 12.4529, MAE: 4.6412, MAPE: 32.3831,Test Loss: 0.00009334\n",
      "=========epoch 395=========\n",
      "Iteraion 10, Train Loss: 0.00007525\n",
      "Iteraion 20, Train Loss: 0.00007150\n",
      "Iteraion 30, Train Loss: 0.00006280\n",
      "Epoch: 395, RMSE: 9.1730, MAE: 3.6828, MAPE: 32.8786,Train Loss: 0.00005111\n",
      "Epoch: 395, RMSE: 12.7642, MAE: 4.7701, MAPE: 32.7093,Test Loss: 0.00009808\n",
      "=========epoch 396=========\n",
      "Iteraion 10, Train Loss: 0.00006791\n",
      "Iteraion 20, Train Loss: 0.00007387\n",
      "Iteraion 30, Train Loss: 0.00008200\n",
      "Epoch: 396, RMSE: 8.9184, MAE: 3.5859, MAPE: 33.0082,Train Loss: 0.00004831\n",
      "Epoch: 396, RMSE: 12.5018, MAE: 4.6494, MAPE: 32.5563,Test Loss: 0.00009406\n",
      "=========epoch 397=========\n",
      "Iteraion 10, Train Loss: 0.00007675\n",
      "Iteraion 20, Train Loss: 0.00006806\n",
      "Iteraion 30, Train Loss: 0.00007224\n",
      "Epoch: 397, RMSE: 8.9300, MAE: 3.5629, MAPE: 32.7295,Train Loss: 0.00004844\n",
      "Epoch: 397, RMSE: 12.5080, MAE: 4.6397, MAPE: 32.4746,Test Loss: 0.00009417\n",
      "=========epoch 398=========\n",
      "Iteraion 10, Train Loss: 0.00007719\n",
      "Iteraion 20, Train Loss: 0.00006581\n",
      "Iteraion 30, Train Loss: 0.00007196\n",
      "Epoch: 398, RMSE: 8.9991, MAE: 3.6338, MAPE: 34.2239,Train Loss: 0.00004918\n",
      "Epoch: 398, RMSE: 12.4673, MAE: 4.6704, MAPE: 33.5298,Test Loss: 0.00009356\n",
      "=========epoch 399=========\n",
      "Iteraion 10, Train Loss: 0.00008672\n",
      "Iteraion 20, Train Loss: 0.00006732\n",
      "Iteraion 30, Train Loss: 0.00007108\n",
      "Epoch: 399, RMSE: 8.7906, MAE: 3.5252, MAPE: 32.8787,Train Loss: 0.00004693\n",
      "Epoch: 399, RMSE: 12.4548, MAE: 4.6212, MAPE: 32.5777,Test Loss: 0.00009335\n",
      "=========epoch 400=========\n",
      "Iteraion 10, Train Loss: 0.00006490\n",
      "Iteraion 20, Train Loss: 0.00006928\n",
      "Iteraion 30, Train Loss: 0.00007070\n",
      "Epoch: 400, RMSE: 8.8372, MAE: 3.5551, MAPE: 33.2218,Train Loss: 0.00004743\n",
      "Epoch: 400, RMSE: 12.4903, MAE: 4.6161, MAPE: 32.7337,Test Loss: 0.00009393\n",
      "=========epoch 401=========\n",
      "Iteraion 10, Train Loss: 0.00006406\n",
      "Iteraion 20, Train Loss: 0.00007649\n",
      "Iteraion 30, Train Loss: 0.00007089\n",
      "Epoch: 401, RMSE: 8.7095, MAE: 3.5202, MAPE: 33.6273,Train Loss: 0.00004607\n",
      "Epoch: 401, RMSE: 12.3453, MAE: 4.5924, MAPE: 32.9450,Test Loss: 0.00009175\n",
      "=========epoch 402=========\n",
      "Iteraion 10, Train Loss: 0.00007020\n",
      "Iteraion 20, Train Loss: 0.00007141\n",
      "Iteraion 30, Train Loss: 0.00008312\n",
      "Epoch: 402, RMSE: 9.2861, MAE: 3.6792, MAPE: 32.3104,Train Loss: 0.00005238\n",
      "Epoch: 402, RMSE: 12.8138, MAE: 4.7575, MAPE: 32.3523,Test Loss: 0.00009885\n",
      "=========epoch 403=========\n",
      "Iteraion 10, Train Loss: 0.00007671\n",
      "Iteraion 20, Train Loss: 0.00007017\n",
      "Iteraion 30, Train Loss: 0.00006620\n",
      "Epoch: 403, RMSE: 8.8622, MAE: 3.5672, MAPE: 33.0022,Train Loss: 0.00004770\n",
      "Epoch: 403, RMSE: 12.4986, MAE: 4.6440, MAPE: 32.5744,Test Loss: 0.00009404\n",
      "=========epoch 404=========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraion 10, Train Loss: 0.00006494\n",
      "Iteraion 20, Train Loss: 0.00007320\n",
      "Iteraion 30, Train Loss: 0.00007245\n",
      "Epoch: 404, RMSE: 8.9847, MAE: 3.5600, MAPE: 32.4015,Train Loss: 0.00004903\n",
      "Epoch: 404, RMSE: 12.5761, MAE: 4.6550, MAPE: 32.3048,Test Loss: 0.00009518\n",
      "=========epoch 405=========\n",
      "Iteraion 10, Train Loss: 0.00007193\n",
      "Iteraion 20, Train Loss: 0.00006967\n",
      "Iteraion 30, Train Loss: 0.00007320\n",
      "Epoch: 405, RMSE: 8.8766, MAE: 3.5649, MAPE: 33.6466,Train Loss: 0.00004786\n",
      "Epoch: 405, RMSE: 12.4503, MAE: 4.6224, MAPE: 33.2057,Test Loss: 0.00009331\n",
      "=========epoch 406=========\n",
      "Iteraion 10, Train Loss: 0.00007347\n",
      "Iteraion 20, Train Loss: 0.00007157\n",
      "Iteraion 30, Train Loss: 0.00007488\n",
      "Epoch: 406, RMSE: 8.7768, MAE: 3.5600, MAPE: 34.4984,Train Loss: 0.00004679\n",
      "Epoch: 406, RMSE: 12.4517, MAE: 4.6173, MAPE: 33.8130,Test Loss: 0.00009336\n",
      "=========epoch 407=========\n",
      "Iteraion 10, Train Loss: 0.00008268\n",
      "Iteraion 20, Train Loss: 0.00006290\n",
      "Iteraion 30, Train Loss: 0.00007313\n",
      "Epoch: 407, RMSE: 8.9765, MAE: 3.6099, MAPE: 32.3088,Train Loss: 0.00004894\n",
      "Epoch: 407, RMSE: 12.6159, MAE: 4.7132, MAPE: 32.3843,Test Loss: 0.00009580\n",
      "=========epoch 408=========\n",
      "Iteraion 10, Train Loss: 0.00006922\n",
      "Iteraion 20, Train Loss: 0.00006593\n",
      "Iteraion 30, Train Loss: 0.00007758\n",
      "Epoch: 408, RMSE: 8.8333, MAE: 3.5382, MAPE: 32.8228,Train Loss: 0.00004739\n",
      "Epoch: 408, RMSE: 12.5137, MAE: 4.6468, MAPE: 32.6209,Test Loss: 0.00009424\n",
      "=========epoch 409=========\n",
      "Iteraion 10, Train Loss: 0.00007326\n",
      "Iteraion 20, Train Loss: 0.00007184\n",
      "Iteraion 30, Train Loss: 0.00007241\n",
      "Epoch: 409, RMSE: 8.7475, MAE: 3.5224, MAPE: 32.8372,Train Loss: 0.00004647\n",
      "Epoch: 409, RMSE: 12.5647, MAE: 4.6603, MAPE: 32.4805,Test Loss: 0.00009500\n",
      "=========epoch 410=========\n",
      "Iteraion 10, Train Loss: 0.00007386\n",
      "Iteraion 20, Train Loss: 0.00008363\n",
      "Iteraion 30, Train Loss: 0.00006992\n",
      "Epoch: 410, RMSE: 8.8257, MAE: 3.5443, MAPE: 33.2678,Train Loss: 0.00004731\n",
      "Epoch: 410, RMSE: 12.4932, MAE: 4.6288, MAPE: 32.9346,Test Loss: 0.00009395\n",
      "=========epoch 411=========\n",
      "Iteraion 10, Train Loss: 0.00008114\n",
      "Iteraion 20, Train Loss: 0.00007969\n",
      "Iteraion 30, Train Loss: 0.00006633\n",
      "Epoch: 411, RMSE: 8.9123, MAE: 3.5816, MAPE: 32.2567,Train Loss: 0.00004824\n",
      "Epoch: 411, RMSE: 12.7085, MAE: 4.7388, MAPE: 32.3234,Test Loss: 0.00009718\n",
      "=========epoch 412=========\n",
      "Iteraion 10, Train Loss: 0.00006239\n",
      "Iteraion 20, Train Loss: 0.00007004\n",
      "Iteraion 30, Train Loss: 0.00006645\n",
      "Epoch: 412, RMSE: 8.8225, MAE: 3.5554, MAPE: 33.1815,Train Loss: 0.00004728\n",
      "Epoch: 412, RMSE: 12.5546, MAE: 4.6481, MAPE: 32.7726,Test Loss: 0.00009486\n",
      "=========epoch 413=========\n",
      "Iteraion 10, Train Loss: 0.00006759\n",
      "Iteraion 20, Train Loss: 0.00007341\n",
      "Iteraion 30, Train Loss: 0.00007903\n",
      "Epoch: 413, RMSE: 9.8953, MAE: 4.0637, MAPE: 36.6943,Train Loss: 0.00005947\n",
      "Epoch: 413, RMSE: 13.2759, MAE: 5.0265, MAPE: 35.5727,Test Loss: 0.00010615\n",
      "=========epoch 414=========\n",
      "Iteraion 10, Train Loss: 0.00006894\n",
      "Iteraion 20, Train Loss: 0.00006998\n",
      "Iteraion 30, Train Loss: 0.00007003\n",
      "Epoch: 414, RMSE: 8.8507, MAE: 3.5562, MAPE: 32.4486,Train Loss: 0.00004758\n",
      "Epoch: 414, RMSE: 12.6168, MAE: 4.6917, MAPE: 32.3326,Test Loss: 0.00009580\n",
      "=========epoch 415=========\n",
      "Iteraion 10, Train Loss: 0.00007930\n",
      "Iteraion 20, Train Loss: 0.00008065\n",
      "Iteraion 30, Train Loss: 0.00006779\n",
      "Epoch: 415, RMSE: 8.9882, MAE: 3.5908, MAPE: 32.5510,Train Loss: 0.00004906\n",
      "Epoch: 415, RMSE: 12.7104, MAE: 4.7030, MAPE: 32.3372,Test Loss: 0.00009721\n",
      "=========epoch 416=========\n",
      "Iteraion 10, Train Loss: 0.00007164\n",
      "Iteraion 20, Train Loss: 0.00006923\n",
      "Iteraion 30, Train Loss: 0.00007192\n",
      "Epoch: 416, RMSE: 8.9511, MAE: 3.5755, MAPE: 32.5452,Train Loss: 0.00004866\n",
      "Epoch: 416, RMSE: 12.6663, MAE: 4.6872, MAPE: 32.4582,Test Loss: 0.00009661\n",
      "=========epoch 417=========\n",
      "Iteraion 10, Train Loss: 0.00007758\n",
      "Iteraion 20, Train Loss: 0.00006668\n",
      "Iteraion 30, Train Loss: 0.00007252\n",
      "Epoch: 417, RMSE: 8.6734, MAE: 3.5271, MAPE: 33.2444,Train Loss: 0.00004569\n",
      "Epoch: 417, RMSE: 12.4520, MAE: 4.6347, MAPE: 33.0121,Test Loss: 0.00009333\n",
      "=========epoch 418=========\n",
      "Iteraion 10, Train Loss: 0.00008583\n",
      "Iteraion 20, Train Loss: 0.00008065\n",
      "Iteraion 30, Train Loss: 0.00007249\n",
      "Epoch: 418, RMSE: 8.8543, MAE: 3.5595, MAPE: 32.5564,Train Loss: 0.00004762\n",
      "Epoch: 418, RMSE: 12.7036, MAE: 4.6995, MAPE: 32.5726,Test Loss: 0.00009713\n",
      "=========epoch 419=========\n",
      "Iteraion 10, Train Loss: 0.00006532\n",
      "Iteraion 20, Train Loss: 0.00005604\n",
      "Iteraion 30, Train Loss: 0.00006716\n",
      "Epoch: 419, RMSE: 8.7342, MAE: 3.5173, MAPE: 32.2748,Train Loss: 0.00004633\n",
      "Epoch: 419, RMSE: 12.5637, MAE: 4.6705, MAPE: 32.2528,Test Loss: 0.00009497\n",
      "=========epoch 420=========\n",
      "Iteraion 10, Train Loss: 0.00006847\n",
      "Iteraion 20, Train Loss: 0.00006691\n",
      "Iteraion 30, Train Loss: 0.00006160\n",
      "Epoch: 420, RMSE: 8.6872, MAE: 3.5127, MAPE: 32.7835,Train Loss: 0.00004583\n",
      "Epoch: 420, RMSE: 12.4464, MAE: 4.6239, MAPE: 32.5931,Test Loss: 0.00009325\n",
      "=========epoch 421=========\n",
      "Iteraion 10, Train Loss: 0.00007470\n",
      "Iteraion 20, Train Loss: 0.00006593\n",
      "Iteraion 30, Train Loss: 0.00006340\n",
      "Epoch: 421, RMSE: 9.1474, MAE: 3.7260, MAPE: 32.2357,Train Loss: 0.00005082\n",
      "Epoch: 421, RMSE: 12.9661, MAE: 4.9147, MAPE: 32.5127,Test Loss: 0.00010117\n",
      "=========epoch 422=========\n",
      "Iteraion 10, Train Loss: 0.00007247\n",
      "Iteraion 20, Train Loss: 0.00007052\n",
      "Iteraion 30, Train Loss: 0.00007276\n",
      "Epoch: 422, RMSE: 8.8845, MAE: 3.5788, MAPE: 32.6096,Train Loss: 0.00004794\n",
      "Epoch: 422, RMSE: 12.7282, MAE: 4.7434, MAPE: 32.6648,Test Loss: 0.00009752\n",
      "=========epoch 423=========\n",
      "Iteraion 10, Train Loss: 0.00006060\n",
      "Iteraion 20, Train Loss: 0.00007941\n",
      "Iteraion 30, Train Loss: 0.00006897\n",
      "Epoch: 423, RMSE: 8.7393, MAE: 3.5381, MAPE: 33.0272,Train Loss: 0.00004639\n",
      "Epoch: 423, RMSE: 12.5283, MAE: 4.6598, MAPE: 32.9471,Test Loss: 0.00009449\n",
      "=========epoch 424=========\n",
      "Iteraion 10, Train Loss: 0.00006622\n",
      "Iteraion 20, Train Loss: 0.00007241\n",
      "Iteraion 30, Train Loss: 0.00007014\n",
      "Epoch: 424, RMSE: 8.5700, MAE: 3.4567, MAPE: 32.8532,Train Loss: 0.00004461\n",
      "Epoch: 424, RMSE: 12.4571, MAE: 4.6065, MAPE: 32.6204,Test Loss: 0.00009339\n",
      "=========epoch 425=========\n",
      "Iteraion 10, Train Loss: 0.00006099\n",
      "Iteraion 20, Train Loss: 0.00006942\n",
      "Iteraion 30, Train Loss: 0.00007110\n",
      "Epoch: 425, RMSE: 8.6915, MAE: 3.5200, MAPE: 32.2882,Train Loss: 0.00004588\n",
      "Epoch: 425, RMSE: 12.5804, MAE: 4.6923, MAPE: 32.3580,Test Loss: 0.00009523\n",
      "=========epoch 426=========\n",
      "Iteraion 10, Train Loss: 0.00007232\n",
      "Iteraion 20, Train Loss: 0.00007699\n",
      "Iteraion 30, Train Loss: 0.00006043\n",
      "Epoch: 426, RMSE: 8.6505, MAE: 3.5262, MAPE: 33.3801,Train Loss: 0.00004544\n",
      "Epoch: 426, RMSE: 12.4538, MAE: 4.6265, MAPE: 32.9623,Test Loss: 0.00009337\n",
      "=========epoch 427=========\n",
      "Iteraion 10, Train Loss: 0.00006929\n",
      "Iteraion 20, Train Loss: 0.00006666\n",
      "Iteraion 30, Train Loss: 0.00007032\n",
      "Epoch: 427, RMSE: 8.6474, MAE: 3.4972, MAPE: 32.8270,Train Loss: 0.00004542\n",
      "Epoch: 427, RMSE: 12.5200, MAE: 4.6539, MAPE: 32.8834,Test Loss: 0.00009435\n",
      "=========epoch 428=========\n",
      "Iteraion 10, Train Loss: 0.00006656\n",
      "Iteraion 20, Train Loss: 0.00007326\n",
      "Iteraion 30, Train Loss: 0.00007759\n",
      "Epoch: 428, RMSE: 8.6714, MAE: 3.4961, MAPE: 32.2087,Train Loss: 0.00004567\n",
      "Epoch: 428, RMSE: 12.4366, MAE: 4.6295, MAPE: 32.2254,Test Loss: 0.00009306\n",
      "=========epoch 429=========\n",
      "Iteraion 10, Train Loss: 0.00007003\n",
      "Iteraion 20, Train Loss: 0.00006257\n",
      "Iteraion 30, Train Loss: 0.00006689\n",
      "Epoch: 429, RMSE: 8.5815, MAE: 3.4650, MAPE: 32.8625,Train Loss: 0.00004473\n",
      "Epoch: 429, RMSE: 12.4418, MAE: 4.5823, MAPE: 32.6738,Test Loss: 0.00009318\n",
      "=========epoch 430=========\n",
      "Iteraion 10, Train Loss: 0.00006628\n",
      "Iteraion 20, Train Loss: 0.00006680\n",
      "Iteraion 30, Train Loss: 0.00007919\n",
      "Epoch: 430, RMSE: 8.4933, MAE: 3.4441, MAPE: 32.4932,Train Loss: 0.00004382\n",
      "Epoch: 430, RMSE: 12.4066, MAE: 4.5954, MAPE: 32.4287,Test Loss: 0.00009263\n",
      "=========epoch 431=========\n",
      "Iteraion 10, Train Loss: 0.00007016\n",
      "Iteraion 20, Train Loss: 0.00006709\n",
      "Iteraion 30, Train Loss: 0.00006707\n",
      "Epoch: 431, RMSE: 8.4955, MAE: 3.4484, MAPE: 32.8643,Train Loss: 0.00004384\n",
      "Epoch: 431, RMSE: 12.3819, MAE: 4.6010, MAPE: 32.8258,Test Loss: 0.00009228\n",
      "=========epoch 432=========\n",
      "Iteraion 10, Train Loss: 0.00007836\n",
      "Iteraion 20, Train Loss: 0.00005964\n",
      "Iteraion 30, Train Loss: 0.00006972\n",
      "Epoch: 432, RMSE: 8.5327, MAE: 3.4577, MAPE: 33.1427,Train Loss: 0.00004422\n",
      "Epoch: 432, RMSE: 12.4960, MAE: 4.6326, MAPE: 33.0922,Test Loss: 0.00009396\n",
      "=========epoch 433=========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraion 10, Train Loss: 0.00007072\n",
      "Iteraion 20, Train Loss: 0.00006963\n",
      "Iteraion 30, Train Loss: 0.00006610\n",
      "Epoch: 433, RMSE: 10.1307, MAE: 3.9198, MAPE: 32.5073,Train Loss: 0.00006232\n",
      "Epoch: 433, RMSE: 13.7935, MAE: 5.0448, MAPE: 32.6117,Test Loss: 0.00011444\n",
      "=========epoch 434=========\n",
      "Iteraion 10, Train Loss: 0.00007095\n",
      "Iteraion 20, Train Loss: 0.00007235\n",
      "Iteraion 30, Train Loss: 0.00007590\n",
      "Epoch: 434, RMSE: 9.0095, MAE: 3.5892, MAPE: 32.7378,Train Loss: 0.00004930\n",
      "Epoch: 434, RMSE: 12.8739, MAE: 4.7321, MAPE: 32.8168,Test Loss: 0.00009972\n",
      "=========epoch 435=========\n",
      "Iteraion 10, Train Loss: 0.00007204\n",
      "Iteraion 20, Train Loss: 0.00006726\n",
      "Iteraion 30, Train Loss: 0.00007391\n",
      "Epoch: 435, RMSE: 8.6221, MAE: 3.4776, MAPE: 32.4851,Train Loss: 0.00004515\n",
      "Epoch: 435, RMSE: 12.5847, MAE: 4.6566, MAPE: 32.5576,Test Loss: 0.00009536\n",
      "=========epoch 436=========\n",
      "Iteraion 10, Train Loss: 0.00006721\n",
      "Iteraion 20, Train Loss: 0.00007166\n",
      "Iteraion 30, Train Loss: 0.00006223\n",
      "Epoch: 436, RMSE: 8.5054, MAE: 3.4471, MAPE: 32.5565,Train Loss: 0.00004394\n",
      "Epoch: 436, RMSE: 12.3635, MAE: 4.5854, MAPE: 32.5735,Test Loss: 0.00009203\n",
      "=========epoch 437=========\n",
      "Iteraion 10, Train Loss: 0.00006598\n",
      "Iteraion 20, Train Loss: 0.00006306\n",
      "Iteraion 30, Train Loss: 0.00007927\n",
      "Epoch: 437, RMSE: 8.4992, MAE: 3.4516, MAPE: 32.4142,Train Loss: 0.00004387\n",
      "Epoch: 437, RMSE: 12.4127, MAE: 4.5968, MAPE: 32.4535,Test Loss: 0.00009276\n",
      "=========epoch 438=========\n",
      "Iteraion 10, Train Loss: 0.00006540\n",
      "Iteraion 20, Train Loss: 0.00007556\n",
      "Iteraion 30, Train Loss: 0.00006242\n",
      "Epoch: 438, RMSE: 8.5892, MAE: 3.4683, MAPE: 32.3542,Train Loss: 0.00004481\n",
      "Epoch: 438, RMSE: 12.5890, MAE: 4.6482, MAPE: 32.5197,Test Loss: 0.00009538\n",
      "=========epoch 439=========\n",
      "Iteraion 10, Train Loss: 0.00007165\n",
      "Iteraion 20, Train Loss: 0.00006983\n",
      "Iteraion 30, Train Loss: 0.00007886\n",
      "Epoch: 439, RMSE: 8.5664, MAE: 3.4920, MAPE: 33.6508,Train Loss: 0.00004457\n",
      "Epoch: 439, RMSE: 12.5770, MAE: 4.6590, MAPE: 33.4048,Test Loss: 0.00009523\n",
      "=========epoch 440=========\n",
      "Iteraion 10, Train Loss: 0.00005860\n",
      "Iteraion 20, Train Loss: 0.00006548\n",
      "Iteraion 30, Train Loss: 0.00006128\n",
      "Epoch: 440, RMSE: 8.6999, MAE: 3.5375, MAPE: 33.5497,Train Loss: 0.00004597\n",
      "Epoch: 440, RMSE: 12.5687, MAE: 4.6611, MAPE: 33.3179,Test Loss: 0.00009508\n",
      "=========epoch 441=========\n",
      "Iteraion 10, Train Loss: 0.00005797\n",
      "Iteraion 20, Train Loss: 0.00006415\n",
      "Iteraion 30, Train Loss: 0.00005760\n",
      "Epoch: 441, RMSE: 8.6292, MAE: 3.5174, MAPE: 32.1884,Train Loss: 0.00004523\n",
      "Epoch: 441, RMSE: 12.5737, MAE: 4.6966, MAPE: 32.2446,Test Loss: 0.00009518\n",
      "=========epoch 442=========\n",
      "Iteraion 10, Train Loss: 0.00006795\n",
      "Iteraion 20, Train Loss: 0.00006053\n",
      "Iteraion 30, Train Loss: 0.00006257\n",
      "Epoch: 442, RMSE: 8.3921, MAE: 3.4197, MAPE: 32.8662,Train Loss: 0.00004278\n",
      "Epoch: 442, RMSE: 12.4319, MAE: 4.6091, MAPE: 32.8528,Test Loss: 0.00009300\n",
      "=========epoch 443=========\n",
      "Iteraion 10, Train Loss: 0.00006802\n",
      "Iteraion 20, Train Loss: 0.00006839\n",
      "Iteraion 30, Train Loss: 0.00006291\n",
      "Epoch: 443, RMSE: 8.7188, MAE: 3.5575, MAPE: 34.4860,Train Loss: 0.00004617\n",
      "Epoch: 443, RMSE: 12.5774, MAE: 4.6675, MAPE: 34.0216,Test Loss: 0.00009522\n",
      "=========epoch 444=========\n",
      "Iteraion 10, Train Loss: 0.00007202\n",
      "Iteraion 20, Train Loss: 0.00006987\n",
      "Iteraion 30, Train Loss: 0.00006756\n",
      "Epoch: 444, RMSE: 8.3932, MAE: 3.4105, MAPE: 33.5015,Train Loss: 0.00004279\n",
      "Epoch: 444, RMSE: 12.3311, MAE: 4.5519, MAPE: 32.9608,Test Loss: 0.00009151\n",
      "=========epoch 445=========\n",
      "Iteraion 10, Train Loss: 0.00007361\n",
      "Iteraion 20, Train Loss: 0.00006863\n",
      "Iteraion 30, Train Loss: 0.00006877\n",
      "Epoch: 445, RMSE: 8.5215, MAE: 3.4529, MAPE: 32.2016,Train Loss: 0.00004410\n",
      "Epoch: 445, RMSE: 12.5460, MAE: 4.6518, MAPE: 32.4026,Test Loss: 0.00009473\n",
      "=========epoch 446=========\n",
      "Iteraion 10, Train Loss: 0.00007054\n",
      "Iteraion 20, Train Loss: 0.00007385\n",
      "Iteraion 30, Train Loss: 0.00006064\n",
      "Epoch: 446, RMSE: 8.6061, MAE: 3.5030, MAPE: 33.2856,Train Loss: 0.00004498\n",
      "Epoch: 446, RMSE: 12.5449, MAE: 4.6537, MAPE: 33.3029,Test Loss: 0.00009471\n",
      "=========epoch 447=========\n",
      "Iteraion 10, Train Loss: 0.00007185\n",
      "Iteraion 20, Train Loss: 0.00006207\n",
      "Iteraion 30, Train Loss: 0.00006904\n",
      "Epoch: 447, RMSE: 8.4779, MAE: 3.4515, MAPE: 33.0073,Train Loss: 0.00004366\n",
      "Epoch: 447, RMSE: 12.4847, MAE: 4.6125, MAPE: 32.7885,Test Loss: 0.00009383\n",
      "=========epoch 448=========\n",
      "Iteraion 10, Train Loss: 0.00007735\n",
      "Iteraion 20, Train Loss: 0.00006857\n",
      "Iteraion 30, Train Loss: 0.00006842\n",
      "Epoch: 448, RMSE: 8.4933, MAE: 3.4579, MAPE: 32.8794,Train Loss: 0.00004381\n",
      "Epoch: 448, RMSE: 12.4801, MAE: 4.6156, MAPE: 32.8901,Test Loss: 0.00009376\n",
      "=========epoch 449=========\n",
      "Iteraion 10, Train Loss: 0.00006136\n",
      "Iteraion 20, Train Loss: 0.00006101\n",
      "Iteraion 30, Train Loss: 0.00006793\n",
      "Epoch: 449, RMSE: 8.4735, MAE: 3.4503, MAPE: 32.5311,Train Loss: 0.00004361\n",
      "Epoch: 449, RMSE: 12.6298, MAE: 4.6690, MAPE: 32.6072,Test Loss: 0.00009600\n",
      "=========epoch 450=========\n",
      "Iteraion 10, Train Loss: 0.00007167\n",
      "Iteraion 20, Train Loss: 0.00006963\n",
      "Iteraion 30, Train Loss: 0.00006942\n",
      "Epoch: 450, RMSE: 8.4764, MAE: 3.4333, MAPE: 32.3297,Train Loss: 0.00004364\n",
      "Epoch: 450, RMSE: 12.4267, MAE: 4.6029, MAPE: 32.4488,Test Loss: 0.00009299\n",
      "=========epoch 451=========\n",
      "Iteraion 10, Train Loss: 0.00006371\n",
      "Iteraion 20, Train Loss: 0.00006353\n",
      "Iteraion 30, Train Loss: 0.00006615\n",
      "Epoch: 451, RMSE: 8.5298, MAE: 3.4898, MAPE: 34.1791,Train Loss: 0.00004419\n",
      "Epoch: 451, RMSE: 12.4859, MAE: 4.6166, MAPE: 33.7173,Test Loss: 0.00009384\n",
      "=========epoch 452=========\n",
      "Iteraion 10, Train Loss: 0.00006577\n",
      "Iteraion 20, Train Loss: 0.00007019\n",
      "Iteraion 30, Train Loss: 0.00008061\n",
      "Epoch: 452, RMSE: 9.1474, MAE: 3.6626, MAPE: 33.2989,Train Loss: 0.00005082\n",
      "Epoch: 452, RMSE: 13.0579, MAE: 4.8155, MAPE: 33.4132,Test Loss: 0.00010272\n",
      "=========epoch 453=========\n",
      "Iteraion 10, Train Loss: 0.00007142\n",
      "Iteraion 20, Train Loss: 0.00007030\n",
      "Iteraion 30, Train Loss: 0.00006657\n",
      "Epoch: 453, RMSE: 8.7114, MAE: 3.4911, MAPE: 32.5860,Train Loss: 0.00004609\n",
      "Epoch: 453, RMSE: 12.7377, MAE: 4.6898, MAPE: 32.6169,Test Loss: 0.00009768\n",
      "=========epoch 454=========\n",
      "Iteraion 10, Train Loss: 0.00007338\n",
      "Iteraion 20, Train Loss: 0.00006945\n",
      "Iteraion 30, Train Loss: 0.00006699\n",
      "Epoch: 454, RMSE: 8.3994, MAE: 3.4217, MAPE: 33.0009,Train Loss: 0.00004285\n",
      "Epoch: 454, RMSE: 12.2703, MAE: 4.5555, MAPE: 32.9209,Test Loss: 0.00009063\n",
      "=========epoch 455=========\n",
      "Iteraion 10, Train Loss: 0.00005682\n",
      "Iteraion 20, Train Loss: 0.00006862\n",
      "Iteraion 30, Train Loss: 0.00006900\n",
      "Epoch: 455, RMSE: 8.5855, MAE: 3.4630, MAPE: 32.3116,Train Loss: 0.00004477\n",
      "Epoch: 455, RMSE: 12.5571, MAE: 4.6426, MAPE: 32.4830,Test Loss: 0.00009492\n",
      "=========epoch 456=========\n",
      "Iteraion 10, Train Loss: 0.00006554\n",
      "Iteraion 20, Train Loss: 0.00006289\n",
      "Iteraion 30, Train Loss: 0.00007185\n",
      "Epoch: 456, RMSE: 8.5662, MAE: 3.4604, MAPE: 32.5099,Train Loss: 0.00004457\n",
      "Epoch: 456, RMSE: 12.5615, MAE: 4.6258, MAPE: 32.4493,Test Loss: 0.00009503\n",
      "=========epoch 457=========\n",
      "Iteraion 10, Train Loss: 0.00007362\n",
      "Iteraion 20, Train Loss: 0.00006050\n",
      "Iteraion 30, Train Loss: 0.00005960\n",
      "Epoch: 457, RMSE: 8.3752, MAE: 3.4080, MAPE: 32.0953,Train Loss: 0.00004260\n",
      "Epoch: 457, RMSE: 12.3933, MAE: 4.5771, MAPE: 32.2500,Test Loss: 0.00009245\n",
      "=========epoch 458=========\n",
      "Iteraion 10, Train Loss: 0.00007170\n",
      "Iteraion 20, Train Loss: 0.00007388\n",
      "Iteraion 30, Train Loss: 0.00006979\n",
      "Epoch: 458, RMSE: 8.7129, MAE: 3.5648, MAPE: 34.5996,Train Loss: 0.00004611\n",
      "Epoch: 458, RMSE: 12.6185, MAE: 4.6648, MAPE: 33.8795,Test Loss: 0.00009586\n",
      "=========epoch 459=========\n",
      "Iteraion 10, Train Loss: 0.00007683\n",
      "Iteraion 20, Train Loss: 0.00006705\n",
      "Iteraion 30, Train Loss: 0.00006848\n",
      "Epoch: 459, RMSE: 8.3109, MAE: 3.4059, MAPE: 33.5422,Train Loss: 0.00004195\n",
      "Epoch: 459, RMSE: 12.3571, MAE: 4.5805, MAPE: 33.3355,Test Loss: 0.00009193\n",
      "=========epoch 460=========\n",
      "Iteraion 10, Train Loss: 0.00006082\n",
      "Iteraion 20, Train Loss: 0.00006709\n",
      "Iteraion 30, Train Loss: 0.00006655\n",
      "Epoch: 460, RMSE: 8.4376, MAE: 3.4347, MAPE: 32.3733,Train Loss: 0.00004324\n",
      "Epoch: 460, RMSE: 12.5961, MAE: 4.6534, MAPE: 32.6102,Test Loss: 0.00009551\n",
      "=========epoch 461=========\n",
      "Iteraion 10, Train Loss: 0.00006500\n",
      "Iteraion 20, Train Loss: 0.00006893\n",
      "Iteraion 30, Train Loss: 0.00006760\n",
      "Epoch: 461, RMSE: 8.3561, MAE: 3.4305, MAPE: 33.6299,Train Loss: 0.00004241\n",
      "Epoch: 461, RMSE: 12.4703, MAE: 4.6056, MAPE: 33.3703,Test Loss: 0.00009363\n",
      "=========epoch 462=========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraion 10, Train Loss: 0.00006055\n",
      "Iteraion 20, Train Loss: 0.00007632\n",
      "Iteraion 30, Train Loss: 0.00007583\n",
      "Epoch: 462, RMSE: 8.5069, MAE: 3.4562, MAPE: 32.0197,Train Loss: 0.00004395\n",
      "Epoch: 462, RMSE: 12.6044, MAE: 4.6848, MAPE: 32.3312,Test Loss: 0.00009562\n",
      "=========epoch 463=========\n",
      "Iteraion 10, Train Loss: 0.00005906\n",
      "Iteraion 20, Train Loss: 0.00006702\n",
      "Iteraion 30, Train Loss: 0.00006314\n",
      "Epoch: 463, RMSE: 8.3761, MAE: 3.4363, MAPE: 33.2223,Train Loss: 0.00004261\n",
      "Epoch: 463, RMSE: 12.4444, MAE: 4.5992, MAPE: 33.0331,Test Loss: 0.00009323\n",
      "=========epoch 464=========\n",
      "Iteraion 10, Train Loss: 0.00006729\n",
      "Iteraion 20, Train Loss: 0.00006850\n",
      "Iteraion 30, Train Loss: 0.00006563\n",
      "Epoch: 464, RMSE: 8.8580, MAE: 3.5605, MAPE: 32.0357,Train Loss: 0.00004767\n",
      "Epoch: 464, RMSE: 12.8285, MAE: 4.7432, MAPE: 32.3915,Test Loss: 0.00009903\n",
      "=========epoch 465=========\n",
      "Iteraion 10, Train Loss: 0.00007346\n",
      "Iteraion 20, Train Loss: 0.00006249\n",
      "Iteraion 30, Train Loss: 0.00005947\n",
      "Epoch: 465, RMSE: 8.1435, MAE: 3.3355, MAPE: 32.7732,Train Loss: 0.00004028\n",
      "Epoch: 465, RMSE: 12.3855, MAE: 4.5610, MAPE: 32.9652,Test Loss: 0.00009232\n",
      "=========epoch 466=========\n",
      "Iteraion 10, Train Loss: 0.00005910\n",
      "Iteraion 20, Train Loss: 0.00006159\n",
      "Iteraion 30, Train Loss: 0.00005893\n",
      "Epoch: 466, RMSE: 8.4731, MAE: 3.4532, MAPE: 32.2852,Train Loss: 0.00004361\n",
      "Epoch: 466, RMSE: 12.5542, MAE: 4.6547, MAPE: 32.5270,Test Loss: 0.00009486\n",
      "=========epoch 467=========\n",
      "Iteraion 10, Train Loss: 0.00006591\n",
      "Iteraion 20, Train Loss: 0.00007065\n",
      "Iteraion 30, Train Loss: 0.00007363\n",
      "Epoch: 467, RMSE: 8.3555, MAE: 3.3920, MAPE: 32.4350,Train Loss: 0.00004241\n",
      "Epoch: 467, RMSE: 12.5917, MAE: 4.6406, MAPE: 32.7979,Test Loss: 0.00009543\n",
      "=========epoch 468=========\n",
      "Iteraion 10, Train Loss: 0.00006668\n",
      "Iteraion 20, Train Loss: 0.00006117\n",
      "Iteraion 30, Train Loss: 0.00006070\n",
      "Epoch: 468, RMSE: 8.6970, MAE: 3.4681, MAPE: 32.6386,Train Loss: 0.00004593\n",
      "Epoch: 468, RMSE: 12.6726, MAE: 4.6352, MAPE: 32.5966,Test Loss: 0.00009670\n",
      "=========epoch 469=========\n",
      "Iteraion 10, Train Loss: 0.00006435\n",
      "Iteraion 20, Train Loss: 0.00006878\n",
      "Iteraion 30, Train Loss: 0.00006873\n",
      "Epoch: 469, RMSE: 8.4522, MAE: 3.4515, MAPE: 33.6670,Train Loss: 0.00004339\n",
      "Epoch: 469, RMSE: 12.5583, MAE: 4.6246, MAPE: 33.5000,Test Loss: 0.00009495\n",
      "=========epoch 470=========\n",
      "Iteraion 10, Train Loss: 0.00006534\n",
      "Iteraion 20, Train Loss: 0.00007215\n",
      "Iteraion 30, Train Loss: 0.00007129\n",
      "Epoch: 470, RMSE: 8.5252, MAE: 3.4753, MAPE: 32.1201,Train Loss: 0.00004414\n",
      "Epoch: 470, RMSE: 12.6956, MAE: 4.7137, MAPE: 32.4530,Test Loss: 0.00009696\n",
      "=========epoch 471=========\n",
      "Iteraion 10, Train Loss: 0.00006158\n",
      "Iteraion 20, Train Loss: 0.00006541\n",
      "Iteraion 30, Train Loss: 0.00006309\n",
      "Epoch: 471, RMSE: 8.1748, MAE: 3.3543, MAPE: 32.7809,Train Loss: 0.00004059\n",
      "Epoch: 471, RMSE: 12.4574, MAE: 4.5818, MAPE: 32.8866,Test Loss: 0.00009343\n",
      "=========epoch 472=========\n",
      "Iteraion 10, Train Loss: 0.00006366\n",
      "Iteraion 20, Train Loss: 0.00006232\n",
      "Iteraion 30, Train Loss: 0.00006578\n",
      "Epoch: 472, RMSE: 8.2114, MAE: 3.3446, MAPE: 32.9282,Train Loss: 0.00004095\n",
      "Epoch: 472, RMSE: 12.3302, MAE: 4.5304, MAPE: 32.9479,Test Loss: 0.00009149\n",
      "=========epoch 473=========\n",
      "Iteraion 10, Train Loss: 0.00006624\n",
      "Iteraion 20, Train Loss: 0.00005826\n",
      "Iteraion 30, Train Loss: 0.00006142\n",
      "Epoch: 473, RMSE: 8.5441, MAE: 3.4346, MAPE: 32.7935,Train Loss: 0.00004434\n",
      "Epoch: 473, RMSE: 12.6069, MAE: 4.6163, MAPE: 33.0849,Test Loss: 0.00009569\n",
      "=========epoch 474=========\n",
      "Iteraion 10, Train Loss: 0.00006799\n",
      "Iteraion 20, Train Loss: 0.00007213\n",
      "Iteraion 30, Train Loss: 0.00006126\n",
      "Epoch: 474, RMSE: 8.5431, MAE: 3.4034, MAPE: 32.8487,Train Loss: 0.00004433\n",
      "Epoch: 474, RMSE: 12.6648, MAE: 4.6110, MAPE: 32.9292,Test Loss: 0.00009654\n",
      "=========epoch 475=========\n",
      "Iteraion 10, Train Loss: 0.00006286\n",
      "Iteraion 20, Train Loss: 0.00004916\n",
      "Iteraion 30, Train Loss: 0.00006926\n",
      "Epoch: 475, RMSE: 8.2136, MAE: 3.3725, MAPE: 33.1801,Train Loss: 0.00004098\n",
      "Epoch: 475, RMSE: 12.4529, MAE: 4.5938, MAPE: 33.1793,Test Loss: 0.00009335\n",
      "=========epoch 476=========\n",
      "Iteraion 10, Train Loss: 0.00005618\n",
      "Iteraion 20, Train Loss: 0.00006995\n",
      "Iteraion 30, Train Loss: 0.00006581\n",
      "Epoch: 476, RMSE: 8.4470, MAE: 3.4542, MAPE: 33.2048,Train Loss: 0.00004334\n",
      "Epoch: 476, RMSE: 12.6809, MAE: 4.6685, MAPE: 33.3169,Test Loss: 0.00009679\n",
      "=========epoch 477=========\n",
      "Iteraion 10, Train Loss: 0.00006414\n",
      "Iteraion 20, Train Loss: 0.00005953\n",
      "Iteraion 30, Train Loss: 0.00006848\n",
      "Epoch: 477, RMSE: 8.4371, MAE: 3.4062, MAPE: 32.6402,Train Loss: 0.00004323\n",
      "Epoch: 477, RMSE: 12.4734, MAE: 4.5794, MAPE: 32.6651,Test Loss: 0.00009369\n",
      "=========epoch 478=========\n",
      "Iteraion 10, Train Loss: 0.00007045\n",
      "Iteraion 20, Train Loss: 0.00006324\n",
      "Iteraion 30, Train Loss: 0.00006427\n",
      "Epoch: 478, RMSE: 8.5759, MAE: 3.5001, MAPE: 33.8575,Train Loss: 0.00004467\n",
      "Epoch: 478, RMSE: 12.6254, MAE: 4.6518, MAPE: 33.5929,Test Loss: 0.00009596\n",
      "=========epoch 479=========\n",
      "Iteraion 10, Train Loss: 0.00005957\n",
      "Iteraion 20, Train Loss: 0.00006575\n",
      "Iteraion 30, Train Loss: 0.00005388\n",
      "Epoch: 479, RMSE: 8.4600, MAE: 3.4206, MAPE: 32.3497,Train Loss: 0.00004347\n",
      "Epoch: 479, RMSE: 12.6113, MAE: 4.6323, MAPE: 32.5748,Test Loss: 0.00009572\n",
      "=========epoch 480=========\n",
      "Iteraion 10, Train Loss: 0.00006875\n",
      "Iteraion 20, Train Loss: 0.00006548\n",
      "Iteraion 30, Train Loss: 0.00006324\n",
      "Epoch: 480, RMSE: 8.0809, MAE: 3.3207, MAPE: 32.2313,Train Loss: 0.00003966\n",
      "Epoch: 480, RMSE: 12.3097, MAE: 4.5471, MAPE: 32.4406,Test Loss: 0.00009121\n",
      "=========epoch 481=========\n",
      "Iteraion 10, Train Loss: 0.00006222\n",
      "Iteraion 20, Train Loss: 0.00006523\n",
      "Iteraion 30, Train Loss: 0.00007145\n",
      "Epoch: 481, RMSE: 8.1894, MAE: 3.3498, MAPE: 32.9870,Train Loss: 0.00004073\n",
      "Epoch: 481, RMSE: 12.4595, MAE: 4.5845, MAPE: 33.1273,Test Loss: 0.00009346\n",
      "=========epoch 482=========\n",
      "Iteraion 10, Train Loss: 0.00006810\n",
      "Iteraion 20, Train Loss: 0.00005975\n",
      "Iteraion 30, Train Loss: 0.00006088\n",
      "Epoch: 482, RMSE: 8.2762, MAE: 3.3756, MAPE: 33.0059,Train Loss: 0.00004161\n",
      "Epoch: 482, RMSE: 12.4864, MAE: 4.5985, MAPE: 33.1256,Test Loss: 0.00009385\n",
      "=========epoch 483=========\n",
      "Iteraion 10, Train Loss: 0.00007092\n",
      "Iteraion 20, Train Loss: 0.00006101\n",
      "Iteraion 30, Train Loss: 0.00006417\n",
      "Epoch: 483, RMSE: 8.3146, MAE: 3.3747, MAPE: 32.6073,Train Loss: 0.00004199\n",
      "Epoch: 483, RMSE: 12.4952, MAE: 4.5803, MAPE: 32.7024,Test Loss: 0.00009399\n",
      "=========epoch 484=========\n",
      "Iteraion 10, Train Loss: 0.00006821\n",
      "Iteraion 20, Train Loss: 0.00006446\n",
      "Iteraion 30, Train Loss: 0.00006933\n",
      "Epoch: 484, RMSE: 8.1596, MAE: 3.3616, MAPE: 33.7396,Train Loss: 0.00004044\n",
      "Epoch: 484, RMSE: 12.4046, MAE: 4.5683, MAPE: 33.4530,Test Loss: 0.00009263\n",
      "=========epoch 485=========\n",
      "Iteraion 10, Train Loss: 0.00006319\n",
      "Iteraion 20, Train Loss: 0.00005995\n",
      "Iteraion 30, Train Loss: 0.00005726\n",
      "Epoch: 485, RMSE: 8.2524, MAE: 3.3822, MAPE: 32.8306,Train Loss: 0.00004136\n",
      "Epoch: 485, RMSE: 12.5429, MAE: 4.6068, MAPE: 33.1286,Test Loss: 0.00009468\n",
      "=========epoch 486=========\n",
      "Iteraion 10, Train Loss: 0.00006730\n",
      "Iteraion 20, Train Loss: 0.00006078\n",
      "Iteraion 30, Train Loss: 0.00006521\n",
      "Epoch: 486, RMSE: 8.2018, MAE: 3.3400, MAPE: 32.7335,Train Loss: 0.00004086\n",
      "Epoch: 486, RMSE: 12.4085, MAE: 4.5531, MAPE: 32.8383,Test Loss: 0.00009266\n",
      "=========epoch 487=========\n",
      "Iteraion 10, Train Loss: 0.00005882\n",
      "Iteraion 20, Train Loss: 0.00005985\n",
      "Iteraion 30, Train Loss: 0.00006777\n",
      "Epoch: 487, RMSE: 8.1526, MAE: 3.3481, MAPE: 32.5215,Train Loss: 0.00004037\n",
      "Epoch: 487, RMSE: 12.4252, MAE: 4.5827, MAPE: 32.7266,Test Loss: 0.00009291\n",
      "=========epoch 488=========\n",
      "Iteraion 10, Train Loss: 0.00006362\n",
      "Iteraion 20, Train Loss: 0.00006454\n",
      "Iteraion 30, Train Loss: 0.00006392\n",
      "Epoch: 488, RMSE: 8.1699, MAE: 3.3822, MAPE: 34.1289,Train Loss: 0.00004054\n",
      "Epoch: 488, RMSE: 12.4917, MAE: 4.6140, MAPE: 34.0532,Test Loss: 0.00009394\n",
      "=========epoch 489=========\n",
      "Iteraion 10, Train Loss: 0.00007208\n",
      "Iteraion 20, Train Loss: 0.00006011\n",
      "Iteraion 30, Train Loss: 0.00006919\n",
      "Epoch: 489, RMSE: 8.2002, MAE: 3.3393, MAPE: 32.6588,Train Loss: 0.00004084\n",
      "Epoch: 489, RMSE: 12.4868, MAE: 4.5898, MAPE: 32.8850,Test Loss: 0.00009384\n",
      "=========epoch 490=========\n",
      "Iteraion 10, Train Loss: 0.00005752\n",
      "Iteraion 20, Train Loss: 0.00005972\n",
      "Iteraion 30, Train Loss: 0.00006337\n",
      "Epoch: 490, RMSE: 8.8405, MAE: 3.5490, MAPE: 32.0582,Train Loss: 0.00004747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 490, RMSE: 12.9819, MAE: 4.7848, MAPE: 32.4193,Test Loss: 0.00010152\n",
      "=========epoch 491=========\n",
      "Iteraion 10, Train Loss: 0.00006460\n",
      "Iteraion 20, Train Loss: 0.00006308\n",
      "Iteraion 30, Train Loss: 0.00007139\n",
      "Epoch: 491, RMSE: 8.5504, MAE: 3.5605, MAPE: 35.3849,Train Loss: 0.00004440\n",
      "Epoch: 491, RMSE: 12.6878, MAE: 4.6990, MAPE: 34.6554,Test Loss: 0.00009690\n",
      "=========epoch 492=========\n",
      "Iteraion 10, Train Loss: 0.00005804\n",
      "Iteraion 20, Train Loss: 0.00006056\n",
      "Iteraion 30, Train Loss: 0.00006526\n",
      "Epoch: 492, RMSE: 8.2507, MAE: 3.3640, MAPE: 32.5129,Train Loss: 0.00004135\n",
      "Epoch: 492, RMSE: 12.5934, MAE: 4.6157, MAPE: 32.8389,Test Loss: 0.00009545\n",
      "=========epoch 493=========\n",
      "Iteraion 10, Train Loss: 0.00005234\n",
      "Iteraion 20, Train Loss: 0.00006383\n",
      "Iteraion 30, Train Loss: 0.00005650\n",
      "Epoch: 493, RMSE: 8.0753, MAE: 3.3281, MAPE: 33.1327,Train Loss: 0.00003961\n",
      "Epoch: 493, RMSE: 12.4149, MAE: 4.5702, MAPE: 33.0967,Test Loss: 0.00009279\n",
      "=========epoch 494=========\n",
      "Iteraion 10, Train Loss: 0.00006315\n",
      "Iteraion 20, Train Loss: 0.00006282\n",
      "Iteraion 30, Train Loss: 0.00006567\n",
      "Epoch: 494, RMSE: 8.1027, MAE: 3.3376, MAPE: 33.6136,Train Loss: 0.00003988\n",
      "Epoch: 494, RMSE: 12.5089, MAE: 4.5916, MAPE: 33.6309,Test Loss: 0.00009419\n",
      "=========epoch 495=========\n",
      "Iteraion 10, Train Loss: 0.00005832\n",
      "Iteraion 20, Train Loss: 0.00005958\n",
      "Iteraion 30, Train Loss: 0.00005960\n",
      "Epoch: 495, RMSE: 8.3134, MAE: 3.4361, MAPE: 33.8620,Train Loss: 0.00004198\n",
      "Epoch: 495, RMSE: 12.5508, MAE: 4.6346, MAPE: 33.8585,Test Loss: 0.00009483\n",
      "=========epoch 496=========\n",
      "Iteraion 10, Train Loss: 0.00005563\n",
      "Iteraion 20, Train Loss: 0.00007026\n",
      "Iteraion 30, Train Loss: 0.00005628\n",
      "Epoch: 496, RMSE: 8.9914, MAE: 3.6039, MAPE: 31.9377,Train Loss: 0.00004911\n",
      "Epoch: 496, RMSE: 13.1988, MAE: 4.8620, MAPE: 32.5982,Test Loss: 0.00010485\n",
      "=========epoch 497=========\n",
      "Iteraion 10, Train Loss: 0.00006355\n",
      "Iteraion 20, Train Loss: 0.00005847\n",
      "Iteraion 30, Train Loss: 0.00006584\n",
      "Epoch: 497, RMSE: 8.2394, MAE: 3.3749, MAPE: 32.1724,Train Loss: 0.00004124\n",
      "Epoch: 497, RMSE: 12.6224, MAE: 4.6515, MAPE: 32.6361,Test Loss: 0.00009590\n",
      "=========epoch 498=========\n",
      "Iteraion 10, Train Loss: 0.00005531\n",
      "Iteraion 20, Train Loss: 0.00006317\n",
      "Iteraion 30, Train Loss: 0.00007243\n",
      "Epoch: 498, RMSE: 8.0121, MAE: 3.2972, MAPE: 32.2238,Train Loss: 0.00003899\n",
      "Epoch: 498, RMSE: 12.3300, MAE: 4.5374, MAPE: 32.4873,Test Loss: 0.00009153\n",
      "=========epoch 499=========\n",
      "Iteraion 10, Train Loss: 0.00006950\n",
      "Iteraion 20, Train Loss: 0.00005822\n",
      "Iteraion 30, Train Loss: 0.00006918\n",
      "Epoch: 499, RMSE: 8.7972, MAE: 3.5101, MAPE: 32.1107,Train Loss: 0.00004700\n",
      "Epoch: 499, RMSE: 13.0489, MAE: 4.7679, MAPE: 32.7086,Test Loss: 0.00010247\n",
      "=========epoch 500=========\n",
      "Iteraion 10, Train Loss: 0.00005725\n",
      "Iteraion 20, Train Loss: 0.00006401\n",
      "Iteraion 30, Train Loss: 0.00007469\n",
      "Epoch: 500, RMSE: 8.3449, MAE: 3.4084, MAPE: 32.3831,Train Loss: 0.00004229\n",
      "Epoch: 500, RMSE: 12.6504, MAE: 4.6548, MAPE: 32.7804,Test Loss: 0.00009635\n",
      "=========epoch 501=========\n",
      "Iteraion 10, Train Loss: 0.00005339\n",
      "Iteraion 20, Train Loss: 0.00005448\n",
      "Iteraion 30, Train Loss: 0.00006135\n",
      "Epoch: 501, RMSE: 7.7628, MAE: 3.2179, MAPE: 32.3364,Train Loss: 0.00003660\n",
      "Epoch: 501, RMSE: 12.2831, MAE: 4.5285, MAPE: 32.7702,Test Loss: 0.00009083\n",
      "=========epoch 502=========\n",
      "Iteraion 10, Train Loss: 0.00005839\n",
      "Iteraion 20, Train Loss: 0.00005604\n",
      "Iteraion 30, Train Loss: 0.00005755\n",
      "Epoch: 502, RMSE: 7.6653, MAE: 3.1848, MAPE: 32.4724,Train Loss: 0.00003569\n",
      "Epoch: 502, RMSE: 12.2225, MAE: 4.4970, MAPE: 32.8291,Test Loss: 0.00008993\n",
      "=========epoch 503=========\n",
      "Iteraion 10, Train Loss: 0.00005367\n",
      "Iteraion 20, Train Loss: 0.00006068\n",
      "Iteraion 30, Train Loss: 0.00005832\n",
      "Epoch: 503, RMSE: 7.6360, MAE: 3.1768, MAPE: 32.5427,Train Loss: 0.00003542\n",
      "Epoch: 503, RMSE: 12.2148, MAE: 4.4881, MAPE: 32.8872,Test Loss: 0.00008982\n",
      "=========epoch 504=========\n",
      "Iteraion 10, Train Loss: 0.00006070\n",
      "Iteraion 20, Train Loss: 0.00005610\n",
      "Iteraion 30, Train Loss: 0.00005249\n",
      "Epoch: 504, RMSE: 7.6313, MAE: 3.1803, MAPE: 32.4579,Train Loss: 0.00003537\n",
      "Epoch: 504, RMSE: 12.1834, MAE: 4.4868, MAPE: 32.7875,Test Loss: 0.00008936\n",
      "=========epoch 505=========\n",
      "Iteraion 10, Train Loss: 0.00005382\n",
      "Iteraion 20, Train Loss: 0.00005262\n",
      "Iteraion 30, Train Loss: 0.00005791\n",
      "Epoch: 505, RMSE: 7.6061, MAE: 3.1725, MAPE: 32.4648,Train Loss: 0.00003514\n",
      "Epoch: 505, RMSE: 12.1854, MAE: 4.4839, MAPE: 32.7899,Test Loss: 0.00008939\n",
      "=========epoch 506=========\n",
      "Iteraion 10, Train Loss: 0.00005319\n",
      "Iteraion 20, Train Loss: 0.00005834\n",
      "Iteraion 30, Train Loss: 0.00006263\n",
      "Epoch: 506, RMSE: 7.5961, MAE: 3.1727, MAPE: 32.7583,Train Loss: 0.00003505\n",
      "Epoch: 506, RMSE: 12.1819, MAE: 4.4802, MAPE: 33.0254,Test Loss: 0.00008934\n",
      "=========epoch 507=========\n",
      "Iteraion 10, Train Loss: 0.00005313\n",
      "Iteraion 20, Train Loss: 0.00005301\n",
      "Iteraion 30, Train Loss: 0.00005539\n",
      "Epoch: 507, RMSE: 7.5963, MAE: 3.1687, MAPE: 32.4251,Train Loss: 0.00003505\n",
      "Epoch: 507, RMSE: 12.1625, MAE: 4.4787, MAPE: 32.8093,Test Loss: 0.00008905\n",
      "=========epoch 508=========\n",
      "Iteraion 10, Train Loss: 0.00005933\n",
      "Iteraion 20, Train Loss: 0.00005820\n",
      "Iteraion 30, Train Loss: 0.00005988\n",
      "Epoch: 508, RMSE: 7.6024, MAE: 3.1695, MAPE: 32.3221,Train Loss: 0.00003511\n",
      "Epoch: 508, RMSE: 12.1963, MAE: 4.4905, MAPE: 32.7342,Test Loss: 0.00008954\n",
      "=========epoch 509=========\n",
      "Iteraion 10, Train Loss: 0.00006061\n",
      "Iteraion 20, Train Loss: 0.00005956\n",
      "Iteraion 30, Train Loss: 0.00005964\n",
      "Epoch: 509, RMSE: 7.5769, MAE: 3.1646, MAPE: 32.6559,Train Loss: 0.00003487\n",
      "Epoch: 509, RMSE: 12.1735, MAE: 4.4757, MAPE: 32.8915,Test Loss: 0.00008922\n",
      "=========epoch 510=========\n",
      "Iteraion 10, Train Loss: 0.00005366\n",
      "Iteraion 20, Train Loss: 0.00005179\n",
      "Iteraion 30, Train Loss: 0.00005865\n",
      "Epoch: 510, RMSE: 7.6191, MAE: 3.1711, MAPE: 32.3073,Train Loss: 0.00003526\n",
      "Epoch: 510, RMSE: 12.2057, MAE: 4.4896, MAPE: 32.7016,Test Loss: 0.00008968\n",
      "=========epoch 511=========\n",
      "Iteraion 10, Train Loss: 0.00005388\n",
      "Iteraion 20, Train Loss: 0.00005081\n",
      "Iteraion 30, Train Loss: 0.00005842\n",
      "Epoch: 511, RMSE: 7.5702, MAE: 3.1581, MAPE: 32.4962,Train Loss: 0.00003481\n",
      "Epoch: 511, RMSE: 12.1654, MAE: 4.4764, MAPE: 32.8642,Test Loss: 0.00008909\n",
      "=========epoch 512=========\n",
      "Iteraion 10, Train Loss: 0.00005797\n",
      "Iteraion 20, Train Loss: 0.00005262\n",
      "Iteraion 30, Train Loss: 0.00005589\n",
      "Epoch: 512, RMSE: 7.5996, MAE: 3.1620, MAPE: 32.2190,Train Loss: 0.00003508\n",
      "Epoch: 512, RMSE: 12.2080, MAE: 4.4828, MAPE: 32.6169,Test Loss: 0.00008971\n",
      "=========epoch 513=========\n",
      "Iteraion 10, Train Loss: 0.00005969\n",
      "Iteraion 20, Train Loss: 0.00006224\n",
      "Iteraion 30, Train Loss: 0.00005889\n",
      "Epoch: 513, RMSE: 7.5998, MAE: 3.1625, MAPE: 32.3693,Train Loss: 0.00003508\n",
      "Epoch: 513, RMSE: 12.2088, MAE: 4.4808, MAPE: 32.7157,Test Loss: 0.00008973\n",
      "=========epoch 514=========\n",
      "Iteraion 10, Train Loss: 0.00005288\n",
      "Iteraion 20, Train Loss: 0.00005239\n",
      "Iteraion 30, Train Loss: 0.00006072\n",
      "Epoch: 514, RMSE: 7.5733, MAE: 3.1578, MAPE: 32.3461,Train Loss: 0.00003484\n",
      "Epoch: 514, RMSE: 12.1722, MAE: 4.4703, MAPE: 32.6908,Test Loss: 0.00008919\n",
      "=========epoch 515=========\n",
      "Iteraion 10, Train Loss: 0.00006026\n",
      "Iteraion 20, Train Loss: 0.00005680\n",
      "Iteraion 30, Train Loss: 0.00005575\n",
      "Epoch: 515, RMSE: 7.5727, MAE: 3.1638, MAPE: 32.2806,Train Loss: 0.00003483\n",
      "Epoch: 515, RMSE: 12.2273, MAE: 4.4955, MAPE: 32.7338,Test Loss: 0.00009001\n",
      "=========epoch 516=========\n",
      "Iteraion 10, Train Loss: 0.00004673\n",
      "Iteraion 20, Train Loss: 0.00005281\n",
      "Iteraion 30, Train Loss: 0.00005297\n",
      "Epoch: 516, RMSE: 7.5638, MAE: 3.1524, MAPE: 32.2531,Train Loss: 0.00003475\n",
      "Epoch: 516, RMSE: 12.1810, MAE: 4.4722, MAPE: 32.6743,Test Loss: 0.00008932\n",
      "=========epoch 517=========\n",
      "Iteraion 10, Train Loss: 0.00005239\n",
      "Iteraion 20, Train Loss: 0.00005521\n",
      "Iteraion 30, Train Loss: 0.00005231\n",
      "Epoch: 517, RMSE: 7.5755, MAE: 3.1557, MAPE: 32.4057,Train Loss: 0.00003486\n",
      "Epoch: 517, RMSE: 12.1918, MAE: 4.4742, MAPE: 32.8321,Test Loss: 0.00008948\n",
      "=========epoch 518=========\n",
      "Iteraion 10, Train Loss: 0.00005696\n",
      "Iteraion 20, Train Loss: 0.00004510\n",
      "Iteraion 30, Train Loss: 0.00004552\n",
      "Epoch: 518, RMSE: 7.6456, MAE: 3.1778, MAPE: 31.9567,Train Loss: 0.00003551\n",
      "Epoch: 518, RMSE: 12.2523, MAE: 4.5056, MAPE: 32.4625,Test Loss: 0.00009036\n",
      "=========epoch 519=========\n",
      "Iteraion 10, Train Loss: 0.00005191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraion 20, Train Loss: 0.00005484\n",
      "Iteraion 30, Train Loss: 0.00004683\n",
      "Epoch: 519, RMSE: 7.5720, MAE: 3.1553, MAPE: 32.1135,Train Loss: 0.00003483\n",
      "Epoch: 519, RMSE: 12.2382, MAE: 4.4965, MAPE: 32.6223,Test Loss: 0.00009015\n",
      "=========epoch 520=========\n",
      "Iteraion 10, Train Loss: 0.00005834\n",
      "Iteraion 20, Train Loss: 0.00005003\n",
      "Iteraion 30, Train Loss: 0.00005060\n",
      "Epoch: 520, RMSE: 7.5664, MAE: 3.1538, MAPE: 32.4671,Train Loss: 0.00003477\n",
      "Epoch: 520, RMSE: 12.1959, MAE: 4.4746, MAPE: 32.8090,Test Loss: 0.00008954\n",
      "=========epoch 521=========\n",
      "Iteraion 10, Train Loss: 0.00005704\n",
      "Iteraion 20, Train Loss: 0.00005963\n",
      "Iteraion 30, Train Loss: 0.00005462\n",
      "Epoch: 521, RMSE: 7.5495, MAE: 3.1526, MAPE: 32.4091,Train Loss: 0.00003462\n",
      "Epoch: 521, RMSE: 12.2079, MAE: 4.4879, MAPE: 32.8731,Test Loss: 0.00008972\n",
      "=========epoch 522=========\n",
      "Iteraion 10, Train Loss: 0.00005415\n",
      "Iteraion 20, Train Loss: 0.00004751\n",
      "Iteraion 30, Train Loss: 0.00005407\n",
      "Epoch: 522, RMSE: 7.5644, MAE: 3.1503, MAPE: 32.2135,Train Loss: 0.00003476\n",
      "Epoch: 522, RMSE: 12.2223, MAE: 4.4859, MAPE: 32.6835,Test Loss: 0.00008993\n",
      "=========epoch 523=========\n",
      "Iteraion 10, Train Loss: 0.00005789\n",
      "Iteraion 20, Train Loss: 0.00005557\n",
      "Iteraion 30, Train Loss: 0.00005749\n",
      "Epoch: 523, RMSE: 7.5579, MAE: 3.1469, MAPE: 32.1678,Train Loss: 0.00003470\n",
      "Epoch: 523, RMSE: 12.1891, MAE: 4.4778, MAPE: 32.6434,Test Loss: 0.00008944\n",
      "=========epoch 524=========\n",
      "Iteraion 10, Train Loss: 0.00005422\n",
      "Iteraion 20, Train Loss: 0.00005457\n",
      "Iteraion 30, Train Loss: 0.00006205\n",
      "Epoch: 524, RMSE: 7.5414, MAE: 3.1463, MAPE: 32.3845,Train Loss: 0.00003455\n",
      "Epoch: 524, RMSE: 12.1789, MAE: 4.4767, MAPE: 32.8074,Test Loss: 0.00008930\n",
      "=========epoch 525=========\n",
      "Iteraion 10, Train Loss: 0.00005719\n",
      "Iteraion 20, Train Loss: 0.00006116\n",
      "Iteraion 30, Train Loss: 0.00005356\n",
      "Epoch: 525, RMSE: 7.5441, MAE: 3.1483, MAPE: 32.3983,Train Loss: 0.00003457\n",
      "Epoch: 525, RMSE: 12.1986, MAE: 4.4759, MAPE: 32.8144,Test Loss: 0.00008958\n",
      "=========epoch 526=========\n",
      "Iteraion 10, Train Loss: 0.00005647\n",
      "Iteraion 20, Train Loss: 0.00005899\n",
      "Iteraion 30, Train Loss: 0.00005911\n",
      "Epoch: 526, RMSE: 7.5676, MAE: 3.1496, MAPE: 32.2138,Train Loss: 0.00003479\n",
      "Epoch: 526, RMSE: 12.2021, MAE: 4.4776, MAPE: 32.6787,Test Loss: 0.00008963\n",
      "=========epoch 527=========\n",
      "Iteraion 10, Train Loss: 0.00005575\n",
      "Iteraion 20, Train Loss: 0.00005368\n",
      "Iteraion 30, Train Loss: 0.00005450\n",
      "Epoch: 527, RMSE: 7.5357, MAE: 3.1496, MAPE: 32.3588,Train Loss: 0.00003449\n",
      "Epoch: 527, RMSE: 12.1953, MAE: 4.4817, MAPE: 32.7569,Test Loss: 0.00008953\n",
      "=========epoch 528=========\n",
      "Iteraion 10, Train Loss: 0.00006005\n",
      "Iteraion 20, Train Loss: 0.00005549\n",
      "Iteraion 30, Train Loss: 0.00005325\n",
      "Epoch: 528, RMSE: 7.5537, MAE: 3.1468, MAPE: 32.2007,Train Loss: 0.00003466\n",
      "Epoch: 528, RMSE: 12.2133, MAE: 4.4812, MAPE: 32.6568,Test Loss: 0.00008980\n",
      "=========epoch 529=========\n",
      "Iteraion 10, Train Loss: 0.00005454\n",
      "Iteraion 20, Train Loss: 0.00004912\n",
      "Iteraion 30, Train Loss: 0.00005633\n",
      "Epoch: 529, RMSE: 7.5401, MAE: 3.1430, MAPE: 32.4249,Train Loss: 0.00003453\n",
      "Epoch: 529, RMSE: 12.1786, MAE: 4.4681, MAPE: 32.8245,Test Loss: 0.00008929\n",
      "=========epoch 530=========\n",
      "Iteraion 10, Train Loss: 0.00005006\n",
      "Iteraion 20, Train Loss: 0.00005192\n",
      "Iteraion 30, Train Loss: 0.00005390\n",
      "Epoch: 530, RMSE: 7.6194, MAE: 3.1605, MAPE: 32.1815,Train Loss: 0.00003526\n",
      "Epoch: 530, RMSE: 12.2494, MAE: 4.4903, MAPE: 32.6407,Test Loss: 0.00009033\n",
      "=========epoch 531=========\n",
      "Iteraion 10, Train Loss: 0.00005894\n",
      "Iteraion 20, Train Loss: 0.00005706\n",
      "Iteraion 30, Train Loss: 0.00006317\n",
      "Epoch: 531, RMSE: 7.5348, MAE: 3.1412, MAPE: 32.2449,Train Loss: 0.00003448\n",
      "Epoch: 531, RMSE: 12.1887, MAE: 4.4746, MAPE: 32.6932,Test Loss: 0.00008943\n",
      "=========epoch 532=========\n",
      "Iteraion 10, Train Loss: 0.00005395\n",
      "Iteraion 20, Train Loss: 0.00005021\n",
      "Iteraion 30, Train Loss: 0.00005733\n",
      "Epoch: 532, RMSE: 7.5133, MAE: 3.1367, MAPE: 32.3917,Train Loss: 0.00003429\n",
      "Epoch: 532, RMSE: 12.1855, MAE: 4.4714, MAPE: 32.7749,Test Loss: 0.00008939\n",
      "=========epoch 533=========\n",
      "Iteraion 10, Train Loss: 0.00005761\n",
      "Iteraion 20, Train Loss: 0.00005498\n",
      "Iteraion 30, Train Loss: 0.00005357\n",
      "Epoch: 533, RMSE: 7.5248, MAE: 3.1398, MAPE: 32.1634,Train Loss: 0.00003439\n",
      "Epoch: 533, RMSE: 12.2091, MAE: 4.4813, MAPE: 32.6415,Test Loss: 0.00008973\n",
      "=========epoch 534=========\n",
      "Iteraion 10, Train Loss: 0.00005359\n",
      "Iteraion 20, Train Loss: 0.00005156\n",
      "Iteraion 30, Train Loss: 0.00005673\n",
      "Epoch: 534, RMSE: 7.5280, MAE: 3.1390, MAPE: 32.2945,Train Loss: 0.00003442\n",
      "Epoch: 534, RMSE: 12.1795, MAE: 4.4724, MAPE: 32.7693,Test Loss: 0.00008929\n",
      "=========epoch 535=========\n",
      "Iteraion 10, Train Loss: 0.00005423\n",
      "Iteraion 20, Train Loss: 0.00004806\n",
      "Iteraion 30, Train Loss: 0.00005393\n",
      "Epoch: 535, RMSE: 7.5676, MAE: 3.1468, MAPE: 32.1422,Train Loss: 0.00003479\n",
      "Epoch: 535, RMSE: 12.2100, MAE: 4.4792, MAPE: 32.6451,Test Loss: 0.00008975\n",
      "=========epoch 536=========\n",
      "Iteraion 10, Train Loss: 0.00005001\n",
      "Iteraion 20, Train Loss: 0.00005831\n",
      "Iteraion 30, Train Loss: 0.00005380\n",
      "Epoch: 536, RMSE: 7.5786, MAE: 3.1514, MAPE: 32.0401,Train Loss: 0.00003489\n",
      "Epoch: 536, RMSE: 12.2303, MAE: 4.4868, MAPE: 32.5895,Test Loss: 0.00009004\n",
      "=========epoch 537=========\n",
      "Iteraion 10, Train Loss: 0.00005404\n",
      "Iteraion 20, Train Loss: 0.00005350\n",
      "Iteraion 30, Train Loss: 0.00005846\n",
      "Epoch: 537, RMSE: 7.5467, MAE: 3.1416, MAPE: 32.4049,Train Loss: 0.00003459\n",
      "Epoch: 537, RMSE: 12.2190, MAE: 4.4800, MAPE: 32.8724,Test Loss: 0.00008988\n",
      "=========epoch 538=========\n",
      "Iteraion 10, Train Loss: 0.00005777\n",
      "Iteraion 20, Train Loss: 0.00005930\n",
      "Iteraion 30, Train Loss: 0.00005775\n",
      "Epoch: 538, RMSE: 7.5138, MAE: 3.1337, MAPE: 32.4348,Train Loss: 0.00003429\n",
      "Epoch: 538, RMSE: 12.1814, MAE: 4.4700, MAPE: 32.8632,Test Loss: 0.00008933\n",
      "=========epoch 539=========\n",
      "Iteraion 10, Train Loss: 0.00005436\n",
      "Iteraion 20, Train Loss: 0.00005520\n",
      "Iteraion 30, Train Loss: 0.00006171\n",
      "Epoch: 539, RMSE: 7.5008, MAE: 3.1322, MAPE: 32.5259,Train Loss: 0.00003417\n",
      "Epoch: 539, RMSE: 12.1632, MAE: 4.4633, MAPE: 32.9277,Test Loss: 0.00008906\n",
      "=========epoch 540=========\n",
      "Iteraion 10, Train Loss: 0.00004626\n",
      "Iteraion 20, Train Loss: 0.00005852\n",
      "Iteraion 30, Train Loss: 0.00006090\n",
      "Epoch: 540, RMSE: 7.5158, MAE: 3.1391, MAPE: 32.4042,Train Loss: 0.00003431\n",
      "Epoch: 540, RMSE: 12.1893, MAE: 4.4724, MAPE: 32.8880,Test Loss: 0.00008945\n",
      "=========epoch 541=========\n",
      "Iteraion 10, Train Loss: 0.00005379\n",
      "Iteraion 20, Train Loss: 0.00006024\n",
      "Iteraion 30, Train Loss: 0.00005002\n",
      "Epoch: 541, RMSE: 7.5649, MAE: 3.1431, MAPE: 32.1537,Train Loss: 0.00003476\n",
      "Epoch: 541, RMSE: 12.2350, MAE: 4.4855, MAPE: 32.7011,Test Loss: 0.00009011\n",
      "=========epoch 542=========\n",
      "Iteraion 10, Train Loss: 0.00005856\n",
      "Iteraion 20, Train Loss: 0.00005732\n",
      "Iteraion 30, Train Loss: 0.00004893\n",
      "Epoch: 542, RMSE: 7.5015, MAE: 3.1306, MAPE: 32.3691,Train Loss: 0.00003418\n",
      "Epoch: 542, RMSE: 12.2061, MAE: 4.4727, MAPE: 32.8228,Test Loss: 0.00008968\n",
      "=========epoch 543=========\n",
      "Iteraion 10, Train Loss: 0.00005540\n",
      "Iteraion 20, Train Loss: 0.00006221\n",
      "Iteraion 30, Train Loss: 0.00005196\n",
      "Epoch: 543, RMSE: 7.4987, MAE: 3.1263, MAPE: 32.5452,Train Loss: 0.00003415\n",
      "Epoch: 543, RMSE: 12.1890, MAE: 4.4632, MAPE: 32.9443,Test Loss: 0.00008944\n",
      "=========epoch 544=========\n",
      "Iteraion 10, Train Loss: 0.00005093\n",
      "Iteraion 20, Train Loss: 0.00005330\n",
      "Iteraion 30, Train Loss: 0.00005320\n",
      "Epoch: 544, RMSE: 7.5079, MAE: 3.1305, MAPE: 32.2479,Train Loss: 0.00003424\n",
      "Epoch: 544, RMSE: 12.2049, MAE: 4.4768, MAPE: 32.7401,Test Loss: 0.00008967\n",
      "=========epoch 545=========\n",
      "Iteraion 10, Train Loss: 0.00005747\n",
      "Iteraion 20, Train Loss: 0.00006117\n",
      "Iteraion 30, Train Loss: 0.00006137\n",
      "Epoch: 545, RMSE: 7.5229, MAE: 3.1336, MAPE: 32.5135,Train Loss: 0.00003438\n",
      "Epoch: 545, RMSE: 12.1752, MAE: 4.4595, MAPE: 32.9164,Test Loss: 0.00008924\n",
      "=========epoch 546=========\n",
      "Iteraion 10, Train Loss: 0.00005433\n",
      "Iteraion 20, Train Loss: 0.00005609\n",
      "Iteraion 30, Train Loss: 0.00005862\n",
      "Epoch: 546, RMSE: 7.5542, MAE: 3.1370, MAPE: 32.3147,Train Loss: 0.00003466\n",
      "Epoch: 546, RMSE: 12.2192, MAE: 4.4749, MAPE: 32.8179,Test Loss: 0.00008988\n",
      "=========epoch 547=========\n",
      "Iteraion 10, Train Loss: 0.00005583\n",
      "Iteraion 20, Train Loss: 0.00005791\n",
      "Iteraion 30, Train Loss: 0.00005341\n",
      "Epoch: 547, RMSE: 7.4790, MAE: 3.1275, MAPE: 32.5722,Train Loss: 0.00003397\n",
      "Epoch: 547, RMSE: 12.1444, MAE: 4.4531, MAPE: 32.9590,Test Loss: 0.00008878\n",
      "=========epoch 548=========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraion 10, Train Loss: 0.00005858\n",
      "Iteraion 20, Train Loss: 0.00005453\n",
      "Iteraion 30, Train Loss: 0.00005374\n",
      "Epoch: 548, RMSE: 7.4852, MAE: 3.1281, MAPE: 32.6942,Train Loss: 0.00003403\n",
      "Epoch: 548, RMSE: 12.1590, MAE: 4.4605, MAPE: 33.0700,Test Loss: 0.00008899\n",
      "=========epoch 549=========\n",
      "Iteraion 10, Train Loss: 0.00006408\n",
      "Iteraion 20, Train Loss: 0.00004797\n",
      "Iteraion 30, Train Loss: 0.00005387\n",
      "Epoch: 549, RMSE: 7.4886, MAE: 3.1267, MAPE: 32.4721,Train Loss: 0.00003406\n",
      "Epoch: 549, RMSE: 12.1695, MAE: 4.4618, MAPE: 32.8984,Test Loss: 0.00008915\n",
      "=========epoch 550=========\n",
      "Iteraion 10, Train Loss: 0.00005488\n",
      "Iteraion 20, Train Loss: 0.00004559\n",
      "Iteraion 30, Train Loss: 0.00005849\n",
      "Epoch: 550, RMSE: 7.5080, MAE: 3.1372, MAPE: 32.2794,Train Loss: 0.00003424\n",
      "Epoch: 550, RMSE: 12.1946, MAE: 4.4769, MAPE: 32.7750,Test Loss: 0.00008951\n",
      "=========epoch 551=========\n",
      "Iteraion 10, Train Loss: 0.00005370\n",
      "Iteraion 20, Train Loss: 0.00006032\n",
      "Iteraion 30, Train Loss: 0.00004993\n",
      "Epoch: 551, RMSE: 7.4869, MAE: 3.1236, MAPE: 32.4371,Train Loss: 0.00003405\n",
      "Epoch: 551, RMSE: 12.1904, MAE: 4.4666, MAPE: 32.9335,Test Loss: 0.00008946\n",
      "=========epoch 552=========\n",
      "Iteraion 10, Train Loss: 0.00004925\n",
      "Iteraion 20, Train Loss: 0.00004989\n",
      "Iteraion 30, Train Loss: 0.00005379\n",
      "Epoch: 552, RMSE: 7.5174, MAE: 3.1277, MAPE: 32.4997,Train Loss: 0.00003433\n",
      "Epoch: 552, RMSE: 12.2179, MAE: 4.4650, MAPE: 32.9344,Test Loss: 0.00008986\n",
      "=========epoch 553=========\n",
      "Iteraion 10, Train Loss: 0.00005645\n",
      "Iteraion 20, Train Loss: 0.00005766\n",
      "Iteraion 30, Train Loss: 0.00005337\n",
      "Epoch: 553, RMSE: 7.5578, MAE: 3.1465, MAPE: 32.1680,Train Loss: 0.00003470\n",
      "Epoch: 553, RMSE: 12.2352, MAE: 4.4922, MAPE: 32.7330,Test Loss: 0.00009011\n",
      "=========epoch 554=========\n",
      "Iteraion 10, Train Loss: 0.00005075\n",
      "Iteraion 20, Train Loss: 0.00005359\n",
      "Iteraion 30, Train Loss: 0.00005494\n",
      "Epoch: 554, RMSE: 7.4792, MAE: 3.1202, MAPE: 32.4906,Train Loss: 0.00003398\n",
      "Epoch: 554, RMSE: 12.1961, MAE: 4.4630, MAPE: 32.9441,Test Loss: 0.00008953\n",
      "=========epoch 555=========\n",
      "Iteraion 10, Train Loss: 0.00004964\n",
      "Iteraion 20, Train Loss: 0.00005317\n",
      "Iteraion 30, Train Loss: 0.00005139\n",
      "Epoch: 555, RMSE: 7.4912, MAE: 3.1303, MAPE: 32.5346,Train Loss: 0.00003409\n",
      "Epoch: 555, RMSE: 12.1996, MAE: 4.4718, MAPE: 32.9633,Test Loss: 0.00008960\n",
      "=========epoch 556=========\n",
      "Iteraion 10, Train Loss: 0.00005511\n",
      "Iteraion 20, Train Loss: 0.00005331\n",
      "Iteraion 30, Train Loss: 0.00004227\n",
      "Epoch: 556, RMSE: 7.4984, MAE: 3.1243, MAPE: 32.3170,Train Loss: 0.00003415\n",
      "Epoch: 556, RMSE: 12.1954, MAE: 4.4714, MAPE: 32.8357,Test Loss: 0.00008952\n",
      "=========epoch 557=========\n",
      "Iteraion 10, Train Loss: 0.00005448\n",
      "Iteraion 20, Train Loss: 0.00005467\n",
      "Iteraion 30, Train Loss: 0.00005775\n",
      "Epoch: 557, RMSE: 7.4794, MAE: 3.1237, MAPE: 32.4926,Train Loss: 0.00003398\n",
      "Epoch: 557, RMSE: 12.2037, MAE: 4.4670, MAPE: 32.9332,Test Loss: 0.00008965\n",
      "=========epoch 558=========\n",
      "Iteraion 10, Train Loss: 0.00005571\n",
      "Iteraion 20, Train Loss: 0.00004957\n",
      "Iteraion 30, Train Loss: 0.00005458\n",
      "Epoch: 558, RMSE: 7.4985, MAE: 3.1259, MAPE: 32.1719,Train Loss: 0.00003415\n",
      "Epoch: 558, RMSE: 12.2369, MAE: 4.4836, MAPE: 32.7533,Test Loss: 0.00009013\n",
      "=========epoch 559=========\n",
      "Iteraion 10, Train Loss: 0.00005710\n",
      "Iteraion 20, Train Loss: 0.00005257\n",
      "Iteraion 30, Train Loss: 0.00005710\n",
      "Epoch: 559, RMSE: 7.4764, MAE: 3.1173, MAPE: 32.4726,Train Loss: 0.00003395\n",
      "Epoch: 559, RMSE: 12.2187, MAE: 4.4728, MAPE: 32.9759,Test Loss: 0.00008987\n",
      "=========epoch 560=========\n",
      "Iteraion 10, Train Loss: 0.00005882\n",
      "Iteraion 20, Train Loss: 0.00005454\n",
      "Iteraion 30, Train Loss: 0.00005279\n",
      "Epoch: 560, RMSE: 7.4580, MAE: 3.1190, MAPE: 32.6895,Train Loss: 0.00003378\n",
      "Epoch: 560, RMSE: 12.1855, MAE: 4.4685, MAPE: 33.1117,Test Loss: 0.00008939\n",
      "=========epoch 561=========\n",
      "Iteraion 10, Train Loss: 0.00005336\n",
      "Iteraion 20, Train Loss: 0.00005532\n",
      "Iteraion 30, Train Loss: 0.00005228\n",
      "Epoch: 561, RMSE: 7.5435, MAE: 3.1362, MAPE: 32.1813,Train Loss: 0.00003456\n",
      "Epoch: 561, RMSE: 12.2456, MAE: 4.4890, MAPE: 32.7810,Test Loss: 0.00009026\n",
      "=========epoch 562=========\n",
      "Iteraion 10, Train Loss: 0.00006148\n",
      "Iteraion 20, Train Loss: 0.00005034\n",
      "Iteraion 30, Train Loss: 0.00005181\n",
      "Epoch: 562, RMSE: 7.4851, MAE: 3.1236, MAPE: 32.3197,Train Loss: 0.00003403\n",
      "Epoch: 562, RMSE: 12.2075, MAE: 4.4759, MAPE: 32.8325,Test Loss: 0.00008971\n",
      "=========epoch 563=========\n",
      "Iteraion 10, Train Loss: 0.00005269\n",
      "Iteraion 20, Train Loss: 0.00005620\n",
      "Iteraion 30, Train Loss: 0.00005802\n",
      "Epoch: 563, RMSE: 7.4843, MAE: 3.1219, MAPE: 32.3577,Train Loss: 0.00003402\n",
      "Epoch: 563, RMSE: 12.2419, MAE: 4.4757, MAPE: 32.8446,Test Loss: 0.00009021\n",
      "=========epoch 564=========\n",
      "Iteraion 10, Train Loss: 0.00005851\n",
      "Iteraion 20, Train Loss: 0.00005550\n",
      "Iteraion 30, Train Loss: 0.00005333\n",
      "Epoch: 564, RMSE: 7.4753, MAE: 3.1214, MAPE: 32.5865,Train Loss: 0.00003394\n",
      "Epoch: 564, RMSE: 12.2021, MAE: 4.4677, MAPE: 33.0166,Test Loss: 0.00008962\n",
      "=========epoch 565=========\n",
      "Iteraion 10, Train Loss: 0.00005247\n",
      "Iteraion 20, Train Loss: 0.00005250\n",
      "Iteraion 30, Train Loss: 0.00006041\n",
      "Epoch: 565, RMSE: 7.4741, MAE: 3.1195, MAPE: 32.6569,Train Loss: 0.00003393\n",
      "Epoch: 565, RMSE: 12.2034, MAE: 4.4664, MAPE: 33.0386,Test Loss: 0.00008965\n",
      "=========epoch 566=========\n",
      "Iteraion 10, Train Loss: 0.00005166\n",
      "Iteraion 20, Train Loss: 0.00006333\n",
      "Iteraion 30, Train Loss: 0.00005797\n",
      "Epoch: 566, RMSE: 7.4652, MAE: 3.1186, MAPE: 32.6035,Train Loss: 0.00003385\n",
      "Epoch: 566, RMSE: 12.1903, MAE: 4.4641, MAPE: 33.0315,Test Loss: 0.00008945\n",
      "=========epoch 567=========\n",
      "Iteraion 10, Train Loss: 0.00005089\n",
      "Iteraion 20, Train Loss: 0.00005390\n",
      "Iteraion 30, Train Loss: 0.00005452\n",
      "Epoch: 567, RMSE: 7.4807, MAE: 3.1198, MAPE: 32.2946,Train Loss: 0.00003399\n",
      "Epoch: 567, RMSE: 12.1592, MAE: 4.4583, MAPE: 32.8102,Test Loss: 0.00008900\n",
      "=========epoch 568=========\n",
      "Iteraion 10, Train Loss: 0.00005629\n",
      "Iteraion 20, Train Loss: 0.00005584\n",
      "Iteraion 30, Train Loss: 0.00005334\n",
      "Epoch: 568, RMSE: 7.4626, MAE: 3.1177, MAPE: 32.4545,Train Loss: 0.00003383\n",
      "Epoch: 568, RMSE: 12.2513, MAE: 4.4757, MAPE: 32.9304,Test Loss: 0.00009035\n",
      "=========epoch 569=========\n",
      "Iteraion 10, Train Loss: 0.00005204\n",
      "Iteraion 20, Train Loss: 0.00005201\n",
      "Iteraion 30, Train Loss: 0.00005445\n",
      "Epoch: 569, RMSE: 7.5594, MAE: 3.1466, MAPE: 32.0773,Train Loss: 0.00003471\n",
      "Epoch: 569, RMSE: 12.2660, MAE: 4.4986, MAPE: 32.7362,Test Loss: 0.00009056\n",
      "=========epoch 570=========\n",
      "Iteraion 10, Train Loss: 0.00005949\n",
      "Iteraion 20, Train Loss: 0.00005766\n",
      "Iteraion 30, Train Loss: 0.00005201\n",
      "Epoch: 570, RMSE: 7.4851, MAE: 3.1211, MAPE: 32.2226,Train Loss: 0.00003403\n",
      "Epoch: 570, RMSE: 12.2083, MAE: 4.4663, MAPE: 32.7734,Test Loss: 0.00008971\n",
      "=========epoch 571=========\n",
      "Iteraion 10, Train Loss: 0.00005897\n",
      "Iteraion 20, Train Loss: 0.00005164\n",
      "Iteraion 30, Train Loss: 0.00006060\n",
      "Epoch: 571, RMSE: 7.4729, MAE: 3.1245, MAPE: 32.6034,Train Loss: 0.00003392\n",
      "Epoch: 571, RMSE: 12.2218, MAE: 4.4692, MAPE: 33.0424,Test Loss: 0.00008992\n",
      "=========epoch 572=========\n",
      "Iteraion 10, Train Loss: 0.00005844\n",
      "Iteraion 20, Train Loss: 0.00005530\n",
      "Iteraion 30, Train Loss: 0.00005638\n",
      "Epoch: 572, RMSE: 7.4605, MAE: 3.1148, MAPE: 32.5351,Train Loss: 0.00003381\n",
      "Epoch: 572, RMSE: 12.2028, MAE: 4.4686, MAPE: 33.0203,Test Loss: 0.00008963\n",
      "=========epoch 573=========\n",
      "Iteraion 10, Train Loss: 0.00005636\n",
      "Iteraion 20, Train Loss: 0.00005323\n",
      "Iteraion 30, Train Loss: 0.00005292\n",
      "Epoch: 573, RMSE: 7.5110, MAE: 3.1254, MAPE: 32.1859,Train Loss: 0.00003427\n",
      "Epoch: 573, RMSE: 12.2599, MAE: 4.4831, MAPE: 32.7788,Test Loss: 0.00009047\n",
      "=========epoch 574=========\n",
      "Iteraion 10, Train Loss: 0.00005598\n",
      "Iteraion 20, Train Loss: 0.00005884\n",
      "Iteraion 30, Train Loss: 0.00005592\n",
      "Epoch: 574, RMSE: 7.4835, MAE: 3.1195, MAPE: 32.2489,Train Loss: 0.00003402\n",
      "Epoch: 574, RMSE: 12.2236, MAE: 4.4707, MAPE: 32.7801,Test Loss: 0.00008994\n",
      "=========epoch 575=========\n",
      "Iteraion 10, Train Loss: 0.00006208\n",
      "Iteraion 20, Train Loss: 0.00004977\n",
      "Iteraion 30, Train Loss: 0.00005515\n",
      "Epoch: 575, RMSE: 7.4552, MAE: 3.1153, MAPE: 32.5761,Train Loss: 0.00003376\n",
      "Epoch: 575, RMSE: 12.2297, MAE: 4.4746, MAPE: 33.0766,Test Loss: 0.00009004\n",
      "=========epoch 576=========\n",
      "Iteraion 10, Train Loss: 0.00005076\n",
      "Iteraion 20, Train Loss: 0.00005035\n",
      "Iteraion 30, Train Loss: 0.00005915\n",
      "Epoch: 576, RMSE: 7.4839, MAE: 3.1218, MAPE: 32.3288,Train Loss: 0.00003402\n",
      "Epoch: 576, RMSE: 12.2569, MAE: 4.4801, MAPE: 32.8556,Test Loss: 0.00009046\n",
      "=========epoch 577=========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraion 10, Train Loss: 0.00005308\n",
      "Iteraion 20, Train Loss: 0.00005142\n",
      "Iteraion 30, Train Loss: 0.00005037\n",
      "Epoch: 577, RMSE: 7.4301, MAE: 3.1057, MAPE: 32.5770,Train Loss: 0.00003353\n",
      "Epoch: 577, RMSE: 12.2150, MAE: 4.4660, MAPE: 33.0872,Test Loss: 0.00008982\n",
      "=========epoch 578=========\n",
      "Iteraion 10, Train Loss: 0.00005398\n",
      "Iteraion 20, Train Loss: 0.00005474\n",
      "Iteraion 30, Train Loss: 0.00005072\n",
      "Epoch: 578, RMSE: 7.4597, MAE: 3.1098, MAPE: 32.4613,Train Loss: 0.00003380\n",
      "Epoch: 578, RMSE: 12.2187, MAE: 4.4653, MAPE: 32.9939,Test Loss: 0.00008988\n",
      "=========epoch 579=========\n",
      "Iteraion 10, Train Loss: 0.00005462\n",
      "Iteraion 20, Train Loss: 0.00006179\n",
      "Iteraion 30, Train Loss: 0.00005622\n",
      "Epoch: 579, RMSE: 7.5157, MAE: 3.1294, MAPE: 32.1427,Train Loss: 0.00003431\n",
      "Epoch: 579, RMSE: 12.2804, MAE: 4.4973, MAPE: 32.8058,Test Loss: 0.00009078\n",
      "=========epoch 580=========\n",
      "Iteraion 10, Train Loss: 0.00005618\n",
      "Iteraion 20, Train Loss: 0.00005054\n",
      "Iteraion 30, Train Loss: 0.00005475\n",
      "Epoch: 580, RMSE: 7.4289, MAE: 3.1026, MAPE: 32.5886,Train Loss: 0.00003352\n",
      "Epoch: 580, RMSE: 12.1861, MAE: 4.4554, MAPE: 33.0555,Test Loss: 0.00008939\n",
      "=========epoch 581=========\n",
      "Iteraion 10, Train Loss: 0.00005418\n",
      "Iteraion 20, Train Loss: 0.00006026\n",
      "Iteraion 30, Train Loss: 0.00005489\n",
      "Epoch: 581, RMSE: 7.4684, MAE: 3.1074, MAPE: 32.4945,Train Loss: 0.00003388\n",
      "Epoch: 581, RMSE: 12.2307, MAE: 4.4659, MAPE: 33.0279,Test Loss: 0.00009005\n",
      "=========epoch 582=========\n",
      "Iteraion 10, Train Loss: 0.00005243\n",
      "Iteraion 20, Train Loss: 0.00005529\n",
      "Iteraion 30, Train Loss: 0.00005239\n",
      "Epoch: 582, RMSE: 7.4584, MAE: 3.1090, MAPE: 32.3029,Train Loss: 0.00003379\n",
      "Epoch: 582, RMSE: 12.2328, MAE: 4.4730, MAPE: 32.8770,Test Loss: 0.00009008\n",
      "=========epoch 583=========\n",
      "Iteraion 10, Train Loss: 0.00005561\n",
      "Iteraion 20, Train Loss: 0.00005464\n",
      "Iteraion 30, Train Loss: 0.00005793\n",
      "Epoch: 583, RMSE: 7.4746, MAE: 3.1145, MAPE: 32.2991,Train Loss: 0.00003394\n",
      "Epoch: 583, RMSE: 12.2518, MAE: 4.4792, MAPE: 32.9073,Test Loss: 0.00009036\n",
      "=========epoch 584=========\n",
      "Iteraion 10, Train Loss: 0.00005341\n",
      "Iteraion 20, Train Loss: 0.00005122\n",
      "Iteraion 30, Train Loss: 0.00005679\n",
      "Epoch: 584, RMSE: 7.4426, MAE: 3.1056, MAPE: 32.3265,Train Loss: 0.00003365\n",
      "Epoch: 584, RMSE: 12.2228, MAE: 4.4709, MAPE: 32.8993,Test Loss: 0.00008993\n",
      "=========epoch 585=========\n",
      "Iteraion 10, Train Loss: 0.00005764\n",
      "Iteraion 20, Train Loss: 0.00006165\n",
      "Iteraion 30, Train Loss: 0.00004882\n",
      "Epoch: 585, RMSE: 7.4313, MAE: 3.1045, MAPE: 32.4367,Train Loss: 0.00003354\n",
      "Epoch: 585, RMSE: 12.1709, MAE: 4.4520, MAPE: 32.9034,Test Loss: 0.00008917\n",
      "=========epoch 586=========\n",
      "Iteraion 10, Train Loss: 0.00005243\n",
      "Iteraion 20, Train Loss: 0.00005569\n",
      "Iteraion 30, Train Loss: 0.00005552\n",
      "Epoch: 586, RMSE: 7.5264, MAE: 3.1257, MAPE: 32.2786,Train Loss: 0.00003441\n",
      "Epoch: 586, RMSE: 12.2690, MAE: 4.4848, MAPE: 32.9079,Test Loss: 0.00009061\n",
      "=========epoch 587=========\n",
      "Iteraion 10, Train Loss: 0.00005619\n",
      "Iteraion 20, Train Loss: 0.00005492\n",
      "Iteraion 30, Train Loss: 0.00005305\n",
      "Epoch: 587, RMSE: 7.4611, MAE: 3.1220, MAPE: 32.7057,Train Loss: 0.00003381\n",
      "Epoch: 587, RMSE: 12.2098, MAE: 4.4683, MAPE: 33.1892,Test Loss: 0.00008975\n",
      "=========epoch 588=========\n",
      "Iteraion 10, Train Loss: 0.00005409\n",
      "Iteraion 20, Train Loss: 0.00005153\n",
      "Iteraion 30, Train Loss: 0.00005801\n",
      "Epoch: 588, RMSE: 7.5500, MAE: 3.1339, MAPE: 32.2418,Train Loss: 0.00003462\n",
      "Epoch: 588, RMSE: 12.2845, MAE: 4.4937, MAPE: 32.8675,Test Loss: 0.00009084\n",
      "=========epoch 589=========\n",
      "Iteraion 10, Train Loss: 0.00005470\n",
      "Iteraion 20, Train Loss: 0.00005284\n",
      "Iteraion 30, Train Loss: 0.00006244\n",
      "Epoch: 589, RMSE: 7.4117, MAE: 3.1020, MAPE: 32.6711,Train Loss: 0.00003337\n",
      "Epoch: 589, RMSE: 12.2100, MAE: 4.4617, MAPE: 33.1511,Test Loss: 0.00008974\n",
      "=========epoch 590=========\n",
      "Iteraion 10, Train Loss: 0.00005009\n",
      "Iteraion 20, Train Loss: 0.00005430\n",
      "Iteraion 30, Train Loss: 0.00006069\n",
      "Epoch: 590, RMSE: 7.4690, MAE: 3.1086, MAPE: 32.4028,Train Loss: 0.00003389\n",
      "Epoch: 590, RMSE: 12.2140, MAE: 4.4640, MAPE: 32.9694,Test Loss: 0.00008980\n",
      "=========epoch 591=========\n",
      "Iteraion 10, Train Loss: 0.00005652\n",
      "Iteraion 20, Train Loss: 0.00005655\n",
      "Iteraion 30, Train Loss: 0.00005953\n",
      "Epoch: 591, RMSE: 7.4187, MAE: 3.1065, MAPE: 32.7570,Train Loss: 0.00003343\n",
      "Epoch: 591, RMSE: 12.2171, MAE: 4.4638, MAPE: 33.2251,Test Loss: 0.00008985\n",
      "=========epoch 592=========\n",
      "Iteraion 10, Train Loss: 0.00006068\n",
      "Iteraion 20, Train Loss: 0.00004533\n",
      "Iteraion 30, Train Loss: 0.00005744\n",
      "Epoch: 592, RMSE: 7.4289, MAE: 3.1034, MAPE: 32.3719,Train Loss: 0.00003352\n",
      "Epoch: 592, RMSE: 12.2102, MAE: 4.4664, MAPE: 32.9582,Test Loss: 0.00008974\n",
      "=========epoch 593=========\n",
      "Iteraion 10, Train Loss: 0.00005357\n",
      "Iteraion 20, Train Loss: 0.00005050\n",
      "Iteraion 30, Train Loss: 0.00005600\n",
      "Epoch: 593, RMSE: 7.4363, MAE: 3.1046, MAPE: 32.3905,Train Loss: 0.00003359\n",
      "Epoch: 593, RMSE: 12.2173, MAE: 4.4677, MAPE: 33.0021,Test Loss: 0.00008985\n",
      "=========epoch 594=========\n",
      "Iteraion 10, Train Loss: 0.00005538\n",
      "Iteraion 20, Train Loss: 0.00004794\n",
      "Iteraion 30, Train Loss: 0.00004938\n",
      "Epoch: 594, RMSE: 7.4669, MAE: 3.1200, MAPE: 32.6418,Train Loss: 0.00003387\n",
      "Epoch: 594, RMSE: 12.2323, MAE: 4.4727, MAPE: 33.1154,Test Loss: 0.00009007\n",
      "=========epoch 595=========\n",
      "Iteraion 10, Train Loss: 0.00005378\n",
      "Iteraion 20, Train Loss: 0.00004508\n",
      "Iteraion 30, Train Loss: 0.00005198\n",
      "Epoch: 595, RMSE: 7.4506, MAE: 3.1163, MAPE: 32.5810,Train Loss: 0.00003372\n",
      "Epoch: 595, RMSE: 12.2096, MAE: 4.4752, MAPE: 33.1463,Test Loss: 0.00008973\n",
      "=========epoch 596=========\n",
      "Iteraion 10, Train Loss: 0.00005159\n",
      "Iteraion 20, Train Loss: 0.00005414\n",
      "Iteraion 30, Train Loss: 0.00005032\n",
      "Epoch: 596, RMSE: 7.4524, MAE: 3.1019, MAPE: 32.4759,Train Loss: 0.00003373\n",
      "Epoch: 596, RMSE: 12.2275, MAE: 4.4637, MAPE: 33.0313,Test Loss: 0.00009000\n",
      "=========epoch 597=========\n",
      "Iteraion 10, Train Loss: 0.00004665\n",
      "Iteraion 20, Train Loss: 0.00005614\n",
      "Iteraion 30, Train Loss: 0.00005120\n",
      "Epoch: 597, RMSE: 7.4301, MAE: 3.1025, MAPE: 32.7313,Train Loss: 0.00003353\n",
      "Epoch: 597, RMSE: 12.2345, MAE: 4.4685, MAPE: 33.2105,Test Loss: 0.00009010\n",
      "=========epoch 598=========\n",
      "Iteraion 10, Train Loss: 0.00005755\n",
      "Iteraion 20, Train Loss: 0.00005343\n",
      "Iteraion 30, Train Loss: 0.00004807\n",
      "Epoch: 598, RMSE: 7.4519, MAE: 3.1073, MAPE: 32.2857,Train Loss: 0.00003373\n",
      "Epoch: 598, RMSE: 12.2163, MAE: 4.4607, MAPE: 32.8427,Test Loss: 0.00008984\n",
      "=========epoch 599=========\n",
      "Iteraion 10, Train Loss: 0.00005920\n",
      "Iteraion 20, Train Loss: 0.00005716\n",
      "Iteraion 30, Train Loss: 0.00006040\n",
      "Epoch: 599, RMSE: 7.4275, MAE: 3.0981, MAPE: 32.5030,Train Loss: 0.00003351\n",
      "Epoch: 599, RMSE: 12.2055, MAE: 4.4596, MAPE: 33.0514,Test Loss: 0.00008967\n",
      "=========epoch 600=========\n",
      "Iteraion 10, Train Loss: 0.00005185\n",
      "Iteraion 20, Train Loss: 0.00004778\n",
      "Iteraion 30, Train Loss: 0.00005041\n",
      "Epoch: 600, RMSE: 7.4103, MAE: 3.0995, MAPE: 32.7602,Train Loss: 0.00003335\n",
      "Epoch: 600, RMSE: 12.1809, MAE: 4.4598, MAPE: 33.2515,Test Loss: 0.00008931\n",
      "=========epoch 601=========\n",
      "Iteraion 10, Train Loss: 0.00006087\n",
      "Iteraion 20, Train Loss: 0.00006101\n",
      "Iteraion 30, Train Loss: 0.00006214\n",
      "Epoch: 601, RMSE: 7.4314, MAE: 3.1050, MAPE: 32.6357,Train Loss: 0.00003355\n",
      "Epoch: 601, RMSE: 12.2229, MAE: 4.4706, MAPE: 33.1478,Test Loss: 0.00008993\n",
      "=========epoch 602=========\n",
      "Iteraion 10, Train Loss: 0.00005403\n",
      "Iteraion 20, Train Loss: 0.00005344\n",
      "Iteraion 30, Train Loss: 0.00005455\n",
      "Epoch: 602, RMSE: 7.4282, MAE: 3.1003, MAPE: 32.3325,Train Loss: 0.00003352\n",
      "Epoch: 602, RMSE: 12.2811, MAE: 4.4863, MAPE: 32.9667,Test Loss: 0.00009079\n",
      "=========epoch 603=========\n",
      "Iteraion 10, Train Loss: 0.00004802\n",
      "Iteraion 20, Train Loss: 0.00005199\n",
      "Iteraion 30, Train Loss: 0.00005248\n",
      "Epoch: 603, RMSE: 7.4460, MAE: 3.1123, MAPE: 32.2654,Train Loss: 0.00003368\n",
      "Epoch: 603, RMSE: 12.2324, MAE: 4.4764, MAPE: 32.8979,Test Loss: 0.00009008\n",
      "=========epoch 604=========\n",
      "Iteraion 10, Train Loss: 0.00005825\n",
      "Iteraion 20, Train Loss: 0.00005588\n",
      "Iteraion 30, Train Loss: 0.00005533\n",
      "Epoch: 604, RMSE: 7.4734, MAE: 3.1133, MAPE: 32.2202,Train Loss: 0.00003392\n",
      "Epoch: 604, RMSE: 12.2449, MAE: 4.4824, MAPE: 32.8640,Test Loss: 0.00009025\n",
      "=========epoch 605=========\n",
      "Iteraion 10, Train Loss: 0.00005482\n",
      "Iteraion 20, Train Loss: 0.00005357\n",
      "Iteraion 30, Train Loss: 0.00004793\n",
      "Epoch: 605, RMSE: 7.4126, MAE: 3.0967, MAPE: 32.3898,Train Loss: 0.00003338\n",
      "Epoch: 605, RMSE: 12.2478, MAE: 4.4797, MAPE: 33.0020,Test Loss: 0.00009030\n",
      "=========epoch 606=========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraion 10, Train Loss: 0.00004957\n",
      "Iteraion 20, Train Loss: 0.00004804\n",
      "Iteraion 30, Train Loss: 0.00005298\n",
      "Epoch: 606, RMSE: 7.4174, MAE: 3.0947, MAPE: 32.4923,Train Loss: 0.00003342\n",
      "Epoch: 606, RMSE: 12.2212, MAE: 4.4629, MAPE: 33.0764,Test Loss: 0.00008989\n",
      "=========epoch 607=========\n",
      "Iteraion 10, Train Loss: 0.00005246\n",
      "Iteraion 20, Train Loss: 0.00004972\n",
      "Iteraion 30, Train Loss: 0.00005550\n",
      "Epoch: 607, RMSE: 7.3933, MAE: 3.0896, MAPE: 32.6380,Train Loss: 0.00003320\n",
      "Epoch: 607, RMSE: 12.2089, MAE: 4.4601, MAPE: 33.1913,Test Loss: 0.00008973\n",
      "=========epoch 608=========\n",
      "Iteraion 10, Train Loss: 0.00004264\n",
      "Iteraion 20, Train Loss: 0.00005573\n",
      "Iteraion 30, Train Loss: 0.00005776\n",
      "Epoch: 608, RMSE: 7.3927, MAE: 3.0927, MAPE: 32.5351,Train Loss: 0.00003320\n",
      "Epoch: 608, RMSE: 12.2081, MAE: 4.4637, MAPE: 33.0790,Test Loss: 0.00008971\n",
      "=========epoch 609=========\n",
      "Iteraion 10, Train Loss: 0.00005679\n",
      "Iteraion 20, Train Loss: 0.00005159\n",
      "Iteraion 30, Train Loss: 0.00005418\n",
      "Epoch: 609, RMSE: 7.4156, MAE: 3.1005, MAPE: 32.3098,Train Loss: 0.00003340\n",
      "Epoch: 609, RMSE: 12.2309, MAE: 4.4768, MAPE: 32.9663,Test Loss: 0.00009004\n",
      "=========epoch 610=========\n",
      "Iteraion 10, Train Loss: 0.00005673\n",
      "Iteraion 20, Train Loss: 0.00005730\n",
      "Iteraion 30, Train Loss: 0.00005189\n",
      "Epoch: 610, RMSE: 7.4071, MAE: 3.0924, MAPE: 32.4472,Train Loss: 0.00003333\n",
      "Epoch: 610, RMSE: 12.2093, MAE: 4.4525, MAPE: 33.0033,Test Loss: 0.00008972\n",
      "=========epoch 611=========\n",
      "Iteraion 10, Train Loss: 0.00005319\n",
      "Iteraion 20, Train Loss: 0.00004644\n",
      "Iteraion 30, Train Loss: 0.00005387\n",
      "Epoch: 611, RMSE: 7.3990, MAE: 3.0919, MAPE: 32.4350,Train Loss: 0.00003325\n",
      "Epoch: 611, RMSE: 12.2144, MAE: 4.4644, MAPE: 33.0130,Test Loss: 0.00008980\n",
      "=========epoch 612=========\n",
      "Iteraion 10, Train Loss: 0.00005435\n",
      "Iteraion 20, Train Loss: 0.00005553\n",
      "Iteraion 30, Train Loss: 0.00005474\n",
      "Epoch: 612, RMSE: 7.4282, MAE: 3.0994, MAPE: 32.2968,Train Loss: 0.00003352\n",
      "Epoch: 612, RMSE: 12.2262, MAE: 4.4722, MAPE: 32.9822,Test Loss: 0.00008998\n",
      "=========epoch 613=========\n",
      "Iteraion 10, Train Loss: 0.00005162\n",
      "Iteraion 20, Train Loss: 0.00004909\n",
      "Iteraion 30, Train Loss: 0.00005721\n",
      "Epoch: 613, RMSE: 7.4986, MAE: 3.1141, MAPE: 32.2508,Train Loss: 0.00003415\n",
      "Epoch: 613, RMSE: 12.2898, MAE: 4.4885, MAPE: 32.9675,Test Loss: 0.00009091\n",
      "=========epoch 614=========\n",
      "Iteraion 10, Train Loss: 0.00005782\n",
      "Iteraion 20, Train Loss: 0.00005446\n",
      "Iteraion 30, Train Loss: 0.00005245\n",
      "Epoch: 614, RMSE: 7.3879, MAE: 3.0876, MAPE: 32.5501,Train Loss: 0.00003315\n",
      "Epoch: 614, RMSE: 12.1875, MAE: 4.4558, MAPE: 33.1176,Test Loss: 0.00008941\n",
      "=========epoch 615=========\n",
      "Iteraion 10, Train Loss: 0.00005245\n",
      "Iteraion 20, Train Loss: 0.00005158\n",
      "Iteraion 30, Train Loss: 0.00005283\n",
      "Epoch: 615, RMSE: 7.3835, MAE: 3.0860, MAPE: 32.5545,Train Loss: 0.00003311\n",
      "Epoch: 615, RMSE: 12.2226, MAE: 4.4594, MAPE: 33.1491,Test Loss: 0.00008993\n",
      "=========epoch 616=========\n",
      "Iteraion 10, Train Loss: 0.00005303\n",
      "Iteraion 20, Train Loss: 0.00005186\n",
      "Iteraion 30, Train Loss: 0.00005445\n",
      "Epoch: 616, RMSE: 7.3883, MAE: 3.0891, MAPE: 32.8862,Train Loss: 0.00003316\n",
      "Epoch: 616, RMSE: 12.2003, MAE: 4.4545, MAPE: 33.3846,Test Loss: 0.00008961\n",
      "=========epoch 617=========\n",
      "Iteraion 10, Train Loss: 0.00005030\n",
      "Iteraion 20, Train Loss: 0.00005680\n",
      "Iteraion 30, Train Loss: 0.00005288\n",
      "Epoch: 617, RMSE: 7.3766, MAE: 3.0885, MAPE: 32.5654,Train Loss: 0.00003305\n",
      "Epoch: 617, RMSE: 12.2043, MAE: 4.4655, MAPE: 33.1251,Test Loss: 0.00008966\n",
      "=========epoch 618=========\n",
      "Iteraion 10, Train Loss: 0.00006381\n",
      "Iteraion 20, Train Loss: 0.00005476\n",
      "Iteraion 30, Train Loss: 0.00005092\n",
      "Epoch: 618, RMSE: 7.3880, MAE: 3.0904, MAPE: 32.3673,Train Loss: 0.00003315\n",
      "Epoch: 618, RMSE: 12.2257, MAE: 4.4688, MAPE: 33.0048,Test Loss: 0.00008998\n",
      "=========epoch 619=========\n",
      "Iteraion 10, Train Loss: 0.00005076\n",
      "Iteraion 20, Train Loss: 0.00005122\n",
      "Iteraion 30, Train Loss: 0.00005643\n",
      "Epoch: 619, RMSE: 7.3848, MAE: 3.0918, MAPE: 32.6120,Train Loss: 0.00003313\n",
      "Epoch: 619, RMSE: 12.2142, MAE: 4.4658, MAPE: 33.1981,Test Loss: 0.00008981\n",
      "=========epoch 620=========\n",
      "Iteraion 10, Train Loss: 0.00005968\n",
      "Iteraion 20, Train Loss: 0.00005449\n",
      "Iteraion 30, Train Loss: 0.00005471\n",
      "Epoch: 620, RMSE: 7.3691, MAE: 3.0869, MAPE: 32.7080,Train Loss: 0.00003298\n",
      "Epoch: 620, RMSE: 12.2140, MAE: 4.4639, MAPE: 33.2800,Test Loss: 0.00008981\n",
      "=========epoch 621=========\n",
      "Iteraion 10, Train Loss: 0.00005633\n",
      "Iteraion 20, Train Loss: 0.00005471\n",
      "Iteraion 30, Train Loss: 0.00005528\n",
      "Epoch: 621, RMSE: 7.4059, MAE: 3.0890, MAPE: 32.4394,Train Loss: 0.00003332\n",
      "Epoch: 621, RMSE: 12.2895, MAE: 4.4832, MAPE: 33.1345,Test Loss: 0.00009093\n",
      "=========epoch 622=========\n",
      "Iteraion 10, Train Loss: 0.00005056\n",
      "Iteraion 20, Train Loss: 0.00004618\n",
      "Iteraion 30, Train Loss: 0.00005409\n",
      "Epoch: 622, RMSE: 7.3908, MAE: 3.0938, MAPE: 32.6582,Train Loss: 0.00003318\n",
      "Epoch: 622, RMSE: 12.2065, MAE: 4.4580, MAPE: 33.1799,Test Loss: 0.00008970\n",
      "=========epoch 623=========\n",
      "Iteraion 10, Train Loss: 0.00005497\n",
      "Iteraion 20, Train Loss: 0.00004984\n",
      "Iteraion 30, Train Loss: 0.00005393\n",
      "Epoch: 623, RMSE: 7.3795, MAE: 3.0870, MAPE: 32.3194,Train Loss: 0.00003308\n",
      "Epoch: 623, RMSE: 12.2206, MAE: 4.4699, MAPE: 33.0106,Test Loss: 0.00008990\n",
      "=========epoch 624=========\n",
      "Iteraion 10, Train Loss: 0.00005456\n",
      "Iteraion 20, Train Loss: 0.00005146\n",
      "Iteraion 30, Train Loss: 0.00005470\n",
      "Epoch: 624, RMSE: 7.3788, MAE: 3.0853, MAPE: 32.4103,Train Loss: 0.00003307\n",
      "Epoch: 624, RMSE: 12.2346, MAE: 4.4679, MAPE: 33.0509,Test Loss: 0.00009010\n",
      "=========epoch 625=========\n",
      "Iteraion 10, Train Loss: 0.00005635\n",
      "Iteraion 20, Train Loss: 0.00005698\n",
      "Iteraion 30, Train Loss: 0.00005464\n",
      "Epoch: 625, RMSE: 7.3908, MAE: 3.0900, MAPE: 32.5503,Train Loss: 0.00003318\n",
      "Epoch: 625, RMSE: 12.2349, MAE: 4.4620, MAPE: 33.1088,Test Loss: 0.00009011\n",
      "=========epoch 626=========\n",
      "Iteraion 10, Train Loss: 0.00005692\n",
      "Iteraion 20, Train Loss: 0.00005861\n",
      "Iteraion 30, Train Loss: 0.00005974\n",
      "Epoch: 626, RMSE: 7.3657, MAE: 3.0870, MAPE: 32.7490,Train Loss: 0.00003296\n",
      "Epoch: 626, RMSE: 12.1906, MAE: 4.4526, MAPE: 33.2416,Test Loss: 0.00008947\n",
      "=========epoch 627=========\n",
      "Iteraion 10, Train Loss: 0.00005373\n",
      "Iteraion 20, Train Loss: 0.00005733\n",
      "Iteraion 30, Train Loss: 0.00005909\n",
      "Epoch: 627, RMSE: 7.3665, MAE: 3.0804, MAPE: 32.5199,Train Loss: 0.00003296\n",
      "Epoch: 627, RMSE: 12.2423, MAE: 4.4634, MAPE: 33.0931,Test Loss: 0.00009022\n",
      "=========epoch 628=========\n",
      "Iteraion 10, Train Loss: 0.00005082\n",
      "Iteraion 20, Train Loss: 0.00005345\n",
      "Iteraion 30, Train Loss: 0.00005278\n",
      "Epoch: 628, RMSE: 7.3846, MAE: 3.0900, MAPE: 32.3177,Train Loss: 0.00003312\n",
      "Epoch: 628, RMSE: 12.2152, MAE: 4.4704, MAPE: 33.0129,Test Loss: 0.00008982\n",
      "=========epoch 629=========\n",
      "Iteraion 10, Train Loss: 0.00004653\n",
      "Iteraion 20, Train Loss: 0.00005565\n",
      "Iteraion 30, Train Loss: 0.00006193\n",
      "Epoch: 629, RMSE: 7.3873, MAE: 3.0841, MAPE: 32.4643,Train Loss: 0.00003315\n",
      "Epoch: 629, RMSE: 12.2597, MAE: 4.4666, MAPE: 33.0964,Test Loss: 0.00009048\n",
      "=========epoch 630=========\n",
      "Iteraion 10, Train Loss: 0.00005039\n",
      "Iteraion 20, Train Loss: 0.00005228\n",
      "Iteraion 30, Train Loss: 0.00005364\n",
      "Epoch: 630, RMSE: 7.3648, MAE: 3.0853, MAPE: 32.5813,Train Loss: 0.00003295\n",
      "Epoch: 630, RMSE: 12.2103, MAE: 4.4634, MAPE: 33.1788,Test Loss: 0.00008976\n",
      "=========epoch 631=========\n",
      "Iteraion 10, Train Loss: 0.00005520\n",
      "Iteraion 20, Train Loss: 0.00005831\n",
      "Iteraion 30, Train Loss: 0.00004647\n",
      "Epoch: 631, RMSE: 7.3922, MAE: 3.0825, MAPE: 32.5401,Train Loss: 0.00003319\n",
      "Epoch: 631, RMSE: 12.2383, MAE: 4.4582, MAPE: 33.1312,Test Loss: 0.00009017\n",
      "=========epoch 632=========\n",
      "Iteraion 10, Train Loss: 0.00006129\n",
      "Iteraion 20, Train Loss: 0.00004846\n",
      "Iteraion 30, Train Loss: 0.00005501\n",
      "Epoch: 632, RMSE: 7.3843, MAE: 3.0862, MAPE: 32.3316,Train Loss: 0.00003312\n",
      "Epoch: 632, RMSE: 12.1984, MAE: 4.4632, MAPE: 33.0357,Test Loss: 0.00008956\n",
      "=========epoch 633=========\n",
      "Iteraion 10, Train Loss: 0.00004965\n",
      "Iteraion 20, Train Loss: 0.00005500\n",
      "Iteraion 30, Train Loss: 0.00006059\n",
      "Epoch: 633, RMSE: 7.5677, MAE: 3.1350, MAPE: 32.2088,Train Loss: 0.00003479\n",
      "Epoch: 633, RMSE: 12.3546, MAE: 4.5091, MAPE: 32.9484,Test Loss: 0.00009188\n",
      "=========epoch 634=========\n",
      "Iteraion 10, Train Loss: 0.00005200\n",
      "Iteraion 20, Train Loss: 0.00005927\n",
      "Iteraion 30, Train Loss: 0.00005195\n",
      "Epoch: 634, RMSE: 7.3771, MAE: 3.0801, MAPE: 32.5380,Train Loss: 0.00003306\n",
      "Epoch: 634, RMSE: 12.2261, MAE: 4.4593, MAPE: 33.1442,Test Loss: 0.00009000\n",
      "=========epoch 635=========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraion 10, Train Loss: 0.00005403\n",
      "Iteraion 20, Train Loss: 0.00005318\n",
      "Iteraion 30, Train Loss: 0.00005352\n",
      "Epoch: 635, RMSE: 7.3711, MAE: 3.0797, MAPE: 32.4890,Train Loss: 0.00003300\n",
      "Epoch: 635, RMSE: 12.2142, MAE: 4.4539, MAPE: 33.0587,Test Loss: 0.00008981\n",
      "=========epoch 636=========\n",
      "Iteraion 10, Train Loss: 0.00004687\n",
      "Iteraion 20, Train Loss: 0.00004909\n",
      "Iteraion 30, Train Loss: 0.00005035\n",
      "Epoch: 636, RMSE: 7.3638, MAE: 3.0817, MAPE: 32.4521,Train Loss: 0.00003294\n",
      "Epoch: 636, RMSE: 12.2610, MAE: 4.4693, MAPE: 33.0669,Test Loss: 0.00009049\n",
      "=========epoch 637=========\n",
      "Iteraion 10, Train Loss: 0.00005359\n",
      "Iteraion 20, Train Loss: 0.00004938\n",
      "Iteraion 30, Train Loss: 0.00005402\n",
      "Epoch: 637, RMSE: 7.3762, MAE: 3.0881, MAPE: 32.6256,Train Loss: 0.00003305\n",
      "Epoch: 637, RMSE: 12.1937, MAE: 4.4545, MAPE: 33.1919,Test Loss: 0.00008951\n",
      "=========epoch 638=========\n",
      "Iteraion 10, Train Loss: 0.00004619\n",
      "Iteraion 20, Train Loss: 0.00004960\n",
      "Iteraion 30, Train Loss: 0.00005314\n",
      "Epoch: 638, RMSE: 7.3710, MAE: 3.0801, MAPE: 32.4432,Train Loss: 0.00003300\n",
      "Epoch: 638, RMSE: 12.2418, MAE: 4.4653, MAPE: 33.1029,Test Loss: 0.00009021\n",
      "=========epoch 639=========\n",
      "Iteraion 10, Train Loss: 0.00005548\n",
      "Iteraion 20, Train Loss: 0.00005609\n",
      "Iteraion 30, Train Loss: 0.00005014\n",
      "Epoch: 639, RMSE: 7.3538, MAE: 3.0794, MAPE: 32.3532,Train Loss: 0.00003285\n",
      "Epoch: 639, RMSE: 12.2658, MAE: 4.4773, MAPE: 33.0427,Test Loss: 0.00009057\n",
      "=========epoch 640=========\n",
      "Iteraion 10, Train Loss: 0.00005436\n",
      "Iteraion 20, Train Loss: 0.00005790\n",
      "Iteraion 30, Train Loss: 0.00005048\n",
      "Epoch: 640, RMSE: 7.4897, MAE: 3.1143, MAPE: 32.2012,Train Loss: 0.00003407\n",
      "Epoch: 640, RMSE: 12.3295, MAE: 4.5014, MAPE: 32.9443,Test Loss: 0.00009150\n",
      "=========epoch 641=========\n",
      "Iteraion 10, Train Loss: 0.00004838\n",
      "Iteraion 20, Train Loss: 0.00005613\n",
      "Iteraion 30, Train Loss: 0.00005423\n",
      "Epoch: 641, RMSE: 7.3773, MAE: 3.0922, MAPE: 32.4785,Train Loss: 0.00003306\n",
      "Epoch: 641, RMSE: 12.2223, MAE: 4.4694, MAPE: 33.1599,Test Loss: 0.00008993\n",
      "=========epoch 642=========\n",
      "Iteraion 10, Train Loss: 0.00005289\n",
      "Iteraion 20, Train Loss: 0.00005397\n",
      "Iteraion 30, Train Loss: 0.00005394\n",
      "Epoch: 642, RMSE: 7.3877, MAE: 3.0838, MAPE: 32.3497,Train Loss: 0.00003315\n",
      "Epoch: 642, RMSE: 12.2782, MAE: 4.4694, MAPE: 33.0500,Test Loss: 0.00009076\n",
      "=========epoch 643=========\n",
      "Iteraion 10, Train Loss: 0.00004942\n",
      "Iteraion 20, Train Loss: 0.00004949\n",
      "Iteraion 30, Train Loss: 0.00005805\n",
      "Epoch: 643, RMSE: 7.4090, MAE: 3.0877, MAPE: 32.4495,Train Loss: 0.00003334\n",
      "Epoch: 643, RMSE: 12.2365, MAE: 4.4578, MAPE: 33.0671,Test Loss: 0.00009013\n",
      "=========epoch 644=========\n",
      "Iteraion 10, Train Loss: 0.00005412\n",
      "Iteraion 20, Train Loss: 0.00004975\n",
      "Iteraion 30, Train Loss: 0.00005491\n",
      "Epoch: 644, RMSE: 7.3636, MAE: 3.0907, MAPE: 32.7940,Train Loss: 0.00003294\n",
      "Epoch: 644, RMSE: 12.2200, MAE: 4.4662, MAPE: 33.3595,Test Loss: 0.00008989\n",
      "=========epoch 645=========\n",
      "Iteraion 10, Train Loss: 0.00005673\n",
      "Iteraion 20, Train Loss: 0.00005227\n",
      "Iteraion 30, Train Loss: 0.00005120\n",
      "Epoch: 645, RMSE: 7.3560, MAE: 3.0757, MAPE: 32.3559,Train Loss: 0.00003287\n",
      "Epoch: 645, RMSE: 12.2382, MAE: 4.4677, MAPE: 33.0461,Test Loss: 0.00009016\n",
      "=========epoch 646=========\n",
      "Iteraion 10, Train Loss: 0.00005125\n",
      "Iteraion 20, Train Loss: 0.00005797\n",
      "Iteraion 30, Train Loss: 0.00005392\n",
      "Epoch: 646, RMSE: 7.3867, MAE: 3.0830, MAPE: 32.3373,Train Loss: 0.00003314\n",
      "Epoch: 646, RMSE: 12.2447, MAE: 4.4667, MAPE: 33.0349,Test Loss: 0.00009025\n",
      "=========epoch 647=========\n",
      "Iteraion 10, Train Loss: 0.00005437\n",
      "Iteraion 20, Train Loss: 0.00005362\n",
      "Iteraion 30, Train Loss: 0.00005441\n",
      "Epoch: 647, RMSE: 7.3669, MAE: 3.0798, MAPE: 32.3127,Train Loss: 0.00003296\n",
      "Epoch: 647, RMSE: 12.2317, MAE: 4.4660, MAPE: 33.0200,Test Loss: 0.00009005\n",
      "=========epoch 648=========\n",
      "Iteraion 10, Train Loss: 0.00005812\n",
      "Iteraion 20, Train Loss: 0.00005558\n",
      "Iteraion 30, Train Loss: 0.00005681\n",
      "Epoch: 648, RMSE: 7.3587, MAE: 3.0781, MAPE: 32.4517,Train Loss: 0.00003289\n",
      "Epoch: 648, RMSE: 12.2702, MAE: 4.4724, MAPE: 33.0439,Test Loss: 0.00009063\n",
      "=========epoch 649=========\n",
      "Iteraion 10, Train Loss: 0.00005988\n",
      "Iteraion 20, Train Loss: 0.00005134\n",
      "Iteraion 30, Train Loss: 0.00005614\n",
      "Epoch: 649, RMSE: 7.3623, MAE: 3.0811, MAPE: 32.4696,Train Loss: 0.00003292\n",
      "Epoch: 649, RMSE: 12.2611, MAE: 4.4735, MAPE: 33.1538,Test Loss: 0.00009050\n",
      "=========epoch 650=========\n",
      "Iteraion 10, Train Loss: 0.00005426\n",
      "Iteraion 20, Train Loss: 0.00005767\n",
      "Iteraion 30, Train Loss: 0.00005035\n",
      "Epoch: 650, RMSE: 7.3341, MAE: 3.0710, MAPE: 32.5153,Train Loss: 0.00003267\n",
      "Epoch: 650, RMSE: 12.1858, MAE: 4.4495, MAPE: 33.1616,Test Loss: 0.00008939\n",
      "=========epoch 651=========\n",
      "Iteraion 10, Train Loss: 0.00005770\n",
      "Iteraion 20, Train Loss: 0.00005352\n",
      "Iteraion 30, Train Loss: 0.00004937\n",
      "Epoch: 651, RMSE: 7.3434, MAE: 3.0771, MAPE: 32.6143,Train Loss: 0.00003275\n",
      "Epoch: 651, RMSE: 12.2064, MAE: 4.4567, MAPE: 33.2140,Test Loss: 0.00008969\n",
      "=========epoch 652=========\n",
      "Iteraion 10, Train Loss: 0.00005014\n",
      "Iteraion 20, Train Loss: 0.00005201\n",
      "Iteraion 30, Train Loss: 0.00005499\n",
      "Epoch: 652, RMSE: 7.3532, MAE: 3.0756, MAPE: 32.4281,Train Loss: 0.00003284\n",
      "Epoch: 652, RMSE: 12.2326, MAE: 4.4583, MAPE: 33.0940,Test Loss: 0.00009008\n",
      "=========epoch 653=========\n",
      "Iteraion 10, Train Loss: 0.00005133\n",
      "Iteraion 20, Train Loss: 0.00004934\n",
      "Iteraion 30, Train Loss: 0.00005661\n",
      "Epoch: 653, RMSE: 7.3709, MAE: 3.0896, MAPE: 32.8111,Train Loss: 0.00003300\n",
      "Epoch: 653, RMSE: 12.2332, MAE: 4.4686, MAPE: 33.4314,Test Loss: 0.00009009\n",
      "=========epoch 654=========\n",
      "Iteraion 10, Train Loss: 0.00005669\n",
      "Iteraion 20, Train Loss: 0.00005541\n",
      "Iteraion 30, Train Loss: 0.00005273\n",
      "Epoch: 654, RMSE: 7.3683, MAE: 3.0766, MAPE: 32.3841,Train Loss: 0.00003298\n",
      "Epoch: 654, RMSE: 12.2375, MAE: 4.4615, MAPE: 33.0803,Test Loss: 0.00009016\n",
      "=========epoch 655=========\n",
      "Iteraion 10, Train Loss: 0.00004646\n",
      "Iteraion 20, Train Loss: 0.00005623\n",
      "Iteraion 30, Train Loss: 0.00004903\n",
      "Epoch: 655, RMSE: 7.3823, MAE: 3.0809, MAPE: 32.3242,Train Loss: 0.00003310\n",
      "Epoch: 655, RMSE: 12.2603, MAE: 4.4700, MAPE: 33.0578,Test Loss: 0.00009049\n",
      "=========epoch 656=========\n",
      "Iteraion 10, Train Loss: 0.00004744\n",
      "Iteraion 20, Train Loss: 0.00004978\n",
      "Iteraion 30, Train Loss: 0.00005504\n",
      "Epoch: 656, RMSE: 7.3506, MAE: 3.0829, MAPE: 33.1323,Train Loss: 0.00003282\n",
      "Epoch: 656, RMSE: 12.2132, MAE: 4.4545, MAPE: 33.6775,Test Loss: 0.00008979\n",
      "=========epoch 657=========\n",
      "Iteraion 10, Train Loss: 0.00005474\n",
      "Iteraion 20, Train Loss: 0.00005875\n",
      "Iteraion 30, Train Loss: 0.00005693\n",
      "Epoch: 657, RMSE: 7.3193, MAE: 3.0663, MAPE: 32.7546,Train Loss: 0.00003254\n",
      "Epoch: 657, RMSE: 12.2054, MAE: 4.4471, MAPE: 33.3399,Test Loss: 0.00008967\n",
      "=========epoch 658=========\n",
      "Iteraion 10, Train Loss: 0.00005062\n",
      "Iteraion 20, Train Loss: 0.00005825\n",
      "Iteraion 30, Train Loss: 0.00005290\n",
      "Epoch: 658, RMSE: 7.4137, MAE: 3.0958, MAPE: 32.2659,Train Loss: 0.00003338\n",
      "Epoch: 658, RMSE: 12.2609, MAE: 4.4778, MAPE: 32.9802,Test Loss: 0.00009050\n",
      "=========epoch 659=========\n",
      "Iteraion 10, Train Loss: 0.00005721\n",
      "Iteraion 20, Train Loss: 0.00005215\n",
      "Iteraion 30, Train Loss: 0.00005526\n",
      "Epoch: 659, RMSE: 7.3140, MAE: 3.0661, MAPE: 32.5583,Train Loss: 0.00003249\n",
      "Epoch: 659, RMSE: 12.1848, MAE: 4.4442, MAPE: 33.1606,Test Loss: 0.00008938\n",
      "=========epoch 660=========\n",
      "Iteraion 10, Train Loss: 0.00005063\n",
      "Iteraion 20, Train Loss: 0.00005213\n",
      "Iteraion 30, Train Loss: 0.00006288\n",
      "Epoch: 660, RMSE: 7.3182, MAE: 3.0675, MAPE: 32.6674,Train Loss: 0.00003253\n",
      "Epoch: 660, RMSE: 12.2504, MAE: 4.4628, MAPE: 33.2931,Test Loss: 0.00009034\n",
      "=========epoch 661=========\n",
      "Iteraion 10, Train Loss: 0.00005445\n",
      "Iteraion 20, Train Loss: 0.00004634\n",
      "Iteraion 30, Train Loss: 0.00005651\n",
      "Epoch: 661, RMSE: 7.3174, MAE: 3.0694, MAPE: 32.7685,Train Loss: 0.00003252\n",
      "Epoch: 661, RMSE: 12.2217, MAE: 4.4554, MAPE: 33.3151,Test Loss: 0.00008992\n",
      "=========epoch 662=========\n",
      "Iteraion 10, Train Loss: 0.00005847\n",
      "Iteraion 20, Train Loss: 0.00005602\n",
      "Iteraion 30, Train Loss: 0.00005439\n",
      "Epoch: 662, RMSE: 7.3209, MAE: 3.0684, MAPE: 32.6191,Train Loss: 0.00003256\n",
      "Epoch: 662, RMSE: 12.2277, MAE: 4.4590, MAPE: 33.2277,Test Loss: 0.00009001\n",
      "=========epoch 663=========\n",
      "Iteraion 10, Train Loss: 0.00005797\n",
      "Iteraion 20, Train Loss: 0.00005986\n",
      "Iteraion 30, Train Loss: 0.00005005\n",
      "Epoch: 663, RMSE: 7.3246, MAE: 3.0671, MAPE: 32.6173,Train Loss: 0.00003259\n",
      "Epoch: 663, RMSE: 12.2317, MAE: 4.4561, MAPE: 33.2437,Test Loss: 0.00009007\n",
      "=========epoch 664=========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraion 10, Train Loss: 0.00005352\n",
      "Iteraion 20, Train Loss: 0.00004942\n",
      "Iteraion 30, Train Loss: 0.00005314\n",
      "Epoch: 664, RMSE: 7.4018, MAE: 3.0893, MAPE: 32.2438,Train Loss: 0.00003328\n",
      "Epoch: 664, RMSE: 12.2795, MAE: 4.4816, MAPE: 32.9817,Test Loss: 0.00009077\n",
      "=========epoch 665=========\n",
      "Iteraion 10, Train Loss: 0.00005371\n",
      "Iteraion 20, Train Loss: 0.00005632\n",
      "Iteraion 30, Train Loss: 0.00005153\n",
      "Epoch: 665, RMSE: 7.3227, MAE: 3.0667, MAPE: 32.8819,Train Loss: 0.00003257\n",
      "Epoch: 665, RMSE: 12.2121, MAE: 4.4502, MAPE: 33.4452,Test Loss: 0.00008978\n",
      "=========epoch 666=========\n",
      "Iteraion 10, Train Loss: 0.00005054\n",
      "Iteraion 20, Train Loss: 0.00005036\n",
      "Iteraion 30, Train Loss: 0.00005341\n",
      "Epoch: 666, RMSE: 7.3132, MAE: 3.0645, MAPE: 32.5938,Train Loss: 0.00003249\n",
      "Epoch: 666, RMSE: 12.2444, MAE: 4.4584, MAPE: 33.2334,Test Loss: 0.00009026\n",
      "=========epoch 667=========\n",
      "Iteraion 10, Train Loss: 0.00005516\n",
      "Iteraion 20, Train Loss: 0.00005323\n",
      "Iteraion 30, Train Loss: 0.00005192\n",
      "Epoch: 667, RMSE: 7.3039, MAE: 3.0707, MAPE: 32.8412,Train Loss: 0.00003240\n",
      "Epoch: 667, RMSE: 12.2128, MAE: 4.4561, MAPE: 33.4229,Test Loss: 0.00008979\n",
      "=========epoch 668=========\n",
      "Iteraion 10, Train Loss: 0.00005115\n",
      "Iteraion 20, Train Loss: 0.00005882\n",
      "Iteraion 30, Train Loss: 0.00005379\n",
      "Epoch: 668, RMSE: 7.2980, MAE: 3.0621, MAPE: 32.9046,Train Loss: 0.00003235\n",
      "Epoch: 668, RMSE: 12.2429, MAE: 4.4587, MAPE: 33.4936,Test Loss: 0.00009022\n",
      "=========epoch 669=========\n",
      "Iteraion 10, Train Loss: 0.00005157\n",
      "Iteraion 20, Train Loss: 0.00005559\n",
      "Iteraion 30, Train Loss: 0.00005417\n",
      "Epoch: 669, RMSE: 7.3274, MAE: 3.0784, MAPE: 32.9024,Train Loss: 0.00003261\n",
      "Epoch: 669, RMSE: 12.2491, MAE: 4.4668, MAPE: 33.5019,Test Loss: 0.00009032\n",
      "=========epoch 670=========\n",
      "Iteraion 10, Train Loss: 0.00005086\n",
      "Iteraion 20, Train Loss: 0.00005406\n",
      "Iteraion 30, Train Loss: 0.00004940\n",
      "Epoch: 670, RMSE: 7.3104, MAE: 3.0650, MAPE: 32.7285,Train Loss: 0.00003246\n",
      "Epoch: 670, RMSE: 12.2420, MAE: 4.4572, MAPE: 33.3177,Test Loss: 0.00009022\n",
      "=========epoch 671=========\n",
      "Iteraion 10, Train Loss: 0.00004984\n",
      "Iteraion 20, Train Loss: 0.00005800\n",
      "Iteraion 30, Train Loss: 0.00005021\n",
      "Epoch: 671, RMSE: 7.3132, MAE: 3.0685, MAPE: 32.5642,Train Loss: 0.00003249\n",
      "Epoch: 671, RMSE: 12.2075, MAE: 4.4535, MAPE: 33.1997,Test Loss: 0.00008971\n",
      "=========epoch 672=========\n",
      "Iteraion 10, Train Loss: 0.00005100\n",
      "Iteraion 20, Train Loss: 0.00005362\n",
      "Iteraion 30, Train Loss: 0.00005521\n",
      "Epoch: 672, RMSE: 7.3020, MAE: 3.0628, MAPE: 32.5780,Train Loss: 0.00003239\n",
      "Epoch: 672, RMSE: 12.2224, MAE: 4.4528, MAPE: 33.2236,Test Loss: 0.00008993\n",
      "=========epoch 673=========\n",
      "Iteraion 10, Train Loss: 0.00005623\n",
      "Iteraion 20, Train Loss: 0.00005580\n",
      "Iteraion 30, Train Loss: 0.00006210\n",
      "Epoch: 673, RMSE: 7.2990, MAE: 3.0583, MAPE: 32.6123,Train Loss: 0.00003236\n",
      "Epoch: 673, RMSE: 12.2147, MAE: 4.4441, MAPE: 33.2341,Test Loss: 0.00008982\n",
      "=========epoch 674=========\n",
      "Iteraion 10, Train Loss: 0.00005297\n",
      "Iteraion 20, Train Loss: 0.00005302\n",
      "Iteraion 30, Train Loss: 0.00004925\n",
      "Epoch: 674, RMSE: 7.3060, MAE: 3.0610, MAPE: 32.6136,Train Loss: 0.00003242\n",
      "Epoch: 674, RMSE: 12.2190, MAE: 4.4591, MAPE: 33.2770,Test Loss: 0.00008987\n",
      "=========epoch 675=========\n",
      "Iteraion 10, Train Loss: 0.00005509\n",
      "Iteraion 20, Train Loss: 0.00005677\n",
      "Iteraion 30, Train Loss: 0.00005144\n",
      "Epoch: 675, RMSE: 7.3157, MAE: 3.0627, MAPE: 32.4993,Train Loss: 0.00003251\n",
      "Epoch: 675, RMSE: 12.2275, MAE: 4.4571, MAPE: 33.1581,Test Loss: 0.00009000\n",
      "=========epoch 676=========\n",
      "Iteraion 10, Train Loss: 0.00005460\n",
      "Iteraion 20, Train Loss: 0.00005374\n",
      "Iteraion 30, Train Loss: 0.00005385\n",
      "Epoch: 676, RMSE: 7.3315, MAE: 3.0688, MAPE: 32.3714,Train Loss: 0.00003265\n",
      "Epoch: 676, RMSE: 12.2133, MAE: 4.4581, MAPE: 33.0865,Test Loss: 0.00008979\n",
      "=========epoch 677=========\n",
      "Iteraion 10, Train Loss: 0.00004916\n",
      "Iteraion 20, Train Loss: 0.00004708\n",
      "Iteraion 30, Train Loss: 0.00004478\n",
      "Epoch: 677, RMSE: 7.2989, MAE: 3.0603, MAPE: 32.6722,Train Loss: 0.00003236\n",
      "Epoch: 677, RMSE: 12.2054, MAE: 4.4542, MAPE: 33.3188,Test Loss: 0.00008968\n",
      "=========epoch 678=========\n",
      "Iteraion 10, Train Loss: 0.00004971\n",
      "Iteraion 20, Train Loss: 0.00005297\n",
      "Iteraion 30, Train Loss: 0.00005555\n",
      "Epoch: 678, RMSE: 7.3100, MAE: 3.0634, MAPE: 32.5398,Train Loss: 0.00003246\n",
      "Epoch: 678, RMSE: 12.2044, MAE: 4.4468, MAPE: 33.1974,Test Loss: 0.00008967\n",
      "=========epoch 679=========\n",
      "Iteraion 10, Train Loss: 0.00005248\n",
      "Iteraion 20, Train Loss: 0.00004671\n",
      "Iteraion 30, Train Loss: 0.00005098\n",
      "Epoch: 679, RMSE: 7.3295, MAE: 3.0850, MAPE: 33.0273,Train Loss: 0.00003263\n",
      "Epoch: 679, RMSE: 12.2465, MAE: 4.4718, MAPE: 33.5730,Test Loss: 0.00009028\n",
      "=========epoch 680=========\n",
      "Iteraion 10, Train Loss: 0.00005394\n",
      "Iteraion 20, Train Loss: 0.00005465\n",
      "Iteraion 30, Train Loss: 0.00005258\n",
      "Epoch: 680, RMSE: 7.2796, MAE: 3.0622, MAPE: 32.9243,Train Loss: 0.00003219\n",
      "Epoch: 680, RMSE: 12.2112, MAE: 4.4511, MAPE: 33.5129,Test Loss: 0.00008976\n",
      "=========epoch 681=========\n",
      "Iteraion 10, Train Loss: 0.00005372\n",
      "Iteraion 20, Train Loss: 0.00004838\n",
      "Iteraion 30, Train Loss: 0.00006049\n",
      "Epoch: 681, RMSE: 7.3137, MAE: 3.0606, MAPE: 32.3602,Train Loss: 0.00003249\n",
      "Epoch: 681, RMSE: 12.2563, MAE: 4.4617, MAPE: 33.0938,Test Loss: 0.00009043\n",
      "=========epoch 682=========\n",
      "Iteraion 10, Train Loss: 0.00004822\n",
      "Iteraion 20, Train Loss: 0.00004685\n",
      "Iteraion 30, Train Loss: 0.00005138\n",
      "Epoch: 682, RMSE: 7.3558, MAE: 3.0708, MAPE: 32.4374,Train Loss: 0.00003287\n",
      "Epoch: 682, RMSE: 12.2948, MAE: 4.4718, MAPE: 33.1717,Test Loss: 0.00009099\n",
      "=========epoch 683=========\n",
      "Iteraion 10, Train Loss: 0.00005110\n",
      "Iteraion 20, Train Loss: 0.00005157\n",
      "Iteraion 30, Train Loss: 0.00005212\n",
      "Epoch: 683, RMSE: 7.2960, MAE: 3.0572, MAPE: 32.6454,Train Loss: 0.00003233\n",
      "Epoch: 683, RMSE: 12.2113, MAE: 4.4510, MAPE: 33.3202,Test Loss: 0.00008976\n",
      "=========epoch 684=========\n",
      "Iteraion 10, Train Loss: 0.00005464\n",
      "Iteraion 20, Train Loss: 0.00004887\n",
      "Iteraion 30, Train Loss: 0.00005265\n",
      "Epoch: 684, RMSE: 7.2890, MAE: 3.0596, MAPE: 32.5666,Train Loss: 0.00003227\n",
      "Epoch: 684, RMSE: 12.2365, MAE: 4.4620, MAPE: 33.2573,Test Loss: 0.00009012\n",
      "=========epoch 685=========\n",
      "Iteraion 10, Train Loss: 0.00005654\n",
      "Iteraion 20, Train Loss: 0.00004841\n",
      "Iteraion 30, Train Loss: 0.00005014\n",
      "Epoch: 685, RMSE: 7.2843, MAE: 3.0610, MAPE: 32.8140,Train Loss: 0.00003223\n",
      "Epoch: 685, RMSE: 12.2263, MAE: 4.4563, MAPE: 33.4332,Test Loss: 0.00008999\n",
      "=========epoch 686=========\n",
      "Iteraion 10, Train Loss: 0.00005086\n",
      "Iteraion 20, Train Loss: 0.00004975\n",
      "Iteraion 30, Train Loss: 0.00005065\n",
      "Epoch: 686, RMSE: 7.2830, MAE: 3.0545, MAPE: 32.7713,Train Loss: 0.00003222\n",
      "Epoch: 686, RMSE: 12.1725, MAE: 4.4403, MAPE: 33.3921,Test Loss: 0.00008919\n",
      "=========epoch 687=========\n",
      "Iteraion 10, Train Loss: 0.00005224\n",
      "Iteraion 20, Train Loss: 0.00005651\n",
      "Iteraion 30, Train Loss: 0.00005421\n",
      "Epoch: 687, RMSE: 7.2863, MAE: 3.0558, MAPE: 32.6326,Train Loss: 0.00003225\n",
      "Epoch: 687, RMSE: 12.1985, MAE: 4.4470, MAPE: 33.2841,Test Loss: 0.00008957\n",
      "=========epoch 688=========\n",
      "Iteraion 10, Train Loss: 0.00005492\n",
      "Iteraion 20, Train Loss: 0.00005655\n",
      "Iteraion 30, Train Loss: 0.00005576\n",
      "Epoch: 688, RMSE: 7.3310, MAE: 3.0635, MAPE: 32.6357,Train Loss: 0.00003264\n",
      "Epoch: 688, RMSE: 12.3014, MAE: 4.4656, MAPE: 33.2992,Test Loss: 0.00009110\n",
      "=========epoch 689=========\n",
      "Iteraion 10, Train Loss: 0.00005501\n",
      "Iteraion 20, Train Loss: 0.00005169\n",
      "Iteraion 30, Train Loss: 0.00005458\n",
      "Epoch: 689, RMSE: 7.2832, MAE: 3.0543, MAPE: 32.7181,Train Loss: 0.00003222\n",
      "Epoch: 689, RMSE: 12.1897, MAE: 4.4425, MAPE: 33.3218,Test Loss: 0.00008944\n",
      "=========epoch 690=========\n",
      "Iteraion 10, Train Loss: 0.00005040\n",
      "Iteraion 20, Train Loss: 0.00005031\n",
      "Iteraion 30, Train Loss: 0.00004861\n",
      "Epoch: 690, RMSE: 7.2833, MAE: 3.0586, MAPE: 32.6599,Train Loss: 0.00003222\n",
      "Epoch: 690, RMSE: 12.2115, MAE: 4.4562, MAPE: 33.3419,Test Loss: 0.00008976\n",
      "=========epoch 691=========\n",
      "Iteraion 10, Train Loss: 0.00004943\n",
      "Iteraion 20, Train Loss: 0.00005361\n",
      "Iteraion 30, Train Loss: 0.00005171\n",
      "Epoch: 691, RMSE: 7.3035, MAE: 3.0569, MAPE: 32.4807,Train Loss: 0.00003240\n",
      "Epoch: 691, RMSE: 12.2290, MAE: 4.4579, MAPE: 33.2046,Test Loss: 0.00009001\n",
      "=========epoch 692=========\n",
      "Iteraion 10, Train Loss: 0.00004833\n",
      "Iteraion 20, Train Loss: 0.00005689\n",
      "Iteraion 30, Train Loss: 0.00005427\n",
      "Epoch: 692, RMSE: 7.3191, MAE: 3.0582, MAPE: 32.5056,Train Loss: 0.00003254\n",
      "Epoch: 692, RMSE: 12.2735, MAE: 4.4586, MAPE: 33.2233,Test Loss: 0.00009068\n",
      "=========epoch 693=========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraion 10, Train Loss: 0.00005700\n",
      "Iteraion 20, Train Loss: 0.00006180\n",
      "Iteraion 30, Train Loss: 0.00005326\n",
      "Epoch: 693, RMSE: 7.3060, MAE: 3.0623, MAPE: 32.5623,Train Loss: 0.00003242\n",
      "Epoch: 693, RMSE: 12.2503, MAE: 4.4648, MAPE: 33.2913,Test Loss: 0.00009034\n",
      "=========epoch 694=========\n",
      "Iteraion 10, Train Loss: 0.00005478\n",
      "Iteraion 20, Train Loss: 0.00005287\n",
      "Iteraion 30, Train Loss: 0.00005943\n",
      "Epoch: 694, RMSE: 7.3506, MAE: 3.0708, MAPE: 32.4171,Train Loss: 0.00003282\n",
      "Epoch: 694, RMSE: 12.3041, MAE: 4.4808, MAPE: 33.1858,Test Loss: 0.00009114\n",
      "=========epoch 695=========\n",
      "Iteraion 10, Train Loss: 0.00005354\n",
      "Iteraion 20, Train Loss: 0.00005633\n",
      "Iteraion 30, Train Loss: 0.00004507\n",
      "Epoch: 695, RMSE: 7.3080, MAE: 3.0693, MAPE: 32.9018,Train Loss: 0.00003244\n",
      "Epoch: 695, RMSE: 12.2281, MAE: 4.4599, MAPE: 33.5341,Test Loss: 0.00009001\n",
      "=========epoch 696=========\n",
      "Iteraion 10, Train Loss: 0.00005310\n",
      "Iteraion 20, Train Loss: 0.00005276\n",
      "Iteraion 30, Train Loss: 0.00005382\n",
      "Epoch: 696, RMSE: 7.3150, MAE: 3.0586, MAPE: 32.4235,Train Loss: 0.00003250\n",
      "Epoch: 696, RMSE: 12.2536, MAE: 4.4625, MAPE: 33.1696,Test Loss: 0.00009038\n",
      "=========epoch 697=========\n",
      "Iteraion 10, Train Loss: 0.00005088\n",
      "Iteraion 20, Train Loss: 0.00005065\n",
      "Iteraion 30, Train Loss: 0.00006378\n",
      "Epoch: 697, RMSE: 7.3424, MAE: 3.0713, MAPE: 32.2957,Train Loss: 0.00003275\n",
      "Epoch: 697, RMSE: 12.2707, MAE: 4.4765, MAPE: 33.0909,Test Loss: 0.00009063\n",
      "=========epoch 698=========\n",
      "Iteraion 10, Train Loss: 0.00005471\n",
      "Iteraion 20, Train Loss: 0.00005101\n",
      "Iteraion 30, Train Loss: 0.00005007\n",
      "Epoch: 698, RMSE: 7.3018, MAE: 3.0539, MAPE: 32.5942,Train Loss: 0.00003239\n",
      "Epoch: 698, RMSE: 12.2599, MAE: 4.4569, MAPE: 33.3176,Test Loss: 0.00009048\n",
      "=========epoch 699=========\n",
      "Iteraion 10, Train Loss: 0.00004864\n",
      "Iteraion 20, Train Loss: 0.00004951\n",
      "Iteraion 30, Train Loss: 0.00004951\n",
      "Epoch: 699, RMSE: 7.2799, MAE: 3.0495, MAPE: 32.6277,Train Loss: 0.00003219\n",
      "Epoch: 699, RMSE: 12.2666, MAE: 4.4556, MAPE: 33.2862,Test Loss: 0.00009058\n",
      "=========epoch 700=========\n",
      "Iteraion 10, Train Loss: 0.00005833\n",
      "Iteraion 20, Train Loss: 0.00005289\n",
      "Iteraion 30, Train Loss: 0.00005533\n",
      "Epoch: 700, RMSE: 7.2687, MAE: 3.0519, MAPE: 32.5793,Train Loss: 0.00003209\n",
      "Epoch: 700, RMSE: 12.2142, MAE: 4.4539, MAPE: 33.2774,Test Loss: 0.00008980\n",
      "=========epoch 701=========\n",
      "Iteraion 10, Train Loss: 0.00005479\n",
      "Iteraion 20, Train Loss: 0.00004454\n",
      "Iteraion 30, Train Loss: 0.00005283\n",
      "Epoch: 701, RMSE: 7.3016, MAE: 3.0548, MAPE: 32.6941,Train Loss: 0.00003238\n",
      "Epoch: 701, RMSE: 12.2823, MAE: 4.4630, MAPE: 33.2917,Test Loss: 0.00009080\n",
      "=========epoch 702=========\n",
      "Iteraion 10, Train Loss: 0.00004900\n",
      "Iteraion 20, Train Loss: 0.00005395\n",
      "Iteraion 30, Train Loss: 0.00004840\n",
      "Epoch: 702, RMSE: 7.2908, MAE: 3.0577, MAPE: 32.5005,Train Loss: 0.00003229\n",
      "Epoch: 702, RMSE: 12.2472, MAE: 4.4592, MAPE: 33.2330,Test Loss: 0.00009029\n",
      "=========epoch 703=========\n",
      "Iteraion 10, Train Loss: 0.00005848\n",
      "Iteraion 20, Train Loss: 0.00006136\n",
      "Iteraion 30, Train Loss: 0.00005784\n",
      "Epoch: 703, RMSE: 7.2891, MAE: 3.0598, MAPE: 32.3313,Train Loss: 0.00003227\n",
      "Epoch: 703, RMSE: 12.2344, MAE: 4.4643, MAPE: 33.1285,Test Loss: 0.00009011\n",
      "=========epoch 704=========\n",
      "Iteraion 10, Train Loss: 0.00004923\n",
      "Iteraion 20, Train Loss: 0.00005853\n",
      "Iteraion 30, Train Loss: 0.00005181\n",
      "Epoch: 704, RMSE: 7.2977, MAE: 3.0561, MAPE: 32.4155,Train Loss: 0.00003235\n",
      "Epoch: 704, RMSE: 12.2668, MAE: 4.4623, MAPE: 33.1597,Test Loss: 0.00009059\n",
      "=========epoch 705=========\n",
      "Iteraion 10, Train Loss: 0.00004589\n",
      "Iteraion 20, Train Loss: 0.00004708\n",
      "Iteraion 30, Train Loss: 0.00005405\n",
      "Epoch: 705, RMSE: 7.3219, MAE: 3.0591, MAPE: 32.3683,Train Loss: 0.00003257\n",
      "Epoch: 705, RMSE: 12.2929, MAE: 4.4735, MAPE: 33.2173,Test Loss: 0.00009098\n",
      "=========epoch 706=========\n",
      "Iteraion 10, Train Loss: 0.00005377\n",
      "Iteraion 20, Train Loss: 0.00005026\n",
      "Iteraion 30, Train Loss: 0.00004904\n",
      "Epoch: 706, RMSE: 7.2740, MAE: 3.0523, MAPE: 32.5298,Train Loss: 0.00003214\n",
      "Epoch: 706, RMSE: 12.2610, MAE: 4.4690, MAPE: 33.3018,Test Loss: 0.00009050\n",
      "=========epoch 707=========\n",
      "Iteraion 10, Train Loss: 0.00005414\n",
      "Iteraion 20, Train Loss: 0.00005579\n",
      "Iteraion 30, Train Loss: 0.00005274\n",
      "Epoch: 707, RMSE: 7.3096, MAE: 3.0655, MAPE: 32.4482,Train Loss: 0.00003245\n",
      "Epoch: 707, RMSE: 12.2772, MAE: 4.4775, MAPE: 33.2512,Test Loss: 0.00009074\n",
      "=========epoch 708=========\n",
      "Iteraion 10, Train Loss: 0.00005507\n",
      "Iteraion 20, Train Loss: 0.00005309\n",
      "Iteraion 30, Train Loss: 0.00005659\n",
      "Epoch: 708, RMSE: 7.2683, MAE: 3.0503, MAPE: 32.6738,Train Loss: 0.00003209\n",
      "Epoch: 708, RMSE: 12.2058, MAE: 4.4510, MAPE: 33.4048,Test Loss: 0.00008967\n",
      "=========epoch 709=========\n",
      "Iteraion 10, Train Loss: 0.00005195\n",
      "Iteraion 20, Train Loss: 0.00005288\n",
      "Iteraion 30, Train Loss: 0.00005560\n",
      "Epoch: 709, RMSE: 7.2728, MAE: 3.0484, MAPE: 32.4424,Train Loss: 0.00003213\n",
      "Epoch: 709, RMSE: 12.2367, MAE: 4.4575, MAPE: 33.2412,Test Loss: 0.00009013\n",
      "=========epoch 710=========\n",
      "Iteraion 10, Train Loss: 0.00004521\n",
      "Iteraion 20, Train Loss: 0.00005663\n",
      "Iteraion 30, Train Loss: 0.00005583\n",
      "Epoch: 710, RMSE: 7.2565, MAE: 3.0500, MAPE: 32.7416,Train Loss: 0.00003198\n",
      "Epoch: 710, RMSE: 12.2315, MAE: 4.4554, MAPE: 33.4482,Test Loss: 0.00009006\n",
      "=========epoch 711=========\n",
      "Iteraion 10, Train Loss: 0.00005457\n",
      "Iteraion 20, Train Loss: 0.00005154\n",
      "Iteraion 30, Train Loss: 0.00005728\n",
      "Epoch: 711, RMSE: 7.2895, MAE: 3.0511, MAPE: 32.4546,Train Loss: 0.00003228\n",
      "Epoch: 711, RMSE: 12.2448, MAE: 4.4548, MAPE: 33.1909,Test Loss: 0.00009026\n",
      "=========epoch 712=========\n",
      "Iteraion 10, Train Loss: 0.00005228\n",
      "Iteraion 20, Train Loss: 0.00005643\n",
      "Iteraion 30, Train Loss: 0.00005589\n",
      "Epoch: 712, RMSE: 7.2773, MAE: 3.0551, MAPE: 32.4676,Train Loss: 0.00003217\n",
      "Epoch: 712, RMSE: 12.2366, MAE: 4.4604, MAPE: 33.2505,Test Loss: 0.00009012\n",
      "=========epoch 713=========\n",
      "Iteraion 10, Train Loss: 0.00005959\n",
      "Iteraion 20, Train Loss: 0.00005636\n",
      "Iteraion 30, Train Loss: 0.00004994\n",
      "Epoch: 713, RMSE: 7.2655, MAE: 3.0485, MAPE: 32.5026,Train Loss: 0.00003206\n",
      "Epoch: 713, RMSE: 12.2348, MAE: 4.4582, MAPE: 33.2281,Test Loss: 0.00009011\n",
      "=========epoch 714=========\n",
      "Iteraion 10, Train Loss: 0.00005367\n",
      "Iteraion 20, Train Loss: 0.00005340\n",
      "Iteraion 30, Train Loss: 0.00005069\n",
      "Epoch: 714, RMSE: 7.2773, MAE: 3.0589, MAPE: 32.6177,Train Loss: 0.00003217\n",
      "Epoch: 714, RMSE: 12.2445, MAE: 4.4669, MAPE: 33.3524,Test Loss: 0.00009025\n",
      "=========epoch 715=========\n",
      "Iteraion 10, Train Loss: 0.00005458\n",
      "Iteraion 20, Train Loss: 0.00005527\n",
      "Iteraion 30, Train Loss: 0.00005247\n",
      "Epoch: 715, RMSE: 7.2596, MAE: 3.0445, MAPE: 32.7101,Train Loss: 0.00003201\n",
      "Epoch: 715, RMSE: 12.2602, MAE: 4.4566, MAPE: 33.3958,Test Loss: 0.00009048\n",
      "=========epoch 716=========\n",
      "Iteraion 10, Train Loss: 0.00004990\n",
      "Iteraion 20, Train Loss: 0.00005021\n",
      "Iteraion 30, Train Loss: 0.00005475\n",
      "Epoch: 716, RMSE: 7.3076, MAE: 3.0574, MAPE: 32.3452,Train Loss: 0.00003244\n",
      "Epoch: 716, RMSE: 12.3136, MAE: 4.4800, MAPE: 33.1524,Test Loss: 0.00009127\n",
      "=========epoch 717=========\n",
      "Iteraion 10, Train Loss: 0.00005425\n",
      "Iteraion 20, Train Loss: 0.00006069\n",
      "Iteraion 30, Train Loss: 0.00004820\n",
      "Epoch: 717, RMSE: 7.2953, MAE: 3.0609, MAPE: 32.3574,Train Loss: 0.00003233\n",
      "Epoch: 717, RMSE: 12.2313, MAE: 4.4644, MAPE: 33.0772,Test Loss: 0.00009006\n",
      "=========epoch 718=========\n",
      "Iteraion 10, Train Loss: 0.00005091\n",
      "Iteraion 20, Train Loss: 0.00005563\n",
      "Iteraion 30, Train Loss: 0.00004918\n",
      "Epoch: 718, RMSE: 7.2391, MAE: 3.0406, MAPE: 32.5017,Train Loss: 0.00003183\n",
      "Epoch: 718, RMSE: 12.2628, MAE: 4.4651, MAPE: 33.2712,Test Loss: 0.00009052\n",
      "=========epoch 719=========\n",
      "Iteraion 10, Train Loss: 0.00004730\n",
      "Iteraion 20, Train Loss: 0.00005050\n",
      "Iteraion 30, Train Loss: 0.00005508\n",
      "Epoch: 719, RMSE: 7.2774, MAE: 3.0530, MAPE: 32.5723,Train Loss: 0.00003217\n",
      "Epoch: 719, RMSE: 12.2357, MAE: 4.4628, MAPE: 33.2790,Test Loss: 0.00009013\n",
      "=========epoch 720=========\n",
      "Iteraion 10, Train Loss: 0.00004420\n",
      "Iteraion 20, Train Loss: 0.00005253\n",
      "Iteraion 30, Train Loss: 0.00004936\n",
      "Epoch: 720, RMSE: 7.2717, MAE: 3.0532, MAPE: 32.3809,Train Loss: 0.00003212\n",
      "Epoch: 720, RMSE: 12.3232, MAE: 4.4853, MAPE: 33.1362,Test Loss: 0.00009141\n",
      "=========epoch 721=========\n",
      "Iteraion 10, Train Loss: 0.00005337\n",
      "Iteraion 20, Train Loss: 0.00005838\n",
      "Iteraion 30, Train Loss: 0.00005788\n",
      "Epoch: 721, RMSE: 7.2518, MAE: 3.0505, MAPE: 32.7349,Train Loss: 0.00003194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 721, RMSE: 12.2716, MAE: 4.4701, MAPE: 33.4035,Test Loss: 0.00009066\n",
      "=========epoch 722=========\n",
      "Iteraion 10, Train Loss: 0.00004971\n",
      "Iteraion 20, Train Loss: 0.00005362\n",
      "Iteraion 30, Train Loss: 0.00005123\n",
      "Epoch: 722, RMSE: 7.2446, MAE: 3.0498, MAPE: 32.9717,Train Loss: 0.00003188\n",
      "Epoch: 722, RMSE: 12.2515, MAE: 4.4598, MAPE: 33.6099,Test Loss: 0.00009035\n",
      "=========epoch 723=========\n",
      "Iteraion 10, Train Loss: 0.00005101\n",
      "Iteraion 20, Train Loss: 0.00005482\n",
      "Iteraion 30, Train Loss: 0.00005061\n",
      "Epoch: 723, RMSE: 7.2583, MAE: 3.0476, MAPE: 32.5971,Train Loss: 0.00003200\n",
      "Epoch: 723, RMSE: 12.2319, MAE: 4.4527, MAPE: 33.2660,Test Loss: 0.00009006\n",
      "=========epoch 724=========\n",
      "Iteraion 10, Train Loss: 0.00005659\n",
      "Iteraion 20, Train Loss: 0.00006007\n",
      "Iteraion 30, Train Loss: 0.00005169\n",
      "Epoch: 724, RMSE: 7.2557, MAE: 3.0479, MAPE: 32.7526,Train Loss: 0.00003198\n",
      "Epoch: 724, RMSE: 12.2554, MAE: 4.4539, MAPE: 33.3844,Test Loss: 0.00009041\n",
      "=========epoch 725=========\n",
      "Iteraion 10, Train Loss: 0.00005223\n",
      "Iteraion 20, Train Loss: 0.00004779\n",
      "Iteraion 30, Train Loss: 0.00006006\n",
      "Epoch: 725, RMSE: 7.2586, MAE: 3.0461, MAPE: 32.4426,Train Loss: 0.00003200\n",
      "Epoch: 725, RMSE: 12.2561, MAE: 4.4647, MAPE: 33.1998,Test Loss: 0.00009043\n",
      "=========epoch 726=========\n",
      "Iteraion 10, Train Loss: 0.00005428\n",
      "Iteraion 20, Train Loss: 0.00005431\n",
      "Iteraion 30, Train Loss: 0.00005508\n",
      "Epoch: 726, RMSE: 7.2662, MAE: 3.0489, MAPE: 32.3114,Train Loss: 0.00003207\n",
      "Epoch: 726, RMSE: 12.2673, MAE: 4.4717, MAPE: 33.1206,Test Loss: 0.00009058\n",
      "=========epoch 727=========\n",
      "Iteraion 10, Train Loss: 0.00004829\n",
      "Iteraion 20, Train Loss: 0.00005384\n",
      "Iteraion 30, Train Loss: 0.00005696\n",
      "Epoch: 727, RMSE: 7.2297, MAE: 3.0411, MAPE: 32.5133,Train Loss: 0.00003175\n",
      "Epoch: 727, RMSE: 12.2217, MAE: 4.4516, MAPE: 33.2695,Test Loss: 0.00008991\n",
      "=========epoch 728=========\n",
      "Iteraion 10, Train Loss: 0.00005836\n",
      "Iteraion 20, Train Loss: 0.00005061\n",
      "Iteraion 30, Train Loss: 0.00005215\n",
      "Epoch: 728, RMSE: 7.2834, MAE: 3.0506, MAPE: 32.3436,Train Loss: 0.00003222\n",
      "Epoch: 728, RMSE: 12.2891, MAE: 4.4706, MAPE: 33.1450,Test Loss: 0.00009091\n",
      "=========epoch 729=========\n",
      "Iteraion 10, Train Loss: 0.00005959\n",
      "Iteraion 20, Train Loss: 0.00006157\n",
      "Iteraion 30, Train Loss: 0.00004705\n",
      "Epoch: 729, RMSE: 7.2457, MAE: 3.0405, MAPE: 32.4053,Train Loss: 0.00003189\n",
      "Epoch: 729, RMSE: 12.2321, MAE: 4.4500, MAPE: 33.1445,Test Loss: 0.00009007\n",
      "=========epoch 730=========\n",
      "Iteraion 10, Train Loss: 0.00005270\n",
      "Iteraion 20, Train Loss: 0.00006067\n",
      "Iteraion 30, Train Loss: 0.00004982\n",
      "Epoch: 730, RMSE: 7.2453, MAE: 3.0408, MAPE: 32.5206,Train Loss: 0.00003189\n",
      "Epoch: 730, RMSE: 12.2321, MAE: 4.4583, MAPE: 33.2942,Test Loss: 0.00009007\n",
      "=========epoch 731=========\n",
      "Iteraion 10, Train Loss: 0.00005504\n",
      "Iteraion 20, Train Loss: 0.00005494\n",
      "Iteraion 30, Train Loss: 0.00005223\n",
      "Epoch: 731, RMSE: 7.2388, MAE: 3.0441, MAPE: 32.5649,Train Loss: 0.00003183\n",
      "Epoch: 731, RMSE: 12.2355, MAE: 4.4594, MAPE: 33.2859,Test Loss: 0.00009012\n",
      "=========epoch 732=========\n",
      "Iteraion 10, Train Loss: 0.00004815\n",
      "Iteraion 20, Train Loss: 0.00005359\n",
      "Iteraion 30, Train Loss: 0.00005408\n",
      "Epoch: 732, RMSE: 7.2239, MAE: 3.0385, MAPE: 32.4820,Train Loss: 0.00003170\n",
      "Epoch: 732, RMSE: 12.2135, MAE: 4.4468, MAPE: 33.2147,Test Loss: 0.00008980\n",
      "=========epoch 733=========\n",
      "Iteraion 10, Train Loss: 0.00005560\n",
      "Iteraion 20, Train Loss: 0.00005283\n",
      "Iteraion 30, Train Loss: 0.00004943\n",
      "Epoch: 733, RMSE: 7.2248, MAE: 3.0399, MAPE: 32.8042,Train Loss: 0.00003171\n",
      "Epoch: 733, RMSE: 12.2359, MAE: 4.4615, MAPE: 33.5074,Test Loss: 0.00009012\n",
      "=========epoch 734=========\n",
      "Iteraion 10, Train Loss: 0.00005587\n",
      "Iteraion 20, Train Loss: 0.00005366\n",
      "Iteraion 30, Train Loss: 0.00005380\n",
      "Epoch: 734, RMSE: 7.2578, MAE: 3.0425, MAPE: 32.6697,Train Loss: 0.00003200\n",
      "Epoch: 734, RMSE: 12.2660, MAE: 4.4630, MAPE: 33.3687,Test Loss: 0.00009057\n",
      "=========epoch 735=========\n",
      "Iteraion 10, Train Loss: 0.00005273\n",
      "Iteraion 20, Train Loss: 0.00005227\n",
      "Iteraion 30, Train Loss: 0.00005021\n",
      "Epoch: 735, RMSE: 7.2461, MAE: 3.0396, MAPE: 32.6899,Train Loss: 0.00003189\n",
      "Epoch: 735, RMSE: 12.2638, MAE: 4.4575, MAPE: 33.3608,Test Loss: 0.00009054\n",
      "=========epoch 736=========\n",
      "Iteraion 10, Train Loss: 0.00004719\n",
      "Iteraion 20, Train Loss: 0.00005238\n",
      "Iteraion 30, Train Loss: 0.00005895\n",
      "Epoch: 736, RMSE: 7.2328, MAE: 3.0392, MAPE: 32.4768,Train Loss: 0.00003178\n",
      "Epoch: 736, RMSE: 12.2792, MAE: 4.4696, MAPE: 33.2414,Test Loss: 0.00009077\n",
      "=========epoch 737=========\n",
      "Iteraion 10, Train Loss: 0.00005051\n",
      "Iteraion 20, Train Loss: 0.00004588\n",
      "Iteraion 30, Train Loss: 0.00004889\n",
      "Epoch: 737, RMSE: 7.2513, MAE: 3.0394, MAPE: 32.4150,Train Loss: 0.00003194\n",
      "Epoch: 737, RMSE: 12.2614, MAE: 4.4577, MAPE: 33.1668,Test Loss: 0.00009051\n",
      "=========epoch 738=========\n",
      "Iteraion 10, Train Loss: 0.00005499\n",
      "Iteraion 20, Train Loss: 0.00004428\n",
      "Iteraion 30, Train Loss: 0.00005348\n",
      "Epoch: 738, RMSE: 7.2246, MAE: 3.0372, MAPE: 32.6822,Train Loss: 0.00003171\n",
      "Epoch: 738, RMSE: 12.2722, MAE: 4.4620, MAPE: 33.3926,Test Loss: 0.00009067\n",
      "=========epoch 739=========\n",
      "Iteraion 10, Train Loss: 0.00006312\n",
      "Iteraion 20, Train Loss: 0.00004703\n",
      "Iteraion 30, Train Loss: 0.00005787\n",
      "Epoch: 739, RMSE: 7.2212, MAE: 3.0337, MAPE: 32.6749,Train Loss: 0.00003168\n",
      "Epoch: 739, RMSE: 12.2371, MAE: 4.4522, MAPE: 33.3996,Test Loss: 0.00009014\n",
      "=========epoch 740=========\n",
      "Iteraion 10, Train Loss: 0.00005607\n",
      "Iteraion 20, Train Loss: 0.00004870\n",
      "Iteraion 30, Train Loss: 0.00004858\n",
      "Epoch: 740, RMSE: 7.2203, MAE: 3.0360, MAPE: 32.6540,Train Loss: 0.00003167\n",
      "Epoch: 740, RMSE: 12.2297, MAE: 4.4476, MAPE: 33.3705,Test Loss: 0.00009004\n",
      "=========epoch 741=========\n",
      "Iteraion 10, Train Loss: 0.00005387\n",
      "Iteraion 20, Train Loss: 0.00005525\n",
      "Iteraion 30, Train Loss: 0.00005552\n",
      "Epoch: 741, RMSE: 7.2327, MAE: 3.0405, MAPE: 32.3373,Train Loss: 0.00003177\n",
      "Epoch: 741, RMSE: 12.2740, MAE: 4.4698, MAPE: 33.1466,Test Loss: 0.00009068\n",
      "=========epoch 742=========\n",
      "Iteraion 10, Train Loss: 0.00005533\n",
      "Iteraion 20, Train Loss: 0.00005291\n",
      "Iteraion 30, Train Loss: 0.00005225\n",
      "Epoch: 742, RMSE: 7.1957, MAE: 3.0321, MAPE: 32.7478,Train Loss: 0.00003145\n",
      "Epoch: 742, RMSE: 12.2306, MAE: 4.4481, MAPE: 33.4510,Test Loss: 0.00009005\n",
      "=========epoch 743=========\n",
      "Iteraion 10, Train Loss: 0.00005065\n",
      "Iteraion 20, Train Loss: 0.00005347\n",
      "Iteraion 30, Train Loss: 0.00005334\n",
      "Epoch: 743, RMSE: 7.2150, MAE: 3.0302, MAPE: 32.6339,Train Loss: 0.00003162\n",
      "Epoch: 743, RMSE: 12.2465, MAE: 4.4500, MAPE: 33.3607,Test Loss: 0.00009028\n",
      "=========epoch 744=========\n",
      "Iteraion 10, Train Loss: 0.00005093\n",
      "Iteraion 20, Train Loss: 0.00005141\n",
      "Iteraion 30, Train Loss: 0.00005773\n",
      "Epoch: 744, RMSE: 7.2466, MAE: 3.0423, MAPE: 32.7850,Train Loss: 0.00003190\n",
      "Epoch: 744, RMSE: 12.3030, MAE: 4.4650, MAPE: 33.4706,Test Loss: 0.00009111\n",
      "=========epoch 745=========\n",
      "Iteraion 10, Train Loss: 0.00004888\n",
      "Iteraion 20, Train Loss: 0.00005078\n",
      "Iteraion 30, Train Loss: 0.00005884\n",
      "Epoch: 745, RMSE: 7.3147, MAE: 3.0602, MAPE: 32.4333,Train Loss: 0.00003250\n",
      "Epoch: 745, RMSE: 12.3042, MAE: 4.4716, MAPE: 33.1547,Test Loss: 0.00009112\n",
      "=========epoch 746=========\n",
      "Iteraion 10, Train Loss: 0.00005732\n",
      "Iteraion 20, Train Loss: 0.00005183\n",
      "Iteraion 30, Train Loss: 0.00005046\n",
      "Epoch: 746, RMSE: 7.2138, MAE: 3.0354, MAPE: 32.4095,Train Loss: 0.00003161\n",
      "Epoch: 746, RMSE: 12.2801, MAE: 4.4608, MAPE: 33.1560,Test Loss: 0.00009078\n",
      "=========epoch 747=========\n",
      "Iteraion 10, Train Loss: 0.00004743\n",
      "Iteraion 20, Train Loss: 0.00005583\n",
      "Iteraion 30, Train Loss: 0.00005257\n",
      "Epoch: 747, RMSE: 7.2160, MAE: 3.0315, MAPE: 32.4335,Train Loss: 0.00003163\n",
      "Epoch: 747, RMSE: 12.2784, MAE: 4.4612, MAPE: 33.1933,Test Loss: 0.00009076\n",
      "=========epoch 748=========\n",
      "Iteraion 10, Train Loss: 0.00005018\n",
      "Iteraion 20, Train Loss: 0.00004824\n",
      "Iteraion 30, Train Loss: 0.00004734\n",
      "Epoch: 748, RMSE: 7.2100, MAE: 3.0371, MAPE: 32.7060,Train Loss: 0.00003158\n",
      "Epoch: 748, RMSE: 12.2179, MAE: 4.4469, MAPE: 33.4107,Test Loss: 0.00008987\n",
      "=========epoch 749=========\n",
      "Iteraion 10, Train Loss: 0.00005877\n",
      "Iteraion 20, Train Loss: 0.00005128\n",
      "Iteraion 30, Train Loss: 0.00005550\n",
      "Epoch: 749, RMSE: 7.2519, MAE: 3.0450, MAPE: 32.4440,Train Loss: 0.00003195\n",
      "Epoch: 749, RMSE: 12.2830, MAE: 4.4734, MAPE: 33.2562,Test Loss: 0.00009083\n",
      "=========epoch 750=========\n",
      "Iteraion 10, Train Loss: 0.00005181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraion 20, Train Loss: 0.00005059\n",
      "Iteraion 30, Train Loss: 0.00005413\n",
      "Epoch: 750, RMSE: 7.2199, MAE: 3.0321, MAPE: 32.4059,Train Loss: 0.00003166\n",
      "Epoch: 750, RMSE: 12.2564, MAE: 4.4609, MAPE: 33.2499,Test Loss: 0.00009043\n",
      "=========epoch 751=========\n",
      "Iteraion 10, Train Loss: 0.00005847\n",
      "Iteraion 20, Train Loss: 0.00004822\n",
      "Iteraion 30, Train Loss: 0.00003859\n",
      "Epoch: 751, RMSE: 7.1683, MAE: 3.0203, MAPE: 32.5487,Train Loss: 0.00003121\n",
      "Epoch: 751, RMSE: 12.2172, MAE: 4.4490, MAPE: 33.3195,Test Loss: 0.00008985\n",
      "=========epoch 752=========\n",
      "Iteraion 10, Train Loss: 0.00005112\n",
      "Iteraion 20, Train Loss: 0.00004875\n",
      "Iteraion 30, Train Loss: 0.00004876\n",
      "Epoch: 752, RMSE: 7.1608, MAE: 3.0201, MAPE: 32.6667,Train Loss: 0.00003115\n",
      "Epoch: 752, RMSE: 12.2073, MAE: 4.4463, MAPE: 33.4022,Test Loss: 0.00008971\n",
      "=========epoch 753=========\n",
      "Iteraion 10, Train Loss: 0.00005465\n",
      "Iteraion 20, Train Loss: 0.00005369\n",
      "Iteraion 30, Train Loss: 0.00005299\n",
      "Epoch: 753, RMSE: 7.1711, MAE: 3.0184, MAPE: 32.4748,Train Loss: 0.00003124\n",
      "Epoch: 753, RMSE: 12.2227, MAE: 4.4489, MAPE: 33.2750,Test Loss: 0.00008993\n",
      "=========epoch 754=========\n",
      "Iteraion 10, Train Loss: 0.00005399\n",
      "Iteraion 20, Train Loss: 0.00004966\n",
      "Iteraion 30, Train Loss: 0.00005793\n",
      "Epoch: 754, RMSE: 7.1848, MAE: 3.0209, MAPE: 32.3930,Train Loss: 0.00003136\n",
      "Epoch: 754, RMSE: 12.2307, MAE: 4.4510, MAPE: 33.1938,Test Loss: 0.00009005\n",
      "=========epoch 755=========\n",
      "Iteraion 10, Train Loss: 0.00004616\n",
      "Iteraion 20, Train Loss: 0.00005144\n",
      "Iteraion 30, Train Loss: 0.00004995\n",
      "Epoch: 755, RMSE: 7.1823, MAE: 3.0204, MAPE: 32.4160,Train Loss: 0.00003133\n",
      "Epoch: 755, RMSE: 12.2313, MAE: 4.4501, MAPE: 33.1988,Test Loss: 0.00009006\n",
      "=========epoch 756=========\n",
      "Iteraion 10, Train Loss: 0.00005037\n",
      "Iteraion 20, Train Loss: 0.00004439\n",
      "Iteraion 30, Train Loss: 0.00005345\n",
      "Epoch: 756, RMSE: 7.1628, MAE: 3.0172, MAPE: 32.4689,Train Loss: 0.00003116\n",
      "Epoch: 756, RMSE: 12.2179, MAE: 4.4471, MAPE: 33.2431,Test Loss: 0.00008986\n",
      "=========epoch 757=========\n",
      "Iteraion 10, Train Loss: 0.00005209\n",
      "Iteraion 20, Train Loss: 0.00005941\n",
      "Iteraion 30, Train Loss: 0.00005082\n",
      "Epoch: 757, RMSE: 7.1578, MAE: 3.0186, MAPE: 32.6435,Train Loss: 0.00003112\n",
      "Epoch: 757, RMSE: 12.2078, MAE: 4.4460, MAPE: 33.3903,Test Loss: 0.00008972\n",
      "=========epoch 758=========\n",
      "Iteraion 10, Train Loss: 0.00005012\n",
      "Iteraion 20, Train Loss: 0.00004570\n",
      "Iteraion 30, Train Loss: 0.00004979\n",
      "Epoch: 758, RMSE: 7.1626, MAE: 3.0169, MAPE: 32.5182,Train Loss: 0.00003116\n",
      "Epoch: 758, RMSE: 12.2132, MAE: 4.4461, MAPE: 33.2858,Test Loss: 0.00008979\n",
      "=========epoch 759=========\n",
      "Iteraion 10, Train Loss: 0.00004867\n",
      "Iteraion 20, Train Loss: 0.00005719\n",
      "Iteraion 30, Train Loss: 0.00004816\n",
      "Epoch: 759, RMSE: 7.1647, MAE: 3.0161, MAPE: 32.4898,Train Loss: 0.00003118\n",
      "Epoch: 759, RMSE: 12.2280, MAE: 4.4474, MAPE: 33.2680,Test Loss: 0.00009001\n",
      "=========epoch 760=========\n",
      "Iteraion 10, Train Loss: 0.00005507\n",
      "Iteraion 20, Train Loss: 0.00005122\n",
      "Iteraion 30, Train Loss: 0.00004641\n",
      "Epoch: 760, RMSE: 7.1680, MAE: 3.0177, MAPE: 32.4322,Train Loss: 0.00003121\n",
      "Epoch: 760, RMSE: 12.2239, MAE: 4.4475, MAPE: 33.2078,Test Loss: 0.00008995\n",
      "=========epoch 761=========\n",
      "Iteraion 10, Train Loss: 0.00004795\n",
      "Iteraion 20, Train Loss: 0.00004993\n",
      "Iteraion 30, Train Loss: 0.00005194\n",
      "Epoch: 761, RMSE: 7.1661, MAE: 3.0166, MAPE: 32.4660,Train Loss: 0.00003119\n",
      "Epoch: 761, RMSE: 12.2243, MAE: 4.4460, MAPE: 33.2236,Test Loss: 0.00008995\n",
      "=========epoch 762=========\n",
      "Iteraion 10, Train Loss: 0.00004503\n",
      "Iteraion 20, Train Loss: 0.00005265\n",
      "Iteraion 30, Train Loss: 0.00004784\n",
      "Epoch: 762, RMSE: 7.1804, MAE: 3.0189, MAPE: 32.4427,Train Loss: 0.00003132\n",
      "Epoch: 762, RMSE: 12.2386, MAE: 4.4492, MAPE: 33.2123,Test Loss: 0.00009016\n",
      "=========epoch 763=========\n",
      "Iteraion 10, Train Loss: 0.00005702\n",
      "Iteraion 20, Train Loss: 0.00005258\n",
      "Iteraion 30, Train Loss: 0.00005270\n",
      "Epoch: 763, RMSE: 7.1776, MAE: 3.0191, MAPE: 32.3790,Train Loss: 0.00003129\n",
      "Epoch: 763, RMSE: 12.2291, MAE: 4.4488, MAPE: 33.1647,Test Loss: 0.00009002\n",
      "=========epoch 764=========\n",
      "Iteraion 10, Train Loss: 0.00004985\n",
      "Iteraion 20, Train Loss: 0.00004778\n",
      "Iteraion 30, Train Loss: 0.00004986\n",
      "Epoch: 764, RMSE: 7.1555, MAE: 3.0167, MAPE: 32.5406,Train Loss: 0.00003110\n",
      "Epoch: 764, RMSE: 12.2144, MAE: 4.4445, MAPE: 33.2854,Test Loss: 0.00008981\n",
      "=========epoch 765=========\n",
      "Iteraion 10, Train Loss: 0.00005587\n",
      "Iteraion 20, Train Loss: 0.00005339\n",
      "Iteraion 30, Train Loss: 0.00005320\n",
      "Epoch: 765, RMSE: 7.1758, MAE: 3.0201, MAPE: 32.3504,Train Loss: 0.00003128\n",
      "Epoch: 765, RMSE: 12.2243, MAE: 4.4503, MAPE: 33.1489,Test Loss: 0.00008995\n",
      "=========epoch 766=========\n",
      "Iteraion 10, Train Loss: 0.00005022\n",
      "Iteraion 20, Train Loss: 0.00005617\n",
      "Iteraion 30, Train Loss: 0.00005441\n",
      "Epoch: 766, RMSE: 7.1650, MAE: 3.0161, MAPE: 32.4601,Train Loss: 0.00003118\n",
      "Epoch: 766, RMSE: 12.2193, MAE: 4.4451, MAPE: 33.2355,Test Loss: 0.00008988\n",
      "=========epoch 767=========\n",
      "Iteraion 10, Train Loss: 0.00005629\n",
      "Iteraion 20, Train Loss: 0.00004898\n",
      "Iteraion 30, Train Loss: 0.00005677\n",
      "Epoch: 767, RMSE: 7.1633, MAE: 3.0166, MAPE: 32.4363,Train Loss: 0.00003117\n",
      "Epoch: 767, RMSE: 12.2218, MAE: 4.4475, MAPE: 33.2200,Test Loss: 0.00008992\n",
      "=========epoch 768=========\n",
      "Iteraion 10, Train Loss: 0.00005227\n",
      "Iteraion 20, Train Loss: 0.00005385\n",
      "Iteraion 30, Train Loss: 0.00005359\n",
      "Epoch: 768, RMSE: 7.1673, MAE: 3.0173, MAPE: 32.3862,Train Loss: 0.00003120\n",
      "Epoch: 768, RMSE: 12.2354, MAE: 4.4518, MAPE: 33.1820,Test Loss: 0.00009012\n",
      "=========epoch 769=========\n",
      "Iteraion 10, Train Loss: 0.00004850\n",
      "Iteraion 20, Train Loss: 0.00004485\n",
      "Iteraion 30, Train Loss: 0.00005444\n",
      "Epoch: 769, RMSE: 7.1603, MAE: 3.0156, MAPE: 32.4681,Train Loss: 0.00003114\n",
      "Epoch: 769, RMSE: 12.2199, MAE: 4.4469, MAPE: 33.2464,Test Loss: 0.00008989\n",
      "=========epoch 770=========\n",
      "Iteraion 10, Train Loss: 0.00005447\n",
      "Iteraion 20, Train Loss: 0.00004862\n",
      "Iteraion 30, Train Loss: 0.00005392\n",
      "Epoch: 770, RMSE: 7.1549, MAE: 3.0153, MAPE: 32.4981,Train Loss: 0.00003110\n",
      "Epoch: 770, RMSE: 12.2198, MAE: 4.4472, MAPE: 33.2722,Test Loss: 0.00008989\n",
      "=========epoch 771=========\n",
      "Iteraion 10, Train Loss: 0.00004940\n",
      "Iteraion 20, Train Loss: 0.00005439\n",
      "Iteraion 30, Train Loss: 0.00005019\n",
      "Epoch: 771, RMSE: 7.1573, MAE: 3.0164, MAPE: 32.4484,Train Loss: 0.00003112\n",
      "Epoch: 771, RMSE: 12.2173, MAE: 4.4467, MAPE: 33.2252,Test Loss: 0.00008985\n",
      "=========epoch 772=========\n",
      "Iteraion 10, Train Loss: 0.00004905\n",
      "Iteraion 20, Train Loss: 0.00005450\n",
      "Iteraion 30, Train Loss: 0.00004681\n",
      "Epoch: 772, RMSE: 7.1547, MAE: 3.0164, MAPE: 32.4873,Train Loss: 0.00003109\n",
      "Epoch: 772, RMSE: 12.2239, MAE: 4.4476, MAPE: 33.2580,Test Loss: 0.00008995\n",
      "=========epoch 773=========\n",
      "Iteraion 10, Train Loss: 0.00005140\n",
      "Iteraion 20, Train Loss: 0.00005511\n",
      "Iteraion 30, Train Loss: 0.00004947\n",
      "Epoch: 773, RMSE: 7.1527, MAE: 3.0152, MAPE: 32.4930,Train Loss: 0.00003108\n",
      "Epoch: 773, RMSE: 12.2233, MAE: 4.4471, MAPE: 33.2669,Test Loss: 0.00008994\n",
      "=========epoch 774=========\n",
      "Iteraion 10, Train Loss: 0.00005094\n",
      "Iteraion 20, Train Loss: 0.00005132\n",
      "Iteraion 30, Train Loss: 0.00005351\n",
      "Epoch: 774, RMSE: 7.1671, MAE: 3.0162, MAPE: 32.4470,Train Loss: 0.00003120\n",
      "Epoch: 774, RMSE: 12.2326, MAE: 4.4485, MAPE: 33.2275,Test Loss: 0.00009008\n",
      "=========epoch 775=========\n",
      "Iteraion 10, Train Loss: 0.00004850\n",
      "Iteraion 20, Train Loss: 0.00004673\n",
      "Iteraion 30, Train Loss: 0.00005437\n",
      "Epoch: 775, RMSE: 7.1616, MAE: 3.0176, MAPE: 32.3977,Train Loss: 0.00003115\n",
      "Epoch: 775, RMSE: 12.2256, MAE: 4.4516, MAPE: 33.2101,Test Loss: 0.00008998\n",
      "=========epoch 776=========\n",
      "Iteraion 10, Train Loss: 0.00005145\n",
      "Iteraion 20, Train Loss: 0.00004471\n",
      "Iteraion 30, Train Loss: 0.00005474\n",
      "Epoch: 776, RMSE: 7.1535, MAE: 3.0166, MAPE: 32.5231,Train Loss: 0.00003108\n",
      "Epoch: 776, RMSE: 12.2185, MAE: 4.4484, MAPE: 33.2966,Test Loss: 0.00008987\n",
      "=========epoch 777=========\n",
      "Iteraion 10, Train Loss: 0.00005079\n",
      "Iteraion 20, Train Loss: 0.00004982\n",
      "Iteraion 30, Train Loss: 0.00004674\n",
      "Epoch: 777, RMSE: 7.1671, MAE: 3.0171, MAPE: 32.3851,Train Loss: 0.00003120\n",
      "Epoch: 777, RMSE: 12.2336, MAE: 4.4507, MAPE: 33.1819,Test Loss: 0.00009009\n",
      "=========epoch 778=========\n",
      "Iteraion 10, Train Loss: 0.00004572\n",
      "Iteraion 20, Train Loss: 0.00005478\n",
      "Iteraion 30, Train Loss: 0.00005710\n",
      "Epoch: 778, RMSE: 7.1615, MAE: 3.0153, MAPE: 32.4161,Train Loss: 0.00003115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 778, RMSE: 12.2337, MAE: 4.4496, MAPE: 33.2141,Test Loss: 0.00009009\n",
      "=========epoch 779=========\n",
      "Iteraion 10, Train Loss: 0.00004768\n",
      "Iteraion 20, Train Loss: 0.00005341\n",
      "Iteraion 30, Train Loss: 0.00005247\n",
      "Epoch: 779, RMSE: 7.1557, MAE: 3.0165, MAPE: 32.4733,Train Loss: 0.00003110\n",
      "Epoch: 779, RMSE: 12.2208, MAE: 4.4478, MAPE: 33.2682,Test Loss: 0.00008990\n",
      "=========epoch 780=========\n",
      "Iteraion 10, Train Loss: 0.00005049\n",
      "Iteraion 20, Train Loss: 0.00005227\n",
      "Iteraion 30, Train Loss: 0.00004940\n",
      "Epoch: 780, RMSE: 7.1617, MAE: 3.0154, MAPE: 32.4137,Train Loss: 0.00003115\n",
      "Epoch: 780, RMSE: 12.2303, MAE: 4.4483, MAPE: 33.2257,Test Loss: 0.00009005\n",
      "=========epoch 781=========\n",
      "Iteraion 10, Train Loss: 0.00005102\n",
      "Iteraion 20, Train Loss: 0.00005426\n",
      "Iteraion 30, Train Loss: 0.00004902\n",
      "Epoch: 781, RMSE: 7.1718, MAE: 3.0164, MAPE: 32.4189,Train Loss: 0.00003124\n",
      "Epoch: 781, RMSE: 12.2451, MAE: 4.4504, MAPE: 33.2294,Test Loss: 0.00009026\n",
      "=========epoch 782=========\n",
      "Iteraion 10, Train Loss: 0.00004770\n",
      "Iteraion 20, Train Loss: 0.00005242\n",
      "Iteraion 30, Train Loss: 0.00005620\n",
      "Epoch: 782, RMSE: 7.1573, MAE: 3.0145, MAPE: 32.4403,Train Loss: 0.00003112\n",
      "Epoch: 782, RMSE: 12.2293, MAE: 4.4471, MAPE: 33.2404,Test Loss: 0.00009003\n",
      "=========epoch 783=========\n",
      "Iteraion 10, Train Loss: 0.00005574\n",
      "Iteraion 20, Train Loss: 0.00004849\n",
      "Iteraion 30, Train Loss: 0.00005460\n",
      "Epoch: 783, RMSE: 7.1623, MAE: 3.0166, MAPE: 32.3750,Train Loss: 0.00003116\n",
      "Epoch: 783, RMSE: 12.2389, MAE: 4.4510, MAPE: 33.1853,Test Loss: 0.00009017\n",
      "=========epoch 784=========\n",
      "Iteraion 10, Train Loss: 0.00005188\n",
      "Iteraion 20, Train Loss: 0.00005428\n",
      "Iteraion 30, Train Loss: 0.00005968\n",
      "Epoch: 784, RMSE: 7.1698, MAE: 3.0174, MAPE: 32.3749,Train Loss: 0.00003123\n",
      "Epoch: 784, RMSE: 12.2366, MAE: 4.4506, MAPE: 33.1783,Test Loss: 0.00009014\n",
      "=========epoch 785=========\n",
      "Iteraion 10, Train Loss: 0.00005081\n",
      "Iteraion 20, Train Loss: 0.00005085\n",
      "Iteraion 30, Train Loss: 0.00005238\n",
      "Epoch: 785, RMSE: 7.1690, MAE: 3.0166, MAPE: 32.3904,Train Loss: 0.00003122\n",
      "Epoch: 785, RMSE: 12.2384, MAE: 4.4501, MAPE: 33.1966,Test Loss: 0.00009017\n",
      "=========epoch 786=========\n",
      "Iteraion 10, Train Loss: 0.00005345\n",
      "Iteraion 20, Train Loss: 0.00004841\n",
      "Iteraion 30, Train Loss: 0.00004929\n",
      "Epoch: 786, RMSE: 7.1639, MAE: 3.0156, MAPE: 32.4220,Train Loss: 0.00003117\n",
      "Epoch: 786, RMSE: 12.2342, MAE: 4.4488, MAPE: 33.2123,Test Loss: 0.00009010\n",
      "=========epoch 787=========\n",
      "Iteraion 10, Train Loss: 0.00005161\n",
      "Iteraion 20, Train Loss: 0.00004813\n",
      "Iteraion 30, Train Loss: 0.00004817\n",
      "Epoch: 787, RMSE: 7.1510, MAE: 3.0146, MAPE: 32.5152,Train Loss: 0.00003106\n",
      "Epoch: 787, RMSE: 12.2215, MAE: 4.4450, MAPE: 33.2786,Test Loss: 0.00008992\n",
      "=========epoch 788=========\n",
      "Iteraion 10, Train Loss: 0.00004793\n",
      "Iteraion 20, Train Loss: 0.00004698\n",
      "Iteraion 30, Train Loss: 0.00005401\n",
      "Epoch: 788, RMSE: 7.1652, MAE: 3.0162, MAPE: 32.3978,Train Loss: 0.00003119\n",
      "Epoch: 788, RMSE: 12.2375, MAE: 4.4499, MAPE: 33.1906,Test Loss: 0.00009015\n",
      "=========epoch 789=========\n",
      "Iteraion 10, Train Loss: 0.00005256\n",
      "Iteraion 20, Train Loss: 0.00005453\n",
      "Iteraion 30, Train Loss: 0.00005035\n",
      "Epoch: 789, RMSE: 7.1521, MAE: 3.0138, MAPE: 32.4647,Train Loss: 0.00003107\n",
      "Epoch: 789, RMSE: 12.2229, MAE: 4.4446, MAPE: 33.2378,Test Loss: 0.00008994\n",
      "=========epoch 790=========\n",
      "Iteraion 10, Train Loss: 0.00005099\n",
      "Iteraion 20, Train Loss: 0.00005119\n",
      "Iteraion 30, Train Loss: 0.00005592\n",
      "Epoch: 790, RMSE: 7.1480, MAE: 3.0147, MAPE: 32.5583,Train Loss: 0.00003104\n",
      "Epoch: 790, RMSE: 12.2094, MAE: 4.4439, MAPE: 33.3218,Test Loss: 0.00008974\n",
      "=========epoch 791=========\n",
      "Iteraion 10, Train Loss: 0.00005328\n",
      "Iteraion 20, Train Loss: 0.00005397\n",
      "Iteraion 30, Train Loss: 0.00004913\n",
      "Epoch: 791, RMSE: 7.1504, MAE: 3.0146, MAPE: 32.5094,Train Loss: 0.00003106\n",
      "Epoch: 791, RMSE: 12.2107, MAE: 4.4449, MAPE: 33.2779,Test Loss: 0.00008976\n",
      "=========epoch 792=========\n",
      "Iteraion 10, Train Loss: 0.00005206\n",
      "Iteraion 20, Train Loss: 0.00005120\n",
      "Iteraion 30, Train Loss: 0.00004526\n",
      "Epoch: 792, RMSE: 7.1582, MAE: 3.0149, MAPE: 32.4225,Train Loss: 0.00003112\n",
      "Epoch: 792, RMSE: 12.2261, MAE: 4.4482, MAPE: 33.2178,Test Loss: 0.00008998\n",
      "=========epoch 793=========\n",
      "Iteraion 10, Train Loss: 0.00005187\n",
      "Iteraion 20, Train Loss: 0.00004622\n",
      "Iteraion 30, Train Loss: 0.00005333\n",
      "Epoch: 793, RMSE: 7.1685, MAE: 3.0183, MAPE: 32.3336,Train Loss: 0.00003121\n",
      "Epoch: 793, RMSE: 12.2504, MAE: 4.4559, MAPE: 33.1608,Test Loss: 0.00009034\n",
      "=========epoch 794=========\n",
      "Iteraion 10, Train Loss: 0.00004857\n",
      "Iteraion 20, Train Loss: 0.00004991\n",
      "Iteraion 30, Train Loss: 0.00004953\n",
      "Epoch: 794, RMSE: 7.1867, MAE: 3.0231, MAPE: 32.2785,Train Loss: 0.00003137\n",
      "Epoch: 794, RMSE: 12.2580, MAE: 4.4593, MAPE: 33.1193,Test Loss: 0.00009046\n",
      "=========epoch 795=========\n",
      "Iteraion 10, Train Loss: 0.00005155\n",
      "Iteraion 20, Train Loss: 0.00004831\n",
      "Iteraion 30, Train Loss: 0.00005767\n",
      "Epoch: 795, RMSE: 7.1577, MAE: 3.0153, MAPE: 32.4346,Train Loss: 0.00003112\n",
      "Epoch: 795, RMSE: 12.2360, MAE: 4.4491, MAPE: 33.2214,Test Loss: 0.00009013\n",
      "=========epoch 796=========\n",
      "Iteraion 10, Train Loss: 0.00005091\n",
      "Iteraion 20, Train Loss: 0.00004773\n",
      "Iteraion 30, Train Loss: 0.00005140\n",
      "Epoch: 796, RMSE: 7.1513, MAE: 3.0141, MAPE: 32.5102,Train Loss: 0.00003106\n",
      "Epoch: 796, RMSE: 12.2339, MAE: 4.4484, MAPE: 33.2890,Test Loss: 0.00009010\n",
      "=========epoch 797=========\n",
      "Iteraion 10, Train Loss: 0.00005279\n",
      "Iteraion 20, Train Loss: 0.00005267\n",
      "Iteraion 30, Train Loss: 0.00005327\n",
      "Epoch: 797, RMSE: 7.1573, MAE: 3.0139, MAPE: 32.4490,Train Loss: 0.00003112\n",
      "Epoch: 797, RMSE: 12.2358, MAE: 4.4486, MAPE: 33.2387,Test Loss: 0.00009013\n",
      "=========epoch 798=========\n",
      "Iteraion 10, Train Loss: 0.00005645\n",
      "Iteraion 20, Train Loss: 0.00005341\n",
      "Iteraion 30, Train Loss: 0.00005297\n",
      "Epoch: 798, RMSE: 7.1689, MAE: 3.0168, MAPE: 32.3889,Train Loss: 0.00003122\n",
      "Epoch: 798, RMSE: 12.2368, MAE: 4.4510, MAPE: 33.1983,Test Loss: 0.00009014\n",
      "=========epoch 799=========\n",
      "Iteraion 10, Train Loss: 0.00005542\n",
      "Iteraion 20, Train Loss: 0.00004908\n",
      "Iteraion 30, Train Loss: 0.00005631\n",
      "Epoch: 799, RMSE: 7.1484, MAE: 3.0137, MAPE: 32.5144,Train Loss: 0.00003104\n",
      "Epoch: 799, RMSE: 12.2241, MAE: 4.4466, MAPE: 33.2783,Test Loss: 0.00008995\n",
      "=========epoch 800=========\n",
      "Iteraion 10, Train Loss: 0.00005117\n",
      "Iteraion 20, Train Loss: 0.00005152\n",
      "Iteraion 30, Train Loss: 0.00004980\n",
      "Epoch: 800, RMSE: 7.1657, MAE: 3.0149, MAPE: 32.4325,Train Loss: 0.00003119\n",
      "Epoch: 800, RMSE: 12.2469, MAE: 4.4508, MAPE: 33.2351,Test Loss: 0.00009029\n",
      "=========epoch 801=========\n",
      "Iteraion 10, Train Loss: 0.00004895\n",
      "Iteraion 20, Train Loss: 0.00005698\n",
      "Iteraion 30, Train Loss: 0.00005354\n",
      "Epoch: 801, RMSE: 7.1606, MAE: 3.0146, MAPE: 32.4273,Train Loss: 0.00003115\n",
      "Epoch: 801, RMSE: 12.2466, MAE: 4.4523, MAPE: 33.2245,Test Loss: 0.00009029\n",
      "=========epoch 802=========\n",
      "Iteraion 10, Train Loss: 0.00005045\n",
      "Iteraion 20, Train Loss: 0.00005320\n",
      "Iteraion 30, Train Loss: 0.00004725\n",
      "Epoch: 802, RMSE: 7.1516, MAE: 3.0136, MAPE: 32.4606,Train Loss: 0.00003107\n",
      "Epoch: 802, RMSE: 12.2290, MAE: 4.4489, MAPE: 33.2483,Test Loss: 0.00009003\n",
      "=========epoch 803=========\n",
      "Iteraion 10, Train Loss: 0.00005371\n",
      "Iteraion 20, Train Loss: 0.00004847\n",
      "Iteraion 30, Train Loss: 0.00005225\n",
      "Epoch: 803, RMSE: 7.1545, MAE: 3.0151, MAPE: 32.4121,Train Loss: 0.00003109\n",
      "Epoch: 803, RMSE: 12.2342, MAE: 4.4520, MAPE: 33.2163,Test Loss: 0.00009010\n",
      "=========epoch 804=========\n",
      "Iteraion 10, Train Loss: 0.00005739\n",
      "Iteraion 20, Train Loss: 0.00004803\n",
      "Iteraion 30, Train Loss: 0.00005253\n",
      "Epoch: 804, RMSE: 7.1526, MAE: 3.0133, MAPE: 32.4585,Train Loss: 0.00003108\n",
      "Epoch: 804, RMSE: 12.2353, MAE: 4.4502, MAPE: 33.2523,Test Loss: 0.00009012\n",
      "=========epoch 805=========\n",
      "Iteraion 10, Train Loss: 0.00005170\n",
      "Iteraion 20, Train Loss: 0.00005333\n",
      "Iteraion 30, Train Loss: 0.00005113\n",
      "Epoch: 805, RMSE: 7.1619, MAE: 3.0147, MAPE: 32.4087,Train Loss: 0.00003116\n",
      "Epoch: 805, RMSE: 12.2362, MAE: 4.4509, MAPE: 33.2050,Test Loss: 0.00009013\n",
      "=========epoch 806=========\n",
      "Iteraion 10, Train Loss: 0.00005060\n",
      "Iteraion 20, Train Loss: 0.00005325\n",
      "Iteraion 30, Train Loss: 0.00005265\n",
      "Epoch: 806, RMSE: 7.1540, MAE: 3.0123, MAPE: 32.4788,Train Loss: 0.00003109\n",
      "Epoch: 806, RMSE: 12.2297, MAE: 4.4474, MAPE: 33.2636,Test Loss: 0.00009004\n",
      "=========epoch 807=========\n",
      "Iteraion 10, Train Loss: 0.00004678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraion 20, Train Loss: 0.00005671\n",
      "Iteraion 30, Train Loss: 0.00005228\n",
      "Epoch: 807, RMSE: 7.1539, MAE: 3.0133, MAPE: 32.4622,Train Loss: 0.00003109\n",
      "Epoch: 807, RMSE: 12.2252, MAE: 4.4476, MAPE: 33.2377,Test Loss: 0.00008997\n",
      "=========epoch 808=========\n",
      "Iteraion 10, Train Loss: 0.00005275\n",
      "Iteraion 20, Train Loss: 0.00004919\n",
      "Iteraion 30, Train Loss: 0.00005240\n",
      "Epoch: 808, RMSE: 7.1499, MAE: 3.0129, MAPE: 32.5077,Train Loss: 0.00003105\n",
      "Epoch: 808, RMSE: 12.2286, MAE: 4.4474, MAPE: 33.2766,Test Loss: 0.00009002\n",
      "=========epoch 809=========\n",
      "Iteraion 10, Train Loss: 0.00005201\n",
      "Iteraion 20, Train Loss: 0.00005167\n",
      "Iteraion 30, Train Loss: 0.00005990\n",
      "Epoch: 809, RMSE: 7.1511, MAE: 3.0127, MAPE: 32.4929,Train Loss: 0.00003106\n",
      "Epoch: 809, RMSE: 12.2241, MAE: 4.4465, MAPE: 33.2613,Test Loss: 0.00008995\n",
      "=========epoch 810=========\n",
      "Iteraion 10, Train Loss: 0.00004899\n",
      "Iteraion 20, Train Loss: 0.00005247\n",
      "Iteraion 30, Train Loss: 0.00005386\n",
      "Epoch: 810, RMSE: 7.1530, MAE: 3.0136, MAPE: 32.4487,Train Loss: 0.00003108\n",
      "Epoch: 810, RMSE: 12.2222, MAE: 4.4476, MAPE: 33.2327,Test Loss: 0.00008992\n",
      "=========epoch 811=========\n",
      "Iteraion 10, Train Loss: 0.00005122\n",
      "Iteraion 20, Train Loss: 0.00004980\n",
      "Iteraion 30, Train Loss: 0.00005668\n",
      "Epoch: 811, RMSE: 7.1447, MAE: 3.0124, MAPE: 32.5205,Train Loss: 0.00003101\n",
      "Epoch: 811, RMSE: 12.2177, MAE: 4.4461, MAPE: 33.2851,Test Loss: 0.00008986\n",
      "=========epoch 812=========\n",
      "Iteraion 10, Train Loss: 0.00005027\n",
      "Iteraion 20, Train Loss: 0.00005168\n",
      "Iteraion 30, Train Loss: 0.00005037\n",
      "Epoch: 812, RMSE: 7.1528, MAE: 3.0134, MAPE: 32.4486,Train Loss: 0.00003108\n",
      "Epoch: 812, RMSE: 12.2336, MAE: 4.4491, MAPE: 33.2420,Test Loss: 0.00009009\n",
      "=========epoch 813=========\n",
      "Iteraion 10, Train Loss: 0.00004875\n",
      "Iteraion 20, Train Loss: 0.00005481\n",
      "Iteraion 30, Train Loss: 0.00005407\n",
      "Epoch: 813, RMSE: 7.1485, MAE: 3.0137, MAPE: 32.4951,Train Loss: 0.00003104\n",
      "Epoch: 813, RMSE: 12.2306, MAE: 4.4489, MAPE: 33.2772,Test Loss: 0.00009005\n",
      "=========epoch 814=========\n",
      "Iteraion 10, Train Loss: 0.00005010\n",
      "Iteraion 20, Train Loss: 0.00005169\n",
      "Iteraion 30, Train Loss: 0.00005006\n",
      "Epoch: 814, RMSE: 7.1628, MAE: 3.0153, MAPE: 32.3711,Train Loss: 0.00003116\n",
      "Epoch: 814, RMSE: 12.2428, MAE: 4.4533, MAPE: 33.1876,Test Loss: 0.00009022\n",
      "=========epoch 815=========\n",
      "Iteraion 10, Train Loss: 0.00005028\n",
      "Iteraion 20, Train Loss: 0.00004867\n",
      "Iteraion 30, Train Loss: 0.00005402\n",
      "Epoch: 815, RMSE: 7.1414, MAE: 3.0114, MAPE: 32.5938,Train Loss: 0.00003098\n",
      "Epoch: 815, RMSE: 12.2231, MAE: 4.4464, MAPE: 33.3521,Test Loss: 0.00008994\n",
      "=========epoch 816=========\n",
      "Iteraion 10, Train Loss: 0.00005436\n",
      "Iteraion 20, Train Loss: 0.00005838\n",
      "Iteraion 30, Train Loss: 0.00004642\n",
      "Epoch: 816, RMSE: 7.1524, MAE: 3.0138, MAPE: 32.4370,Train Loss: 0.00003107\n",
      "Epoch: 816, RMSE: 12.2301, MAE: 4.4503, MAPE: 33.2251,Test Loss: 0.00009004\n",
      "=========epoch 817=========\n",
      "Iteraion 10, Train Loss: 0.00005828\n",
      "Iteraion 20, Train Loss: 0.00004710\n",
      "Iteraion 30, Train Loss: 0.00005584\n",
      "Epoch: 817, RMSE: 7.1613, MAE: 3.0139, MAPE: 32.4595,Train Loss: 0.00003115\n",
      "Epoch: 817, RMSE: 12.2387, MAE: 4.4493, MAPE: 33.2381,Test Loss: 0.00009017\n",
      "=========epoch 818=========\n",
      "Iteraion 10, Train Loss: 0.00005185\n",
      "Iteraion 20, Train Loss: 0.00005257\n",
      "Iteraion 30, Train Loss: 0.00005739\n",
      "Epoch: 818, RMSE: 7.1587, MAE: 3.0147, MAPE: 32.4061,Train Loss: 0.00003113\n",
      "Epoch: 818, RMSE: 12.2349, MAE: 4.4510, MAPE: 33.2084,Test Loss: 0.00009011\n",
      "=========epoch 819=========\n",
      "Iteraion 10, Train Loss: 0.00004583\n",
      "Iteraion 20, Train Loss: 0.00004861\n",
      "Iteraion 30, Train Loss: 0.00005127\n",
      "Epoch: 819, RMSE: 7.1484, MAE: 3.0139, MAPE: 32.4805,Train Loss: 0.00003104\n",
      "Epoch: 819, RMSE: 12.2280, MAE: 4.4498, MAPE: 33.2606,Test Loss: 0.00009001\n",
      "=========epoch 820=========\n",
      "Iteraion 10, Train Loss: 0.00004312\n",
      "Iteraion 20, Train Loss: 0.00004932\n",
      "Iteraion 30, Train Loss: 0.00005738\n",
      "Epoch: 820, RMSE: 7.1472, MAE: 3.0114, MAPE: 32.4670,Train Loss: 0.00003103\n",
      "Epoch: 820, RMSE: 12.2386, MAE: 4.4512, MAPE: 33.2547,Test Loss: 0.00009016\n",
      "=========epoch 821=========\n",
      "Iteraion 10, Train Loss: 0.00005043\n",
      "Iteraion 20, Train Loss: 0.00005195\n",
      "Iteraion 30, Train Loss: 0.00005388\n",
      "Epoch: 821, RMSE: 7.1484, MAE: 3.0110, MAPE: 32.5005,Train Loss: 0.00003104\n",
      "Epoch: 821, RMSE: 12.2324, MAE: 4.4468, MAPE: 33.2709,Test Loss: 0.00009007\n",
      "=========epoch 822=========\n",
      "Iteraion 10, Train Loss: 0.00005111\n",
      "Iteraion 20, Train Loss: 0.00004906\n",
      "Iteraion 30, Train Loss: 0.00005093\n",
      "Epoch: 822, RMSE: 7.1509, MAE: 3.0127, MAPE: 32.4419,Train Loss: 0.00003106\n",
      "Epoch: 822, RMSE: 12.2255, MAE: 4.4461, MAPE: 33.2262,Test Loss: 0.00008997\n",
      "=========epoch 823=========\n",
      "Iteraion 10, Train Loss: 0.00005335\n",
      "Iteraion 20, Train Loss: 0.00005622\n",
      "Iteraion 30, Train Loss: 0.00004984\n",
      "Epoch: 823, RMSE: 7.1563, MAE: 3.0126, MAPE: 32.4579,Train Loss: 0.00003111\n",
      "Epoch: 823, RMSE: 12.2274, MAE: 4.4455, MAPE: 33.2404,Test Loss: 0.00009000\n",
      "=========epoch 824=========\n",
      "Iteraion 10, Train Loss: 0.00005576\n",
      "Iteraion 20, Train Loss: 0.00004935\n",
      "Iteraion 30, Train Loss: 0.00004735\n",
      "Epoch: 824, RMSE: 7.1429, MAE: 3.0115, MAPE: 32.5059,Train Loss: 0.00003099\n",
      "Epoch: 824, RMSE: 12.2274, MAE: 4.4465, MAPE: 33.2815,Test Loss: 0.00009000\n",
      "=========epoch 825=========\n",
      "Iteraion 10, Train Loss: 0.00006267\n",
      "Iteraion 20, Train Loss: 0.00005707\n",
      "Iteraion 30, Train Loss: 0.00005361\n",
      "Epoch: 825, RMSE: 7.1510, MAE: 3.0129, MAPE: 32.4564,Train Loss: 0.00003106\n",
      "Epoch: 825, RMSE: 12.2351, MAE: 4.4494, MAPE: 33.2369,Test Loss: 0.00009012\n",
      "=========epoch 826=========\n",
      "Iteraion 10, Train Loss: 0.00004781\n",
      "Iteraion 20, Train Loss: 0.00004627\n",
      "Iteraion 30, Train Loss: 0.00005290\n",
      "Epoch: 826, RMSE: 7.1535, MAE: 3.0132, MAPE: 32.4916,Train Loss: 0.00003108\n",
      "Epoch: 826, RMSE: 12.2366, MAE: 4.4499, MAPE: 33.2789,Test Loss: 0.00009014\n",
      "=========epoch 827=========\n",
      "Iteraion 10, Train Loss: 0.00004745\n",
      "Iteraion 20, Train Loss: 0.00005172\n",
      "Iteraion 30, Train Loss: 0.00005140\n",
      "Epoch: 827, RMSE: 7.1403, MAE: 3.0105, MAPE: 32.5298,Train Loss: 0.00003097\n",
      "Epoch: 827, RMSE: 12.2247, MAE: 4.4463, MAPE: 33.3077,Test Loss: 0.00008996\n",
      "=========epoch 828=========\n",
      "Iteraion 10, Train Loss: 0.00004990\n",
      "Iteraion 20, Train Loss: 0.00005143\n",
      "Iteraion 30, Train Loss: 0.00004790\n",
      "Epoch: 828, RMSE: 7.1509, MAE: 3.0108, MAPE: 32.4758,Train Loss: 0.00003106\n",
      "Epoch: 828, RMSE: 12.2331, MAE: 4.4474, MAPE: 33.2717,Test Loss: 0.00009008\n",
      "=========epoch 829=========\n",
      "Iteraion 10, Train Loss: 0.00005059\n",
      "Iteraion 20, Train Loss: 0.00004307\n",
      "Iteraion 30, Train Loss: 0.00004471\n",
      "Epoch: 829, RMSE: 7.1395, MAE: 3.0107, MAPE: 32.5794,Train Loss: 0.00003096\n",
      "Epoch: 829, RMSE: 12.2147, MAE: 4.4429, MAPE: 33.3355,Test Loss: 0.00008981\n",
      "=========epoch 830=========\n",
      "Iteraion 10, Train Loss: 0.00005754\n",
      "Iteraion 20, Train Loss: 0.00005453\n",
      "Iteraion 30, Train Loss: 0.00005183\n",
      "Epoch: 830, RMSE: 7.1509, MAE: 3.0119, MAPE: 32.4471,Train Loss: 0.00003106\n",
      "Epoch: 830, RMSE: 12.2261, MAE: 4.4471, MAPE: 33.2475,Test Loss: 0.00008998\n",
      "=========epoch 831=========\n",
      "Iteraion 10, Train Loss: 0.00005545\n",
      "Iteraion 20, Train Loss: 0.00004901\n",
      "Iteraion 30, Train Loss: 0.00004789\n",
      "Epoch: 831, RMSE: 7.1472, MAE: 3.0109, MAPE: 32.5024,Train Loss: 0.00003103\n",
      "Epoch: 831, RMSE: 12.2310, MAE: 4.4478, MAPE: 33.2934,Test Loss: 0.00009005\n",
      "=========epoch 832=========\n",
      "Iteraion 10, Train Loss: 0.00005297\n",
      "Iteraion 20, Train Loss: 0.00004970\n",
      "Iteraion 30, Train Loss: 0.00005628\n",
      "Epoch: 832, RMSE: 7.1400, MAE: 3.0109, MAPE: 32.5533,Train Loss: 0.00003097\n",
      "Epoch: 832, RMSE: 12.2233, MAE: 4.4465, MAPE: 33.3336,Test Loss: 0.00008994\n",
      "=========epoch 833=========\n",
      "Iteraion 10, Train Loss: 0.00004896\n",
      "Iteraion 20, Train Loss: 0.00005868\n",
      "Iteraion 30, Train Loss: 0.00004585\n",
      "Epoch: 833, RMSE: 7.1528, MAE: 3.0121, MAPE: 32.4812,Train Loss: 0.00003108\n",
      "Epoch: 833, RMSE: 12.2224, MAE: 4.4458, MAPE: 33.2748,Test Loss: 0.00008993\n",
      "=========epoch 834=========\n",
      "Iteraion 10, Train Loss: 0.00005027\n",
      "Iteraion 20, Train Loss: 0.00004867\n",
      "Iteraion 30, Train Loss: 0.00005077\n",
      "Epoch: 834, RMSE: 7.1458, MAE: 3.0117, MAPE: 32.4972,Train Loss: 0.00003102\n",
      "Epoch: 834, RMSE: 12.2179, MAE: 4.4455, MAPE: 33.2833,Test Loss: 0.00008986\n",
      "=========epoch 835=========\n",
      "Iteraion 10, Train Loss: 0.00004989\n",
      "Iteraion 20, Train Loss: 0.00005104\n",
      "Iteraion 30, Train Loss: 0.00005338\n",
      "Epoch: 835, RMSE: 7.1529, MAE: 3.0123, MAPE: 32.4619,Train Loss: 0.00003108\n",
      "Epoch: 835, RMSE: 12.2352, MAE: 4.4491, MAPE: 33.2618,Test Loss: 0.00009011\n",
      "=========epoch 836=========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraion 10, Train Loss: 0.00005605\n",
      "Iteraion 20, Train Loss: 0.00005066\n",
      "Iteraion 30, Train Loss: 0.00005791\n",
      "Epoch: 836, RMSE: 7.1421, MAE: 3.0106, MAPE: 32.5464,Train Loss: 0.00003098\n",
      "Epoch: 836, RMSE: 12.2214, MAE: 4.4463, MAPE: 33.3248,Test Loss: 0.00008991\n",
      "=========epoch 837=========\n",
      "Iteraion 10, Train Loss: 0.00004865\n",
      "Iteraion 20, Train Loss: 0.00004954\n",
      "Iteraion 30, Train Loss: 0.00005044\n",
      "Epoch: 837, RMSE: 7.1566, MAE: 3.0119, MAPE: 32.5165,Train Loss: 0.00003111\n",
      "Epoch: 837, RMSE: 12.2308, MAE: 4.4468, MAPE: 33.3053,Test Loss: 0.00009005\n",
      "=========epoch 838=========\n",
      "Iteraion 10, Train Loss: 0.00006077\n",
      "Iteraion 20, Train Loss: 0.00005065\n",
      "Iteraion 30, Train Loss: 0.00005237\n",
      "Epoch: 838, RMSE: 7.1361, MAE: 3.0106, MAPE: 32.6688,Train Loss: 0.00003093\n",
      "Epoch: 838, RMSE: 12.2134, MAE: 4.4433, MAPE: 33.4213,Test Loss: 0.00008980\n",
      "=========epoch 839=========\n",
      "Iteraion 10, Train Loss: 0.00005412\n",
      "Iteraion 20, Train Loss: 0.00005727\n",
      "Iteraion 30, Train Loss: 0.00004992\n",
      "Epoch: 839, RMSE: 7.1467, MAE: 3.0108, MAPE: 32.5260,Train Loss: 0.00003102\n",
      "Epoch: 839, RMSE: 12.2241, MAE: 4.4458, MAPE: 33.3093,Test Loss: 0.00008995\n",
      "=========epoch 840=========\n",
      "Iteraion 10, Train Loss: 0.00004919\n",
      "Iteraion 20, Train Loss: 0.00004724\n",
      "Iteraion 30, Train Loss: 0.00004779\n",
      "Epoch: 840, RMSE: 7.1486, MAE: 3.0112, MAPE: 32.4883,Train Loss: 0.00003104\n",
      "Epoch: 840, RMSE: 12.2290, MAE: 4.4476, MAPE: 33.2893,Test Loss: 0.00009003\n",
      "=========epoch 841=========\n",
      "Iteraion 10, Train Loss: 0.00005136\n",
      "Iteraion 20, Train Loss: 0.00005679\n",
      "Iteraion 30, Train Loss: 0.00004874\n",
      "Epoch: 841, RMSE: 7.1487, MAE: 3.0124, MAPE: 32.4551,Train Loss: 0.00003104\n",
      "Epoch: 841, RMSE: 12.2192, MAE: 4.4473, MAPE: 33.2687,Test Loss: 0.00008988\n",
      "=========epoch 842=========\n",
      "Iteraion 10, Train Loss: 0.00005226\n",
      "Iteraion 20, Train Loss: 0.00005013\n",
      "Iteraion 30, Train Loss: 0.00004733\n",
      "Epoch: 842, RMSE: 7.1383, MAE: 3.0094, MAPE: 32.5643,Train Loss: 0.00003095\n",
      "Epoch: 842, RMSE: 12.2208, MAE: 4.4450, MAPE: 33.3455,Test Loss: 0.00008990\n",
      "=========epoch 843=========\n",
      "Iteraion 10, Train Loss: 0.00005129\n",
      "Iteraion 20, Train Loss: 0.00005206\n",
      "Iteraion 30, Train Loss: 0.00005852\n",
      "Epoch: 843, RMSE: 7.1384, MAE: 3.0122, MAPE: 32.6380,Train Loss: 0.00003095\n",
      "Epoch: 843, RMSE: 12.2304, MAE: 4.4489, MAPE: 33.4155,Test Loss: 0.00009004\n",
      "=========epoch 844=========\n",
      "Iteraion 10, Train Loss: 0.00005081\n",
      "Iteraion 20, Train Loss: 0.00004174\n",
      "Iteraion 30, Train Loss: 0.00004509\n",
      "Epoch: 844, RMSE: 7.1405, MAE: 3.0102, MAPE: 32.5408,Train Loss: 0.00003097\n",
      "Epoch: 844, RMSE: 12.2278, MAE: 4.4483, MAPE: 33.3400,Test Loss: 0.00009001\n",
      "=========epoch 845=========\n",
      "Iteraion 10, Train Loss: 0.00005436\n",
      "Iteraion 20, Train Loss: 0.00005270\n",
      "Iteraion 30, Train Loss: 0.00004903\n",
      "Epoch: 845, RMSE: 7.1554, MAE: 3.0131, MAPE: 32.4137,Train Loss: 0.00003110\n",
      "Epoch: 845, RMSE: 12.2326, MAE: 4.4506, MAPE: 33.2370,Test Loss: 0.00009008\n",
      "=========epoch 846=========\n",
      "Iteraion 10, Train Loss: 0.00004650\n",
      "Iteraion 20, Train Loss: 0.00005350\n",
      "Iteraion 30, Train Loss: 0.00005372\n",
      "Epoch: 846, RMSE: 7.1473, MAE: 3.0118, MAPE: 32.4441,Train Loss: 0.00003103\n",
      "Epoch: 846, RMSE: 12.2291, MAE: 4.4494, MAPE: 33.2520,Test Loss: 0.00009002\n",
      "=========epoch 847=========\n",
      "Iteraion 10, Train Loss: 0.00005342\n",
      "Iteraion 20, Train Loss: 0.00004821\n",
      "Iteraion 30, Train Loss: 0.00005398\n",
      "Epoch: 847, RMSE: 7.1419, MAE: 3.0094, MAPE: 32.5913,Train Loss: 0.00003098\n",
      "Epoch: 847, RMSE: 12.2354, MAE: 4.4465, MAPE: 33.3577,Test Loss: 0.00009012\n",
      "=========epoch 848=========\n",
      "Iteraion 10, Train Loss: 0.00004907\n",
      "Iteraion 20, Train Loss: 0.00004914\n",
      "Iteraion 30, Train Loss: 0.00004930\n",
      "Epoch: 848, RMSE: 7.1628, MAE: 3.0145, MAPE: 32.3814,Train Loss: 0.00003116\n",
      "Epoch: 848, RMSE: 12.2432, MAE: 4.4511, MAPE: 33.1998,Test Loss: 0.00009023\n",
      "=========epoch 849=========\n",
      "Iteraion 10, Train Loss: 0.00005307\n",
      "Iteraion 20, Train Loss: 0.00005514\n",
      "Iteraion 30, Train Loss: 0.00004843\n",
      "Epoch: 849, RMSE: 7.1502, MAE: 3.0117, MAPE: 32.4617,Train Loss: 0.00003105\n",
      "Epoch: 849, RMSE: 12.2296, MAE: 4.4476, MAPE: 33.2605,Test Loss: 0.00009004\n",
      "=========epoch 850=========\n",
      "Iteraion 10, Train Loss: 0.00005218\n",
      "Iteraion 20, Train Loss: 0.00005252\n",
      "Iteraion 30, Train Loss: 0.00005075\n",
      "Epoch: 850, RMSE: 7.1469, MAE: 3.0113, MAPE: 32.4905,Train Loss: 0.00003103\n",
      "Epoch: 850, RMSE: 12.2219, MAE: 4.4456, MAPE: 33.2841,Test Loss: 0.00008992\n",
      "=========epoch 851=========\n",
      "Iteraion 10, Train Loss: 0.00005143\n",
      "Iteraion 20, Train Loss: 0.00005396\n",
      "Iteraion 30, Train Loss: 0.00005171\n",
      "Epoch: 851, RMSE: 7.1526, MAE: 3.0110, MAPE: 32.4713,Train Loss: 0.00003108\n",
      "Epoch: 851, RMSE: 12.2441, MAE: 4.4506, MAPE: 33.2870,Test Loss: 0.00009025\n",
      "=========epoch 852=========\n",
      "Iteraion 10, Train Loss: 0.00006159\n",
      "Iteraion 20, Train Loss: 0.00005367\n",
      "Iteraion 30, Train Loss: 0.00005220\n",
      "Epoch: 852, RMSE: 7.1422, MAE: 3.0108, MAPE: 32.4981,Train Loss: 0.00003099\n",
      "Epoch: 852, RMSE: 12.2231, MAE: 4.4463, MAPE: 33.2978,Test Loss: 0.00008994\n",
      "=========epoch 853=========\n",
      "Iteraion 10, Train Loss: 0.00004666\n",
      "Iteraion 20, Train Loss: 0.00004325\n",
      "Iteraion 30, Train Loss: 0.00005375\n",
      "Epoch: 853, RMSE: 7.1642, MAE: 3.0144, MAPE: 32.3810,Train Loss: 0.00003118\n",
      "Epoch: 853, RMSE: 12.2372, MAE: 4.4513, MAPE: 33.2129,Test Loss: 0.00009014\n",
      "=========epoch 854=========\n",
      "Iteraion 10, Train Loss: 0.00004687\n",
      "Iteraion 20, Train Loss: 0.00004742\n",
      "Iteraion 30, Train Loss: 0.00004447\n",
      "Epoch: 854, RMSE: 7.1552, MAE: 3.0125, MAPE: 32.4044,Train Loss: 0.00003110\n",
      "Epoch: 854, RMSE: 12.2387, MAE: 4.4510, MAPE: 33.2378,Test Loss: 0.00009017\n",
      "=========epoch 855=========\n",
      "Iteraion 10, Train Loss: 0.00005575\n",
      "Iteraion 20, Train Loss: 0.00004185\n",
      "Iteraion 30, Train Loss: 0.00004860\n",
      "Epoch: 855, RMSE: 7.1511, MAE: 3.0116, MAPE: 32.4305,Train Loss: 0.00003106\n",
      "Epoch: 855, RMSE: 12.2345, MAE: 4.4485, MAPE: 33.2570,Test Loss: 0.00009010\n",
      "=========epoch 856=========\n",
      "Iteraion 10, Train Loss: 0.00005022\n",
      "Iteraion 20, Train Loss: 0.00005233\n",
      "Iteraion 30, Train Loss: 0.00005176\n",
      "Epoch: 856, RMSE: 7.1453, MAE: 3.0105, MAPE: 32.4812,Train Loss: 0.00003101\n",
      "Epoch: 856, RMSE: 12.2286, MAE: 4.4476, MAPE: 33.2877,Test Loss: 0.00009002\n",
      "=========epoch 857=========\n",
      "Iteraion 10, Train Loss: 0.00005309\n",
      "Iteraion 20, Train Loss: 0.00004962\n",
      "Iteraion 30, Train Loss: 0.00004981\n",
      "Epoch: 857, RMSE: 7.1434, MAE: 3.0102, MAPE: 32.4646,Train Loss: 0.00003100\n",
      "Epoch: 857, RMSE: 12.2340, MAE: 4.4486, MAPE: 33.2807,Test Loss: 0.00009010\n",
      "=========epoch 858=========\n",
      "Iteraion 10, Train Loss: 0.00005201\n",
      "Iteraion 20, Train Loss: 0.00004970\n",
      "Iteraion 30, Train Loss: 0.00005757\n",
      "Epoch: 858, RMSE: 7.1467, MAE: 3.0116, MAPE: 32.4141,Train Loss: 0.00003102\n",
      "Epoch: 858, RMSE: 12.2298, MAE: 4.4499, MAPE: 33.2314,Test Loss: 0.00009003\n",
      "=========epoch 859=========\n",
      "Iteraion 10, Train Loss: 0.00004833\n",
      "Iteraion 20, Train Loss: 0.00005350\n",
      "Iteraion 30, Train Loss: 0.00004769\n",
      "Epoch: 859, RMSE: 7.1453, MAE: 3.0110, MAPE: 32.4250,Train Loss: 0.00003101\n",
      "Epoch: 859, RMSE: 12.2346, MAE: 4.4513, MAPE: 33.2504,Test Loss: 0.00009010\n",
      "=========epoch 860=========\n",
      "Iteraion 10, Train Loss: 0.00004546\n",
      "Iteraion 20, Train Loss: 0.00005262\n",
      "Iteraion 30, Train Loss: 0.00005642\n",
      "Epoch: 860, RMSE: 7.1422, MAE: 3.0097, MAPE: 32.4712,Train Loss: 0.00003098\n",
      "Epoch: 860, RMSE: 12.2306, MAE: 4.4484, MAPE: 33.2779,Test Loss: 0.00009004\n",
      "=========epoch 861=========\n",
      "Iteraion 10, Train Loss: 0.00005408\n",
      "Iteraion 20, Train Loss: 0.00005177\n",
      "Iteraion 30, Train Loss: 0.00005098\n",
      "Epoch: 861, RMSE: 7.1383, MAE: 3.0093, MAPE: 32.5773,Train Loss: 0.00003095\n",
      "Epoch: 861, RMSE: 12.2242, MAE: 4.4458, MAPE: 33.3588,Test Loss: 0.00008995\n",
      "=========epoch 862=========\n",
      "Iteraion 10, Train Loss: 0.00005695\n",
      "Iteraion 20, Train Loss: 0.00004707\n",
      "Iteraion 30, Train Loss: 0.00005111\n",
      "Epoch: 862, RMSE: 7.1424, MAE: 3.0098, MAPE: 32.4928,Train Loss: 0.00003099\n",
      "Epoch: 862, RMSE: 12.2271, MAE: 4.4460, MAPE: 33.2869,Test Loss: 0.00009000\n",
      "=========epoch 863=========\n",
      "Iteraion 10, Train Loss: 0.00004618\n",
      "Iteraion 20, Train Loss: 0.00005216\n",
      "Iteraion 30, Train Loss: 0.00005640\n",
      "Epoch: 863, RMSE: 7.1614, MAE: 3.0135, MAPE: 32.3832,Train Loss: 0.00003115\n",
      "Epoch: 863, RMSE: 12.2384, MAE: 4.4508, MAPE: 33.2055,Test Loss: 0.00009016\n",
      "=========epoch 864=========\n",
      "Iteraion 10, Train Loss: 0.00006208\n",
      "Iteraion 20, Train Loss: 0.00005507\n",
      "Iteraion 30, Train Loss: 0.00004791\n",
      "Epoch: 864, RMSE: 7.1540, MAE: 3.0116, MAPE: 32.4493,Train Loss: 0.00003109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 864, RMSE: 12.2304, MAE: 4.4471, MAPE: 33.2427,Test Loss: 0.00009004\n",
      "=========epoch 865=========\n",
      "Iteraion 10, Train Loss: 0.00005267\n",
      "Iteraion 20, Train Loss: 0.00005156\n",
      "Iteraion 30, Train Loss: 0.00004762\n",
      "Epoch: 865, RMSE: 7.1486, MAE: 3.0110, MAPE: 32.4685,Train Loss: 0.00003104\n",
      "Epoch: 865, RMSE: 12.2295, MAE: 4.4471, MAPE: 33.2601,Test Loss: 0.00009003\n",
      "=========epoch 866=========\n",
      "Iteraion 10, Train Loss: 0.00005275\n",
      "Iteraion 20, Train Loss: 0.00005039\n",
      "Iteraion 30, Train Loss: 0.00005094\n",
      "Epoch: 866, RMSE: 7.1422, MAE: 3.0104, MAPE: 32.5349,Train Loss: 0.00003099\n",
      "Epoch: 866, RMSE: 12.2245, MAE: 4.4454, MAPE: 33.3037,Test Loss: 0.00008996\n",
      "=========epoch 867=========\n",
      "Iteraion 10, Train Loss: 0.00004363\n",
      "Iteraion 20, Train Loss: 0.00004994\n",
      "Iteraion 30, Train Loss: 0.00005316\n",
      "Epoch: 867, RMSE: 7.1487, MAE: 3.0112, MAPE: 32.4578,Train Loss: 0.00003104\n",
      "Epoch: 867, RMSE: 12.2333, MAE: 4.4481, MAPE: 33.2429,Test Loss: 0.00009009\n",
      "=========epoch 868=========\n",
      "Iteraion 10, Train Loss: 0.00005203\n",
      "Iteraion 20, Train Loss: 0.00005125\n",
      "Iteraion 30, Train Loss: 0.00005529\n",
      "Epoch: 868, RMSE: 7.1442, MAE: 3.0122, MAPE: 32.4476,Train Loss: 0.00003100\n",
      "Epoch: 868, RMSE: 12.2285, MAE: 4.4500, MAPE: 33.2306,Test Loss: 0.00009001\n",
      "=========epoch 869=========\n",
      "Iteraion 10, Train Loss: 0.00005155\n",
      "Iteraion 20, Train Loss: 0.00004786\n",
      "Iteraion 30, Train Loss: 0.00004862\n",
      "Epoch: 869, RMSE: 7.1416, MAE: 3.0098, MAPE: 32.4800,Train Loss: 0.00003098\n",
      "Epoch: 869, RMSE: 12.2252, MAE: 4.4464, MAPE: 33.2794,Test Loss: 0.00008997\n",
      "=========epoch 870=========\n",
      "Iteraion 10, Train Loss: 0.00005628\n",
      "Iteraion 20, Train Loss: 0.00005074\n",
      "Iteraion 30, Train Loss: 0.00005269\n",
      "Epoch: 870, RMSE: 7.1547, MAE: 3.0119, MAPE: 32.4475,Train Loss: 0.00003109\n",
      "Epoch: 870, RMSE: 12.2373, MAE: 4.4504, MAPE: 33.2637,Test Loss: 0.00009014\n",
      "=========epoch 871=========\n",
      "Iteraion 10, Train Loss: 0.00005202\n",
      "Iteraion 20, Train Loss: 0.00004559\n",
      "Iteraion 30, Train Loss: 0.00005579\n",
      "Epoch: 871, RMSE: 7.1381, MAE: 3.0099, MAPE: 32.5521,Train Loss: 0.00003095\n",
      "Epoch: 871, RMSE: 12.2315, MAE: 4.4487, MAPE: 33.3430,Test Loss: 0.00009006\n",
      "=========epoch 872=========\n",
      "Iteraion 10, Train Loss: 0.00004815\n",
      "Iteraion 20, Train Loss: 0.00005905\n",
      "Iteraion 30, Train Loss: 0.00004646\n",
      "Epoch: 872, RMSE: 7.1351, MAE: 3.0089, MAPE: 32.5832,Train Loss: 0.00003092\n",
      "Epoch: 872, RMSE: 12.2175, MAE: 4.4442, MAPE: 33.3615,Test Loss: 0.00008985\n",
      "=========epoch 873=========\n",
      "Iteraion 10, Train Loss: 0.00005546\n",
      "Iteraion 20, Train Loss: 0.00004777\n",
      "Iteraion 30, Train Loss: 0.00005056\n",
      "Epoch: 873, RMSE: 7.1445, MAE: 3.0096, MAPE: 32.4907,Train Loss: 0.00003101\n",
      "Epoch: 873, RMSE: 12.2311, MAE: 4.4470, MAPE: 33.2938,Test Loss: 0.00009005\n",
      "=========epoch 874=========\n",
      "Iteraion 10, Train Loss: 0.00005297\n",
      "Iteraion 20, Train Loss: 0.00005590\n",
      "Iteraion 30, Train Loss: 0.00005009\n",
      "Epoch: 874, RMSE: 7.1366, MAE: 3.0087, MAPE: 32.5197,Train Loss: 0.00003094\n",
      "Epoch: 874, RMSE: 12.2215, MAE: 4.4457, MAPE: 33.3109,Test Loss: 0.00008991\n",
      "=========epoch 875=========\n",
      "Iteraion 10, Train Loss: 0.00005438\n",
      "Iteraion 20, Train Loss: 0.00004909\n",
      "Iteraion 30, Train Loss: 0.00005055\n",
      "Epoch: 875, RMSE: 7.1394, MAE: 3.0083, MAPE: 32.5204,Train Loss: 0.00003096\n",
      "Epoch: 875, RMSE: 12.2246, MAE: 4.4452, MAPE: 33.3083,Test Loss: 0.00008996\n",
      "=========epoch 876=========\n",
      "Iteraion 10, Train Loss: 0.00004977\n",
      "Iteraion 20, Train Loss: 0.00004417\n",
      "Iteraion 30, Train Loss: 0.00004888\n",
      "Epoch: 876, RMSE: 7.1491, MAE: 3.0106, MAPE: 32.4360,Train Loss: 0.00003105\n",
      "Epoch: 876, RMSE: 12.2243, MAE: 4.4455, MAPE: 33.2508,Test Loss: 0.00008995\n",
      "=========epoch 877=========\n",
      "Iteraion 10, Train Loss: 0.00005517\n",
      "Iteraion 20, Train Loss: 0.00005829\n",
      "Iteraion 30, Train Loss: 0.00005720\n",
      "Epoch: 877, RMSE: 7.1371, MAE: 3.0091, MAPE: 32.5349,Train Loss: 0.00003094\n",
      "Epoch: 877, RMSE: 12.2209, MAE: 4.4445, MAPE: 33.3229,Test Loss: 0.00008991\n",
      "=========epoch 878=========\n",
      "Iteraion 10, Train Loss: 0.00005139\n",
      "Iteraion 20, Train Loss: 0.00004889\n",
      "Iteraion 30, Train Loss: 0.00005346\n",
      "Epoch: 878, RMSE: 7.1395, MAE: 3.0089, MAPE: 32.5213,Train Loss: 0.00003096\n",
      "Epoch: 878, RMSE: 12.2214, MAE: 4.4442, MAPE: 33.3191,Test Loss: 0.00008991\n",
      "=========epoch 879=========\n",
      "Iteraion 10, Train Loss: 0.00005005\n",
      "Iteraion 20, Train Loss: 0.00005213\n",
      "Iteraion 30, Train Loss: 0.00005187\n",
      "Epoch: 879, RMSE: 7.1421, MAE: 3.0097, MAPE: 32.4714,Train Loss: 0.00003098\n",
      "Epoch: 879, RMSE: 12.2211, MAE: 4.4454, MAPE: 33.2794,Test Loss: 0.00008991\n",
      "=========epoch 880=========\n",
      "Iteraion 10, Train Loss: 0.00005488\n",
      "Iteraion 20, Train Loss: 0.00005685\n",
      "Iteraion 30, Train Loss: 0.00004561\n",
      "Epoch: 880, RMSE: 7.1376, MAE: 3.0088, MAPE: 32.5445,Train Loss: 0.00003095\n",
      "Epoch: 880, RMSE: 12.2318, MAE: 4.4465, MAPE: 33.3279,Test Loss: 0.00009007\n",
      "=========epoch 881=========\n",
      "Iteraion 10, Train Loss: 0.00005818\n",
      "Iteraion 20, Train Loss: 0.00005118\n",
      "Iteraion 30, Train Loss: 0.00004877\n",
      "Epoch: 881, RMSE: 7.1391, MAE: 3.0096, MAPE: 32.5228,Train Loss: 0.00003096\n",
      "Epoch: 881, RMSE: 12.2330, MAE: 4.4473, MAPE: 33.3240,Test Loss: 0.00009009\n",
      "=========epoch 882=========\n",
      "Iteraion 10, Train Loss: 0.00005104\n",
      "Iteraion 20, Train Loss: 0.00004783\n",
      "Iteraion 30, Train Loss: 0.00005463\n",
      "Epoch: 882, RMSE: 7.1427, MAE: 3.0106, MAPE: 32.4389,Train Loss: 0.00003099\n",
      "Epoch: 882, RMSE: 12.2389, MAE: 4.4512, MAPE: 33.2691,Test Loss: 0.00009017\n",
      "=========epoch 883=========\n",
      "Iteraion 10, Train Loss: 0.00005620\n",
      "Iteraion 20, Train Loss: 0.00004537\n",
      "Iteraion 30, Train Loss: 0.00005384\n",
      "Epoch: 883, RMSE: 7.1391, MAE: 3.0104, MAPE: 32.4444,Train Loss: 0.00003096\n",
      "Epoch: 883, RMSE: 12.2320, MAE: 4.4502, MAPE: 33.2631,Test Loss: 0.00009007\n",
      "=========epoch 884=========\n",
      "Iteraion 10, Train Loss: 0.00005060\n",
      "Iteraion 20, Train Loss: 0.00004554\n",
      "Iteraion 30, Train Loss: 0.00005307\n",
      "Epoch: 884, RMSE: 7.1402, MAE: 3.0094, MAPE: 32.5131,Train Loss: 0.00003097\n",
      "Epoch: 884, RMSE: 12.2366, MAE: 4.4493, MAPE: 33.3267,Test Loss: 0.00009014\n",
      "=========epoch 885=========\n",
      "Iteraion 10, Train Loss: 0.00005438\n",
      "Iteraion 20, Train Loss: 0.00005328\n",
      "Iteraion 30, Train Loss: 0.00005177\n",
      "Epoch: 885, RMSE: 7.1327, MAE: 3.0087, MAPE: 32.5439,Train Loss: 0.00003090\n",
      "Epoch: 885, RMSE: 12.2263, MAE: 4.4461, MAPE: 33.3348,Test Loss: 0.00008998\n",
      "=========epoch 886=========\n",
      "Iteraion 10, Train Loss: 0.00005385\n",
      "Iteraion 20, Train Loss: 0.00005183\n",
      "Iteraion 30, Train Loss: 0.00004853\n",
      "Epoch: 886, RMSE: 7.1470, MAE: 3.0105, MAPE: 32.4399,Train Loss: 0.00003103\n",
      "Epoch: 886, RMSE: 12.2366, MAE: 4.4498, MAPE: 33.2650,Test Loss: 0.00009014\n",
      "=========epoch 887=========\n",
      "Iteraion 10, Train Loss: 0.00005345\n",
      "Iteraion 20, Train Loss: 0.00005326\n",
      "Iteraion 30, Train Loss: 0.00005054\n",
      "Epoch: 887, RMSE: 7.1412, MAE: 3.0092, MAPE: 32.4613,Train Loss: 0.00003098\n",
      "Epoch: 887, RMSE: 12.2325, MAE: 4.4466, MAPE: 33.2745,Test Loss: 0.00009007\n",
      "=========epoch 888=========\n",
      "Iteraion 10, Train Loss: 0.00004927\n",
      "Iteraion 20, Train Loss: 0.00004788\n",
      "Iteraion 30, Train Loss: 0.00005303\n",
      "Epoch: 888, RMSE: 7.1451, MAE: 3.0093, MAPE: 32.4536,Train Loss: 0.00003101\n",
      "Epoch: 888, RMSE: 12.2375, MAE: 4.4481, MAPE: 33.2673,Test Loss: 0.00009015\n",
      "=========epoch 889=========\n",
      "Iteraion 10, Train Loss: 0.00005058\n",
      "Iteraion 20, Train Loss: 0.00004935\n",
      "Iteraion 30, Train Loss: 0.00004450\n",
      "Epoch: 889, RMSE: 7.1446, MAE: 3.0099, MAPE: 32.4460,Train Loss: 0.00003101\n",
      "Epoch: 889, RMSE: 12.2336, MAE: 4.4478, MAPE: 33.2570,Test Loss: 0.00009009\n",
      "=========epoch 890=========\n",
      "Iteraion 10, Train Loss: 0.00005321\n",
      "Iteraion 20, Train Loss: 0.00005530\n",
      "Iteraion 30, Train Loss: 0.00005518\n",
      "Epoch: 890, RMSE: 7.1363, MAE: 3.0084, MAPE: 32.5024,Train Loss: 0.00003093\n",
      "Epoch: 890, RMSE: 12.2333, MAE: 4.4470, MAPE: 33.2941,Test Loss: 0.00009009\n",
      "=========epoch 891=========\n",
      "Iteraion 10, Train Loss: 0.00004953\n",
      "Iteraion 20, Train Loss: 0.00004574\n",
      "Iteraion 30, Train Loss: 0.00005546\n",
      "Epoch: 891, RMSE: 7.1338, MAE: 3.0082, MAPE: 32.5028,Train Loss: 0.00003091\n",
      "Epoch: 891, RMSE: 12.2311, MAE: 4.4464, MAPE: 33.2958,Test Loss: 0.00009005\n",
      "=========epoch 892=========\n",
      "Iteraion 10, Train Loss: 0.00004598\n",
      "Iteraion 20, Train Loss: 0.00005256\n",
      "Iteraion 30, Train Loss: 0.00005332\n",
      "Epoch: 892, RMSE: 7.1399, MAE: 3.0085, MAPE: 32.4849,Train Loss: 0.00003097\n",
      "Epoch: 892, RMSE: 12.2347, MAE: 4.4472, MAPE: 33.2853,Test Loss: 0.00009011\n",
      "=========epoch 893=========\n",
      "Iteraion 10, Train Loss: 0.00005290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraion 20, Train Loss: 0.00004971\n",
      "Iteraion 30, Train Loss: 0.00004659\n",
      "Epoch: 893, RMSE: 7.1447, MAE: 3.0091, MAPE: 32.4398,Train Loss: 0.00003101\n",
      "Epoch: 893, RMSE: 12.2361, MAE: 4.4481, MAPE: 33.2534,Test Loss: 0.00009012\n",
      "=========epoch 894=========\n",
      "Iteraion 10, Train Loss: 0.00005190\n",
      "Iteraion 20, Train Loss: 0.00005010\n",
      "Iteraion 30, Train Loss: 0.00005115\n",
      "Epoch: 894, RMSE: 7.1529, MAE: 3.0121, MAPE: 32.3867,Train Loss: 0.00003108\n",
      "Epoch: 894, RMSE: 12.2326, MAE: 4.4493, MAPE: 33.2123,Test Loss: 0.00009007\n",
      "=========epoch 895=========\n",
      "Iteraion 10, Train Loss: 0.00005178\n",
      "Iteraion 20, Train Loss: 0.00006252\n",
      "Iteraion 30, Train Loss: 0.00005139\n",
      "Epoch: 895, RMSE: 7.1362, MAE: 3.0089, MAPE: 32.5354,Train Loss: 0.00003093\n",
      "Epoch: 895, RMSE: 12.2118, MAE: 4.4435, MAPE: 33.3299,Test Loss: 0.00008977\n",
      "=========epoch 896=========\n",
      "Iteraion 10, Train Loss: 0.00005159\n",
      "Iteraion 20, Train Loss: 0.00004730\n",
      "Iteraion 30, Train Loss: 0.00005178\n",
      "Epoch: 896, RMSE: 7.1352, MAE: 3.0078, MAPE: 32.5140,Train Loss: 0.00003092\n",
      "Epoch: 896, RMSE: 12.2160, MAE: 4.4436, MAPE: 33.3230,Test Loss: 0.00008983\n",
      "=========epoch 897=========\n",
      "Iteraion 10, Train Loss: 0.00005104\n",
      "Iteraion 20, Train Loss: 0.00005274\n",
      "Iteraion 30, Train Loss: 0.00005247\n",
      "Epoch: 897, RMSE: 7.1384, MAE: 3.0089, MAPE: 32.4500,Train Loss: 0.00003095\n",
      "Epoch: 897, RMSE: 12.2320, MAE: 4.4479, MAPE: 33.2622,Test Loss: 0.00009007\n",
      "=========epoch 898=========\n",
      "Iteraion 10, Train Loss: 0.00005563\n",
      "Iteraion 20, Train Loss: 0.00005506\n",
      "Iteraion 30, Train Loss: 0.00005643\n",
      "Epoch: 898, RMSE: 7.1410, MAE: 3.0084, MAPE: 32.4448,Train Loss: 0.00003097\n",
      "Epoch: 898, RMSE: 12.2422, MAE: 4.4499, MAPE: 33.2735,Test Loss: 0.00009022\n",
      "=========epoch 899=========\n",
      "Iteraion 10, Train Loss: 0.00004956\n",
      "Iteraion 20, Train Loss: 0.00005319\n",
      "Iteraion 30, Train Loss: 0.00005209\n",
      "Epoch: 899, RMSE: 7.1413, MAE: 3.0097, MAPE: 32.4016,Train Loss: 0.00003098\n",
      "Epoch: 899, RMSE: 12.2348, MAE: 4.4489, MAPE: 33.2261,Test Loss: 0.00009011\n",
      "=========epoch 900=========\n",
      "Iteraion 10, Train Loss: 0.00005252\n",
      "Iteraion 20, Train Loss: 0.00004723\n",
      "Iteraion 30, Train Loss: 0.00004620\n",
      "Epoch: 900, RMSE: 7.1348, MAE: 3.0083, MAPE: 32.4759,Train Loss: 0.00003092\n",
      "Epoch: 900, RMSE: 12.2270, MAE: 4.4465, MAPE: 33.2791,Test Loss: 0.00009000\n",
      "=========epoch 901=========\n",
      "Iteraion 10, Train Loss: 0.00005191\n",
      "Iteraion 20, Train Loss: 0.00005332\n",
      "Iteraion 30, Train Loss: 0.00004712\n",
      "Epoch: 901, RMSE: 7.1549, MAE: 3.0121, MAPE: 32.3816,Train Loss: 0.00003110\n",
      "Epoch: 901, RMSE: 12.2375, MAE: 4.4499, MAPE: 33.2162,Test Loss: 0.00009015\n",
      "=========epoch 902=========\n",
      "Iteraion 10, Train Loss: 0.00004885\n",
      "Iteraion 20, Train Loss: 0.00005251\n",
      "Iteraion 30, Train Loss: 0.00005315\n",
      "Epoch: 902, RMSE: 7.1398, MAE: 3.0088, MAPE: 32.4427,Train Loss: 0.00003096\n",
      "Epoch: 902, RMSE: 12.2321, MAE: 4.4487, MAPE: 33.2543,Test Loss: 0.00009007\n",
      "=========epoch 903=========\n",
      "Iteraion 10, Train Loss: 0.00004212\n",
      "Iteraion 20, Train Loss: 0.00005219\n",
      "Iteraion 30, Train Loss: 0.00005284\n",
      "Epoch: 903, RMSE: 7.1494, MAE: 3.0111, MAPE: 32.3814,Train Loss: 0.00003105\n",
      "Epoch: 903, RMSE: 12.2399, MAE: 4.4512, MAPE: 33.2042,Test Loss: 0.00009018\n",
      "=========epoch 904=========\n",
      "Iteraion 10, Train Loss: 0.00004913\n",
      "Iteraion 20, Train Loss: 0.00005037\n",
      "Iteraion 30, Train Loss: 0.00004759\n",
      "Epoch: 904, RMSE: 7.1641, MAE: 3.0140, MAPE: 32.3857,Train Loss: 0.00003118\n",
      "Epoch: 904, RMSE: 12.2426, MAE: 4.4512, MAPE: 33.1959,Test Loss: 0.00009022\n",
      "=========epoch 905=========\n",
      "Iteraion 10, Train Loss: 0.00005326\n",
      "Iteraion 20, Train Loss: 0.00005006\n",
      "Iteraion 30, Train Loss: 0.00004818\n",
      "Epoch: 905, RMSE: 7.1533, MAE: 3.0118, MAPE: 32.4112,Train Loss: 0.00003108\n",
      "Epoch: 905, RMSE: 12.2439, MAE: 4.4509, MAPE: 33.2189,Test Loss: 0.00009024\n",
      "=========epoch 906=========\n",
      "Iteraion 10, Train Loss: 0.00005016\n",
      "Iteraion 20, Train Loss: 0.00005156\n",
      "Iteraion 30, Train Loss: 0.00004871\n",
      "Epoch: 906, RMSE: 7.1332, MAE: 3.0091, MAPE: 32.5649,Train Loss: 0.00003091\n",
      "Epoch: 906, RMSE: 12.2152, MAE: 4.4436, MAPE: 33.3374,Test Loss: 0.00008982\n",
      "=========epoch 907=========\n",
      "Iteraion 10, Train Loss: 0.00004816\n",
      "Iteraion 20, Train Loss: 0.00005096\n",
      "Iteraion 30, Train Loss: 0.00005400\n",
      "Epoch: 907, RMSE: 7.1537, MAE: 3.0117, MAPE: 32.3979,Train Loss: 0.00003109\n",
      "Epoch: 907, RMSE: 12.2345, MAE: 4.4489, MAPE: 33.2157,Test Loss: 0.00009010\n",
      "=========epoch 908=========\n",
      "Iteraion 10, Train Loss: 0.00005332\n",
      "Iteraion 20, Train Loss: 0.00004993\n",
      "Iteraion 30, Train Loss: 0.00005309\n",
      "Epoch: 908, RMSE: 7.1424, MAE: 3.0097, MAPE: 32.4579,Train Loss: 0.00003099\n",
      "Epoch: 908, RMSE: 12.2380, MAE: 4.4501, MAPE: 33.2657,Test Loss: 0.00009016\n",
      "=========epoch 909=========\n",
      "Iteraion 10, Train Loss: 0.00004662\n",
      "Iteraion 20, Train Loss: 0.00005628\n",
      "Iteraion 30, Train Loss: 0.00004500\n",
      "Epoch: 909, RMSE: 7.1517, MAE: 3.0108, MAPE: 32.4432,Train Loss: 0.00003107\n",
      "Epoch: 909, RMSE: 12.2385, MAE: 4.4498, MAPE: 33.2559,Test Loss: 0.00009016\n",
      "=========epoch 910=========\n",
      "Iteraion 10, Train Loss: 0.00004470\n",
      "Iteraion 20, Train Loss: 0.00005021\n",
      "Iteraion 30, Train Loss: 0.00004714\n",
      "Epoch: 910, RMSE: 7.1336, MAE: 3.0095, MAPE: 32.5384,Train Loss: 0.00003091\n",
      "Epoch: 910, RMSE: 12.2226, MAE: 4.4474, MAPE: 33.3201,Test Loss: 0.00008993\n",
      "=========epoch 911=========\n",
      "Iteraion 10, Train Loss: 0.00005017\n",
      "Iteraion 20, Train Loss: 0.00005083\n",
      "Iteraion 30, Train Loss: 0.00005016\n",
      "Epoch: 911, RMSE: 7.1733, MAE: 3.0177, MAPE: 32.3242,Train Loss: 0.00003126\n",
      "Epoch: 911, RMSE: 12.2475, MAE: 4.4563, MAPE: 33.1656,Test Loss: 0.00009029\n",
      "=========epoch 912=========\n",
      "Iteraion 10, Train Loss: 0.00004956\n",
      "Iteraion 20, Train Loss: 0.00005117\n",
      "Iteraion 30, Train Loss: 0.00005498\n",
      "Epoch: 912, RMSE: 7.1356, MAE: 3.0082, MAPE: 32.5262,Train Loss: 0.00003093\n",
      "Epoch: 912, RMSE: 12.2339, MAE: 4.4496, MAPE: 33.3061,Test Loss: 0.00009010\n",
      "=========epoch 913=========\n",
      "Iteraion 10, Train Loss: 0.00005008\n",
      "Iteraion 20, Train Loss: 0.00005989\n",
      "Iteraion 30, Train Loss: 0.00004514\n",
      "Epoch: 913, RMSE: 7.1373, MAE: 3.0102, MAPE: 32.4594,Train Loss: 0.00003094\n",
      "Epoch: 913, RMSE: 12.2345, MAE: 4.4519, MAPE: 33.2721,Test Loss: 0.00009011\n",
      "=========epoch 914=========\n",
      "Iteraion 10, Train Loss: 0.00005101\n",
      "Iteraion 20, Train Loss: 0.00005211\n",
      "Iteraion 30, Train Loss: 0.00005032\n",
      "Epoch: 914, RMSE: 7.1463, MAE: 3.0097, MAPE: 32.4193,Train Loss: 0.00003102\n",
      "Epoch: 914, RMSE: 12.2406, MAE: 4.4528, MAPE: 33.2575,Test Loss: 0.00009020\n",
      "=========epoch 915=========\n",
      "Iteraion 10, Train Loss: 0.00004953\n",
      "Iteraion 20, Train Loss: 0.00005329\n",
      "Iteraion 30, Train Loss: 0.00005610\n",
      "Epoch: 915, RMSE: 7.1352, MAE: 3.0089, MAPE: 32.4727,Train Loss: 0.00003092\n",
      "Epoch: 915, RMSE: 12.2308, MAE: 4.4497, MAPE: 33.2790,Test Loss: 0.00009005\n",
      "=========epoch 916=========\n",
      "Iteraion 10, Train Loss: 0.00005227\n",
      "Iteraion 20, Train Loss: 0.00004858\n",
      "Iteraion 30, Train Loss: 0.00005330\n",
      "Epoch: 916, RMSE: 7.1362, MAE: 3.0073, MAPE: 32.4993,Train Loss: 0.00003093\n",
      "Epoch: 916, RMSE: 12.2347, MAE: 4.4480, MAPE: 33.2982,Test Loss: 0.00009011\n",
      "=========epoch 917=========\n",
      "Iteraion 10, Train Loss: 0.00004633\n",
      "Iteraion 20, Train Loss: 0.00005329\n",
      "Iteraion 30, Train Loss: 0.00004644\n",
      "Epoch: 917, RMSE: 7.1418, MAE: 3.0092, MAPE: 32.4272,Train Loss: 0.00003098\n",
      "Epoch: 917, RMSE: 12.2369, MAE: 4.4506, MAPE: 33.2498,Test Loss: 0.00009014\n",
      "=========epoch 918=========\n",
      "Iteraion 10, Train Loss: 0.00004396\n",
      "Iteraion 20, Train Loss: 0.00005563\n",
      "Iteraion 30, Train Loss: 0.00005032\n",
      "Epoch: 918, RMSE: 7.1339, MAE: 3.0081, MAPE: 32.4612,Train Loss: 0.00003091\n",
      "Epoch: 918, RMSE: 12.2230, MAE: 4.4477, MAPE: 33.2775,Test Loss: 0.00008993\n",
      "=========epoch 919=========\n",
      "Iteraion 10, Train Loss: 0.00006173\n",
      "Iteraion 20, Train Loss: 0.00005013\n",
      "Iteraion 30, Train Loss: 0.00005521\n",
      "Epoch: 919, RMSE: 7.1717, MAE: 3.0160, MAPE: 32.3395,Train Loss: 0.00003124\n",
      "Epoch: 919, RMSE: 12.2497, MAE: 4.4549, MAPE: 33.1849,Test Loss: 0.00009033\n",
      "=========epoch 920=========\n",
      "Iteraion 10, Train Loss: 0.00004510\n",
      "Iteraion 20, Train Loss: 0.00005012\n",
      "Iteraion 30, Train Loss: 0.00004999\n",
      "Epoch: 920, RMSE: 7.1413, MAE: 3.0076, MAPE: 32.5184,Train Loss: 0.00003098\n",
      "Epoch: 920, RMSE: 12.2323, MAE: 4.4460, MAPE: 33.3250,Test Loss: 0.00009007\n",
      "=========epoch 921=========\n",
      "Iteraion 10, Train Loss: 0.00004870\n",
      "Iteraion 20, Train Loss: 0.00005233\n",
      "Iteraion 30, Train Loss: 0.00004910\n",
      "Epoch: 921, RMSE: 7.1314, MAE: 3.0075, MAPE: 32.5378,Train Loss: 0.00003089\n",
      "Epoch: 921, RMSE: 12.2274, MAE: 4.4466, MAPE: 33.3381,Test Loss: 0.00009000\n",
      "=========epoch 922=========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraion 10, Train Loss: 0.00005239\n",
      "Iteraion 20, Train Loss: 0.00004920\n",
      "Iteraion 30, Train Loss: 0.00005542\n",
      "Epoch: 922, RMSE: 7.1315, MAE: 3.0066, MAPE: 32.5366,Train Loss: 0.00003089\n",
      "Epoch: 922, RMSE: 12.2301, MAE: 4.4468, MAPE: 33.3306,Test Loss: 0.00009005\n",
      "=========epoch 923=========\n",
      "Iteraion 10, Train Loss: 0.00005371\n",
      "Iteraion 20, Train Loss: 0.00005164\n",
      "Iteraion 30, Train Loss: 0.00005643\n",
      "Epoch: 923, RMSE: 7.1477, MAE: 3.0110, MAPE: 32.3965,Train Loss: 0.00003103\n",
      "Epoch: 923, RMSE: 12.2416, MAE: 4.4530, MAPE: 33.2152,Test Loss: 0.00009021\n",
      "=========epoch 924=========\n",
      "Iteraion 10, Train Loss: 0.00004893\n",
      "Iteraion 20, Train Loss: 0.00005258\n",
      "Iteraion 30, Train Loss: 0.00004618\n",
      "Epoch: 924, RMSE: 7.1552, MAE: 3.0112, MAPE: 32.4155,Train Loss: 0.00003110\n",
      "Epoch: 924, RMSE: 12.2511, MAE: 4.4528, MAPE: 33.2437,Test Loss: 0.00009035\n",
      "=========epoch 925=========\n",
      "Iteraion 10, Train Loss: 0.00005276\n",
      "Iteraion 20, Train Loss: 0.00005481\n",
      "Iteraion 30, Train Loss: 0.00004833\n",
      "Epoch: 925, RMSE: 7.1338, MAE: 3.0084, MAPE: 32.4734,Train Loss: 0.00003091\n",
      "Epoch: 925, RMSE: 12.2320, MAE: 4.4486, MAPE: 33.2908,Test Loss: 0.00009007\n",
      "=========epoch 926=========\n",
      "Iteraion 10, Train Loss: 0.00004945\n",
      "Iteraion 20, Train Loss: 0.00005321\n",
      "Iteraion 30, Train Loss: 0.00004785\n",
      "Epoch: 926, RMSE: 7.1320, MAE: 3.0066, MAPE: 32.5228,Train Loss: 0.00003090\n",
      "Epoch: 926, RMSE: 12.2246, MAE: 4.4455, MAPE: 33.3115,Test Loss: 0.00008996\n",
      "=========epoch 927=========\n",
      "Iteraion 10, Train Loss: 0.00005118\n",
      "Iteraion 20, Train Loss: 0.00005374\n",
      "Iteraion 30, Train Loss: 0.00005043\n",
      "Epoch: 927, RMSE: 7.1376, MAE: 3.0076, MAPE: 32.4672,Train Loss: 0.00003095\n",
      "Epoch: 927, RMSE: 12.2424, MAE: 4.4500, MAPE: 33.2903,Test Loss: 0.00009022\n",
      "=========epoch 928=========\n",
      "Iteraion 10, Train Loss: 0.00005084\n",
      "Iteraion 20, Train Loss: 0.00004995\n",
      "Iteraion 30, Train Loss: 0.00005121\n",
      "Epoch: 928, RMSE: 7.1328, MAE: 3.0081, MAPE: 32.5016,Train Loss: 0.00003090\n",
      "Epoch: 928, RMSE: 12.2340, MAE: 4.4510, MAPE: 33.3238,Test Loss: 0.00009010\n",
      "=========epoch 929=========\n",
      "Iteraion 10, Train Loss: 0.00005239\n",
      "Iteraion 20, Train Loss: 0.00005101\n",
      "Iteraion 30, Train Loss: 0.00006226\n",
      "Epoch: 929, RMSE: 7.1483, MAE: 3.0101, MAPE: 32.4111,Train Loss: 0.00003104\n",
      "Epoch: 929, RMSE: 12.2459, MAE: 4.4521, MAPE: 33.2417,Test Loss: 0.00009027\n",
      "=========epoch 930=========\n",
      "Iteraion 10, Train Loss: 0.00004800\n",
      "Iteraion 20, Train Loss: 0.00005336\n",
      "Iteraion 30, Train Loss: 0.00004565\n",
      "Epoch: 930, RMSE: 7.1343, MAE: 3.0078, MAPE: 32.5043,Train Loss: 0.00003092\n",
      "Epoch: 930, RMSE: 12.2231, MAE: 4.4463, MAPE: 33.3063,Test Loss: 0.00008994\n",
      "=========epoch 931=========\n",
      "Iteraion 10, Train Loss: 0.00004571\n",
      "Iteraion 20, Train Loss: 0.00004886\n",
      "Iteraion 30, Train Loss: 0.00005320\n",
      "Epoch: 931, RMSE: 7.1381, MAE: 3.0070, MAPE: 32.5128,Train Loss: 0.00003095\n",
      "Epoch: 931, RMSE: 12.2384, MAE: 4.4482, MAPE: 33.3186,Test Loss: 0.00009016\n",
      "=========epoch 932=========\n",
      "Iteraion 10, Train Loss: 0.00005307\n",
      "Iteraion 20, Train Loss: 0.00005890\n",
      "Iteraion 30, Train Loss: 0.00005217\n",
      "Epoch: 932, RMSE: 7.1300, MAE: 3.0062, MAPE: 32.5403,Train Loss: 0.00003088\n",
      "Epoch: 932, RMSE: 12.2294, MAE: 4.4469, MAPE: 33.3383,Test Loss: 0.00009003\n",
      "=========epoch 933=========\n",
      "Iteraion 10, Train Loss: 0.00005059\n",
      "Iteraion 20, Train Loss: 0.00005141\n",
      "Iteraion 30, Train Loss: 0.00005397\n",
      "Epoch: 933, RMSE: 7.1342, MAE: 3.0065, MAPE: 32.4908,Train Loss: 0.00003092\n",
      "Epoch: 933, RMSE: 12.2426, MAE: 4.4502, MAPE: 33.2979,Test Loss: 0.00009022\n",
      "=========epoch 934=========\n",
      "Iteraion 10, Train Loss: 0.00004965\n",
      "Iteraion 20, Train Loss: 0.00005452\n",
      "Iteraion 30, Train Loss: 0.00005280\n",
      "Epoch: 934, RMSE: 7.1369, MAE: 3.0072, MAPE: 32.5041,Train Loss: 0.00003094\n",
      "Epoch: 934, RMSE: 12.2403, MAE: 4.4496, MAPE: 33.3027,Test Loss: 0.00009019\n",
      "=========epoch 935=========\n",
      "Iteraion 10, Train Loss: 0.00004674\n",
      "Iteraion 20, Train Loss: 0.00004861\n",
      "Iteraion 30, Train Loss: 0.00005207\n",
      "Epoch: 935, RMSE: 7.1551, MAE: 3.0108, MAPE: 32.4285,Train Loss: 0.00003110\n",
      "Epoch: 935, RMSE: 12.2455, MAE: 4.4500, MAPE: 33.2374,Test Loss: 0.00009027\n",
      "=========epoch 936=========\n",
      "Iteraion 10, Train Loss: 0.00005400\n",
      "Iteraion 20, Train Loss: 0.00004298\n",
      "Iteraion 30, Train Loss: 0.00004712\n",
      "Epoch: 936, RMSE: 7.1462, MAE: 3.0087, MAPE: 32.4405,Train Loss: 0.00003102\n",
      "Epoch: 936, RMSE: 12.2386, MAE: 4.4481, MAPE: 33.2576,Test Loss: 0.00009017\n",
      "=========epoch 937=========\n",
      "Iteraion 10, Train Loss: 0.00004902\n",
      "Iteraion 20, Train Loss: 0.00004679\n",
      "Iteraion 30, Train Loss: 0.00005393\n",
      "Epoch: 937, RMSE: 7.1319, MAE: 3.0072, MAPE: 32.5545,Train Loss: 0.00003090\n",
      "Epoch: 937, RMSE: 12.2209, MAE: 4.4438, MAPE: 33.3367,Test Loss: 0.00008991\n",
      "=========epoch 938=========\n",
      "Iteraion 10, Train Loss: 0.00005258\n",
      "Iteraion 20, Train Loss: 0.00005133\n",
      "Iteraion 30, Train Loss: 0.00005155\n",
      "Epoch: 938, RMSE: 7.1397, MAE: 3.0086, MAPE: 32.4224,Train Loss: 0.00003096\n",
      "Epoch: 938, RMSE: 12.2267, MAE: 4.4476, MAPE: 33.2462,Test Loss: 0.00008999\n",
      "=========epoch 939=========\n",
      "Iteraion 10, Train Loss: 0.00005532\n",
      "Iteraion 20, Train Loss: 0.00004803\n",
      "Iteraion 30, Train Loss: 0.00005019\n",
      "Epoch: 939, RMSE: 7.1380, MAE: 3.0078, MAPE: 32.4572,Train Loss: 0.00003095\n",
      "Epoch: 939, RMSE: 12.2295, MAE: 4.4480, MAPE: 33.2727,Test Loss: 0.00009004\n",
      "=========epoch 940=========\n",
      "Iteraion 10, Train Loss: 0.00004712\n",
      "Iteraion 20, Train Loss: 0.00006093\n",
      "Iteraion 30, Train Loss: 0.00005591\n",
      "Epoch: 940, RMSE: 7.1297, MAE: 3.0067, MAPE: 32.5463,Train Loss: 0.00003088\n",
      "Epoch: 940, RMSE: 12.2107, MAE: 4.4434, MAPE: 33.3324,Test Loss: 0.00008976\n",
      "=========epoch 941=========\n",
      "Iteraion 10, Train Loss: 0.00004963\n",
      "Iteraion 20, Train Loss: 0.00005306\n",
      "Iteraion 30, Train Loss: 0.00004924\n",
      "Epoch: 941, RMSE: 7.1278, MAE: 3.0059, MAPE: 32.5541,Train Loss: 0.00003086\n",
      "Epoch: 941, RMSE: 12.2184, MAE: 4.4447, MAPE: 33.3449,Test Loss: 0.00008987\n",
      "=========epoch 942=========\n",
      "Iteraion 10, Train Loss: 0.00004945\n",
      "Iteraion 20, Train Loss: 0.00005297\n",
      "Iteraion 30, Train Loss: 0.00005174\n",
      "Epoch: 942, RMSE: 7.1386, MAE: 3.0077, MAPE: 32.4650,Train Loss: 0.00003095\n",
      "Epoch: 942, RMSE: 12.2291, MAE: 4.4476, MAPE: 33.2752,Test Loss: 0.00009003\n",
      "=========epoch 943=========\n",
      "Iteraion 10, Train Loss: 0.00004791\n",
      "Iteraion 20, Train Loss: 0.00005988\n",
      "Iteraion 30, Train Loss: 0.00004792\n",
      "Epoch: 943, RMSE: 7.1265, MAE: 3.0057, MAPE: 32.5707,Train Loss: 0.00003085\n",
      "Epoch: 943, RMSE: 12.2257, MAE: 4.4459, MAPE: 33.3520,Test Loss: 0.00008998\n",
      "=========epoch 944=========\n",
      "Iteraion 10, Train Loss: 0.00005504\n",
      "Iteraion 20, Train Loss: 0.00005489\n",
      "Iteraion 30, Train Loss: 0.00004817\n",
      "Epoch: 944, RMSE: 7.1309, MAE: 3.0065, MAPE: 32.5147,Train Loss: 0.00003089\n",
      "Epoch: 944, RMSE: 12.2309, MAE: 4.4485, MAPE: 33.3222,Test Loss: 0.00009005\n",
      "=========epoch 945=========\n",
      "Iteraion 10, Train Loss: 0.00005380\n",
      "Iteraion 20, Train Loss: 0.00004687\n",
      "Iteraion 30, Train Loss: 0.00005529\n",
      "Epoch: 945, RMSE: 7.1196, MAE: 3.0053, MAPE: 32.6758,Train Loss: 0.00003079\n",
      "Epoch: 945, RMSE: 12.2214, MAE: 4.4444, MAPE: 33.4529,Test Loss: 0.00008991\n",
      "=========epoch 946=========\n",
      "Iteraion 10, Train Loss: 0.00004727\n",
      "Iteraion 20, Train Loss: 0.00005693\n",
      "Iteraion 30, Train Loss: 0.00004413\n",
      "Epoch: 946, RMSE: 7.1349, MAE: 3.0061, MAPE: 32.5093,Train Loss: 0.00003092\n",
      "Epoch: 946, RMSE: 12.2359, MAE: 4.4481, MAPE: 33.3229,Test Loss: 0.00009013\n",
      "=========epoch 947=========\n",
      "Iteraion 10, Train Loss: 0.00004833\n",
      "Iteraion 20, Train Loss: 0.00004904\n",
      "Iteraion 30, Train Loss: 0.00005043\n",
      "Epoch: 947, RMSE: 7.1302, MAE: 3.0062, MAPE: 32.5228,Train Loss: 0.00003088\n",
      "Epoch: 947, RMSE: 12.2318, MAE: 4.4476, MAPE: 33.3384,Test Loss: 0.00009007\n",
      "=========epoch 948=========\n",
      "Iteraion 10, Train Loss: 0.00004646\n",
      "Iteraion 20, Train Loss: 0.00005033\n",
      "Iteraion 30, Train Loss: 0.00005248\n",
      "Epoch: 948, RMSE: 7.1384, MAE: 3.0072, MAPE: 32.4956,Train Loss: 0.00003095\n",
      "Epoch: 948, RMSE: 12.2442, MAE: 4.4502, MAPE: 33.3063,Test Loss: 0.00009025\n",
      "=========epoch 949=========\n",
      "Iteraion 10, Train Loss: 0.00005291\n",
      "Iteraion 20, Train Loss: 0.00004879\n",
      "Iteraion 30, Train Loss: 0.00005114\n",
      "Epoch: 949, RMSE: 7.1465, MAE: 3.0087, MAPE: 32.4568,Train Loss: 0.00003102\n",
      "Epoch: 949, RMSE: 12.2411, MAE: 4.4492, MAPE: 33.2787,Test Loss: 0.00009020\n",
      "=========epoch 950=========\n",
      "Iteraion 10, Train Loss: 0.00004822\n",
      "Iteraion 20, Train Loss: 0.00005300\n",
      "Iteraion 30, Train Loss: 0.00006032\n",
      "Epoch: 950, RMSE: 7.1358, MAE: 3.0087, MAPE: 32.4782,Train Loss: 0.00003093\n",
      "Epoch: 950, RMSE: 12.2343, MAE: 4.4510, MAPE: 33.3077,Test Loss: 0.00009010\n",
      "=========epoch 951=========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraion 10, Train Loss: 0.00004858\n",
      "Iteraion 20, Train Loss: 0.00005224\n",
      "Iteraion 30, Train Loss: 0.00004743\n",
      "Epoch: 951, RMSE: 7.1358, MAE: 3.0080, MAPE: 32.4011,Train Loss: 0.00003093\n",
      "Epoch: 951, RMSE: 12.2393, MAE: 4.4531, MAPE: 33.2491,Test Loss: 0.00009018\n",
      "=========epoch 952=========\n",
      "Iteraion 10, Train Loss: 0.00005262\n",
      "Iteraion 20, Train Loss: 0.00005087\n",
      "Iteraion 30, Train Loss: 0.00005357\n",
      "Epoch: 952, RMSE: 7.1429, MAE: 3.0091, MAPE: 32.3723,Train Loss: 0.00003099\n",
      "Epoch: 952, RMSE: 12.2467, MAE: 4.4542, MAPE: 33.2291,Test Loss: 0.00009028\n",
      "=========epoch 953=========\n",
      "Iteraion 10, Train Loss: 0.00004657\n",
      "Iteraion 20, Train Loss: 0.00005281\n",
      "Iteraion 30, Train Loss: 0.00005468\n",
      "Epoch: 953, RMSE: 7.1268, MAE: 3.0075, MAPE: 32.5888,Train Loss: 0.00003085\n",
      "Epoch: 953, RMSE: 12.2299, MAE: 4.4475, MAPE: 33.3897,Test Loss: 0.00009004\n",
      "=========epoch 954=========\n",
      "Iteraion 10, Train Loss: 0.00004739\n",
      "Iteraion 20, Train Loss: 0.00005743\n",
      "Iteraion 30, Train Loss: 0.00005108\n",
      "Epoch: 954, RMSE: 7.1367, MAE: 3.0075, MAPE: 32.4250,Train Loss: 0.00003094\n",
      "Epoch: 954, RMSE: 12.2426, MAE: 4.4515, MAPE: 33.2600,Test Loss: 0.00009022\n",
      "=========epoch 955=========\n",
      "Iteraion 10, Train Loss: 0.00004686\n",
      "Iteraion 20, Train Loss: 0.00005192\n",
      "Iteraion 30, Train Loss: 0.00005051\n",
      "Epoch: 955, RMSE: 7.1491, MAE: 3.0085, MAPE: 32.4331,Train Loss: 0.00003105\n",
      "Epoch: 955, RMSE: 12.2585, MAE: 4.4533, MAPE: 33.2677,Test Loss: 0.00009045\n",
      "=========epoch 956=========\n",
      "Iteraion 10, Train Loss: 0.00005419\n",
      "Iteraion 20, Train Loss: 0.00005263\n",
      "Iteraion 30, Train Loss: 0.00005382\n",
      "Epoch: 956, RMSE: 7.1312, MAE: 3.0050, MAPE: 32.4784,Train Loss: 0.00003089\n",
      "Epoch: 956, RMSE: 12.2359, MAE: 4.4481, MAPE: 33.2937,Test Loss: 0.00009012\n",
      "=========epoch 957=========\n",
      "Iteraion 10, Train Loss: 0.00005042\n",
      "Iteraion 20, Train Loss: 0.00004816\n",
      "Iteraion 30, Train Loss: 0.00005276\n",
      "Epoch: 957, RMSE: 7.1378, MAE: 3.0059, MAPE: 32.4583,Train Loss: 0.00003095\n",
      "Epoch: 957, RMSE: 12.2386, MAE: 4.4472, MAPE: 33.2867,Test Loss: 0.00009016\n",
      "=========epoch 958=========\n",
      "Iteraion 10, Train Loss: 0.00005110\n",
      "Iteraion 20, Train Loss: 0.00005588\n",
      "Iteraion 30, Train Loss: 0.00005082\n",
      "Epoch: 958, RMSE: 7.1268, MAE: 3.0064, MAPE: 32.5055,Train Loss: 0.00003085\n",
      "Epoch: 958, RMSE: 12.2353, MAE: 4.4493, MAPE: 33.3163,Test Loss: 0.00009012\n",
      "=========epoch 959=========\n",
      "Iteraion 10, Train Loss: 0.00004696\n",
      "Iteraion 20, Train Loss: 0.00005389\n",
      "Iteraion 30, Train Loss: 0.00004868\n",
      "Epoch: 959, RMSE: 7.1363, MAE: 3.0056, MAPE: 32.4499,Train Loss: 0.00003093\n",
      "Epoch: 959, RMSE: 12.2418, MAE: 4.4486, MAPE: 33.2777,Test Loss: 0.00009021\n",
      "=========epoch 960=========\n",
      "Iteraion 10, Train Loss: 0.00004888\n",
      "Iteraion 20, Train Loss: 0.00005312\n",
      "Iteraion 30, Train Loss: 0.00005502\n",
      "Epoch: 960, RMSE: 7.1288, MAE: 3.0046, MAPE: 32.4732,Train Loss: 0.00003087\n",
      "Epoch: 960, RMSE: 12.2372, MAE: 4.4476, MAPE: 33.2880,Test Loss: 0.00009015\n",
      "=========epoch 961=========\n",
      "Iteraion 10, Train Loss: 0.00005154\n",
      "Iteraion 20, Train Loss: 0.00006059\n",
      "Iteraion 30, Train Loss: 0.00005094\n",
      "Epoch: 961, RMSE: 7.1341, MAE: 3.0062, MAPE: 32.4316,Train Loss: 0.00003092\n",
      "Epoch: 961, RMSE: 12.2371, MAE: 4.4495, MAPE: 33.2565,Test Loss: 0.00009014\n",
      "=========epoch 962=========\n",
      "Iteraion 10, Train Loss: 0.00005705\n",
      "Iteraion 20, Train Loss: 0.00004958\n",
      "Iteraion 30, Train Loss: 0.00005787\n",
      "Epoch: 962, RMSE: 7.1220, MAE: 3.0039, MAPE: 32.5592,Train Loss: 0.00003081\n",
      "Epoch: 962, RMSE: 12.2256, MAE: 4.4444, MAPE: 33.3534,Test Loss: 0.00008997\n",
      "=========epoch 963=========\n",
      "Iteraion 10, Train Loss: 0.00005102\n",
      "Iteraion 20, Train Loss: 0.00005007\n",
      "Iteraion 30, Train Loss: 0.00005258\n",
      "Epoch: 963, RMSE: 7.1303, MAE: 3.0053, MAPE: 32.4809,Train Loss: 0.00003088\n",
      "Epoch: 963, RMSE: 12.2336, MAE: 4.4466, MAPE: 33.2915,Test Loss: 0.00009009\n",
      "=========epoch 964=========\n",
      "Iteraion 10, Train Loss: 0.00004843\n",
      "Iteraion 20, Train Loss: 0.00004428\n",
      "Iteraion 30, Train Loss: 0.00004968\n",
      "Epoch: 964, RMSE: 7.1291, MAE: 3.0055, MAPE: 32.5222,Train Loss: 0.00003087\n",
      "Epoch: 964, RMSE: 12.2289, MAE: 4.4457, MAPE: 33.3240,Test Loss: 0.00009002\n",
      "=========epoch 965=========\n",
      "Iteraion 10, Train Loss: 0.00005298\n",
      "Iteraion 20, Train Loss: 0.00004786\n",
      "Iteraion 30, Train Loss: 0.00005386\n",
      "Epoch: 965, RMSE: 7.1277, MAE: 3.0056, MAPE: 32.4730,Train Loss: 0.00003086\n",
      "Epoch: 965, RMSE: 12.2331, MAE: 4.4478, MAPE: 33.2924,Test Loss: 0.00009009\n",
      "=========epoch 966=========\n",
      "Iteraion 10, Train Loss: 0.00005327\n",
      "Iteraion 20, Train Loss: 0.00005493\n",
      "Iteraion 30, Train Loss: 0.00005468\n",
      "Epoch: 966, RMSE: 7.1275, MAE: 3.0057, MAPE: 32.4850,Train Loss: 0.00003086\n",
      "Epoch: 966, RMSE: 12.2237, MAE: 4.4458, MAPE: 33.2924,Test Loss: 0.00008994\n",
      "=========epoch 967=========\n",
      "Iteraion 10, Train Loss: 0.00005442\n",
      "Iteraion 20, Train Loss: 0.00004980\n",
      "Iteraion 30, Train Loss: 0.00004837\n",
      "Epoch: 967, RMSE: 7.1231, MAE: 3.0044, MAPE: 32.5734,Train Loss: 0.00003082\n",
      "Epoch: 967, RMSE: 12.2291, MAE: 4.4456, MAPE: 33.3679,Test Loss: 0.00009002\n",
      "=========epoch 968=========\n",
      "Iteraion 10, Train Loss: 0.00005011\n",
      "Iteraion 20, Train Loss: 0.00005161\n",
      "Iteraion 30, Train Loss: 0.00004807\n",
      "Epoch: 968, RMSE: 7.1287, MAE: 3.0054, MAPE: 32.4446,Train Loss: 0.00003087\n",
      "Epoch: 968, RMSE: 12.2342, MAE: 4.4479, MAPE: 33.2581,Test Loss: 0.00009010\n",
      "=========epoch 969=========\n",
      "Iteraion 10, Train Loss: 0.00005498\n",
      "Iteraion 20, Train Loss: 0.00005268\n",
      "Iteraion 30, Train Loss: 0.00005247\n",
      "Epoch: 969, RMSE: 7.1251, MAE: 3.0059, MAPE: 32.5234,Train Loss: 0.00003084\n",
      "Epoch: 969, RMSE: 12.2239, MAE: 4.4456, MAPE: 33.3152,Test Loss: 0.00008995\n",
      "=========epoch 970=========\n",
      "Iteraion 10, Train Loss: 0.00005021\n",
      "Iteraion 20, Train Loss: 0.00004949\n",
      "Iteraion 30, Train Loss: 0.00005362\n",
      "Epoch: 970, RMSE: 7.1312, MAE: 3.0047, MAPE: 32.4768,Train Loss: 0.00003089\n",
      "Epoch: 970, RMSE: 12.2349, MAE: 4.4453, MAPE: 33.2789,Test Loss: 0.00009011\n",
      "=========epoch 971=========\n",
      "Iteraion 10, Train Loss: 0.00005235\n",
      "Iteraion 20, Train Loss: 0.00004402\n",
      "Iteraion 30, Train Loss: 0.00005030\n",
      "Epoch: 971, RMSE: 7.1469, MAE: 3.0093, MAPE: 32.3495,Train Loss: 0.00003103\n",
      "Epoch: 971, RMSE: 12.2511, MAE: 4.4545, MAPE: 33.1962,Test Loss: 0.00009035\n",
      "=========epoch 972=========\n",
      "Iteraion 10, Train Loss: 0.00004872\n",
      "Iteraion 20, Train Loss: 0.00005056\n",
      "Iteraion 30, Train Loss: 0.00004773\n",
      "Epoch: 972, RMSE: 7.1267, MAE: 3.0046, MAPE: 32.4500,Train Loss: 0.00003085\n",
      "Epoch: 972, RMSE: 12.2270, MAE: 4.4464, MAPE: 33.2611,Test Loss: 0.00008999\n",
      "=========epoch 973=========\n",
      "Iteraion 10, Train Loss: 0.00005325\n",
      "Iteraion 20, Train Loss: 0.00005242\n",
      "Iteraion 30, Train Loss: 0.00004820\n",
      "Epoch: 973, RMSE: 7.1207, MAE: 3.0036, MAPE: 32.5321,Train Loss: 0.00003080\n",
      "Epoch: 973, RMSE: 12.2273, MAE: 4.4458, MAPE: 33.3235,Test Loss: 0.00009000\n",
      "=========epoch 974=========\n",
      "Iteraion 10, Train Loss: 0.00005249\n",
      "Iteraion 20, Train Loss: 0.00005132\n",
      "Iteraion 30, Train Loss: 0.00004769\n",
      "Epoch: 974, RMSE: 7.1424, MAE: 3.0091, MAPE: 32.3836,Train Loss: 0.00003099\n",
      "Epoch: 974, RMSE: 12.2319, MAE: 4.4488, MAPE: 33.2054,Test Loss: 0.00009006\n",
      "=========epoch 975=========\n",
      "Iteraion 10, Train Loss: 0.00004902\n",
      "Iteraion 20, Train Loss: 0.00005254\n",
      "Iteraion 30, Train Loss: 0.00005349\n",
      "Epoch: 975, RMSE: 7.1429, MAE: 3.0072, MAPE: 32.4202,Train Loss: 0.00003099\n",
      "Epoch: 975, RMSE: 12.2396, MAE: 4.4479, MAPE: 33.2271,Test Loss: 0.00009018\n",
      "=========epoch 976=========\n",
      "Iteraion 10, Train Loss: 0.00004944\n",
      "Iteraion 20, Train Loss: 0.00004253\n",
      "Iteraion 30, Train Loss: 0.00005193\n",
      "Epoch: 976, RMSE: 7.1275, MAE: 3.0040, MAPE: 32.4910,Train Loss: 0.00003086\n",
      "Epoch: 976, RMSE: 12.2415, MAE: 4.4478, MAPE: 33.3022,Test Loss: 0.00009021\n",
      "=========epoch 977=========\n",
      "Iteraion 10, Train Loss: 0.00004649\n",
      "Iteraion 20, Train Loss: 0.00004403\n",
      "Iteraion 30, Train Loss: 0.00004978\n",
      "Epoch: 977, RMSE: 7.1335, MAE: 3.0056, MAPE: 32.4240,Train Loss: 0.00003091\n",
      "Epoch: 977, RMSE: 12.2352, MAE: 4.4475, MAPE: 33.2507,Test Loss: 0.00009011\n",
      "=========epoch 978=========\n",
      "Iteraion 10, Train Loss: 0.00005415\n",
      "Iteraion 20, Train Loss: 0.00005458\n",
      "Iteraion 30, Train Loss: 0.00005146\n",
      "Epoch: 978, RMSE: 7.1273, MAE: 3.0050, MAPE: 32.4762,Train Loss: 0.00003086\n",
      "Epoch: 978, RMSE: 12.2248, MAE: 4.4450, MAPE: 33.2738,Test Loss: 0.00008996\n",
      "=========epoch 979=========\n",
      "Iteraion 10, Train Loss: 0.00005773\n",
      "Iteraion 20, Train Loss: 0.00005339\n",
      "Iteraion 30, Train Loss: 0.00005327\n",
      "Epoch: 979, RMSE: 7.1277, MAE: 3.0055, MAPE: 32.4847,Train Loss: 0.00003086\n",
      "Epoch: 979, RMSE: 12.2245, MAE: 4.4443, MAPE: 33.2814,Test Loss: 0.00008996\n",
      "=========epoch 980=========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraion 10, Train Loss: 0.00004851\n",
      "Iteraion 20, Train Loss: 0.00004781\n",
      "Iteraion 30, Train Loss: 0.00005315\n",
      "Epoch: 980, RMSE: 7.1318, MAE: 3.0053, MAPE: 32.4761,Train Loss: 0.00003089\n",
      "Epoch: 980, RMSE: 12.2284, MAE: 4.4456, MAPE: 33.2767,Test Loss: 0.00009001\n",
      "=========epoch 981=========\n",
      "Iteraion 10, Train Loss: 0.00005007\n",
      "Iteraion 20, Train Loss: 0.00005413\n",
      "Iteraion 30, Train Loss: 0.00005141\n",
      "Epoch: 981, RMSE: 7.1317, MAE: 3.0052, MAPE: 32.4930,Train Loss: 0.00003089\n",
      "Epoch: 981, RMSE: 12.2323, MAE: 4.4467, MAPE: 33.2904,Test Loss: 0.00009007\n",
      "=========epoch 982=========\n",
      "Iteraion 10, Train Loss: 0.00005253\n",
      "Iteraion 20, Train Loss: 0.00005117\n",
      "Iteraion 30, Train Loss: 0.00005313\n",
      "Epoch: 982, RMSE: 7.1241, MAE: 3.0048, MAPE: 32.5354,Train Loss: 0.00003083\n",
      "Epoch: 982, RMSE: 12.2248, MAE: 4.4469, MAPE: 33.3337,Test Loss: 0.00008996\n",
      "=========epoch 983=========\n",
      "Iteraion 10, Train Loss: 0.00005035\n",
      "Iteraion 20, Train Loss: 0.00005264\n",
      "Iteraion 30, Train Loss: 0.00005330\n",
      "Epoch: 983, RMSE: 7.1210, MAE: 3.0029, MAPE: 32.5681,Train Loss: 0.00003080\n",
      "Epoch: 983, RMSE: 12.2254, MAE: 4.4442, MAPE: 33.3561,Test Loss: 0.00008997\n",
      "=========epoch 984=========\n",
      "Iteraion 10, Train Loss: 0.00005521\n",
      "Iteraion 20, Train Loss: 0.00004904\n",
      "Iteraion 30, Train Loss: 0.00004989\n",
      "Epoch: 984, RMSE: 7.1262, MAE: 3.0043, MAPE: 32.4628,Train Loss: 0.00003085\n",
      "Epoch: 984, RMSE: 12.2221, MAE: 4.4441, MAPE: 33.2743,Test Loss: 0.00008992\n",
      "=========epoch 985=========\n",
      "Iteraion 10, Train Loss: 0.00004770\n",
      "Iteraion 20, Train Loss: 0.00005087\n",
      "Iteraion 30, Train Loss: 0.00004688\n",
      "Epoch: 985, RMSE: 7.1297, MAE: 3.0052, MAPE: 32.5079,Train Loss: 0.00003088\n",
      "Epoch: 985, RMSE: 12.2283, MAE: 4.4445, MAPE: 33.3170,Test Loss: 0.00009001\n",
      "=========epoch 986=========\n",
      "Iteraion 10, Train Loss: 0.00005052\n",
      "Iteraion 20, Train Loss: 0.00004942\n",
      "Iteraion 30, Train Loss: 0.00004504\n",
      "Epoch: 986, RMSE: 7.1314, MAE: 3.0041, MAPE: 32.5412,Train Loss: 0.00003089\n",
      "Epoch: 986, RMSE: 12.2281, MAE: 4.4433, MAPE: 33.3448,Test Loss: 0.00009001\n",
      "=========epoch 987=========\n",
      "Iteraion 10, Train Loss: 0.00005204\n",
      "Iteraion 20, Train Loss: 0.00005298\n",
      "Iteraion 30, Train Loss: 0.00005163\n",
      "Epoch: 987, RMSE: 7.1304, MAE: 3.0043, MAPE: 32.5078,Train Loss: 0.00003088\n",
      "Epoch: 987, RMSE: 12.2342, MAE: 4.4448, MAPE: 33.3108,Test Loss: 0.00009010\n",
      "=========epoch 988=========\n",
      "Iteraion 10, Train Loss: 0.00005215\n",
      "Iteraion 20, Train Loss: 0.00004557\n",
      "Iteraion 30, Train Loss: 0.00005691\n",
      "Epoch: 988, RMSE: 7.1220, MAE: 3.0038, MAPE: 32.5236,Train Loss: 0.00003081\n",
      "Epoch: 988, RMSE: 12.2200, MAE: 4.4438, MAPE: 33.3227,Test Loss: 0.00008989\n",
      "=========epoch 989=========\n",
      "Iteraion 10, Train Loss: 0.00005853\n",
      "Iteraion 20, Train Loss: 0.00005087\n",
      "Iteraion 30, Train Loss: 0.00005722\n",
      "Epoch: 989, RMSE: 7.1238, MAE: 3.0047, MAPE: 32.4893,Train Loss: 0.00003083\n",
      "Epoch: 989, RMSE: 12.2268, MAE: 4.4449, MAPE: 33.2975,Test Loss: 0.00008999\n",
      "=========epoch 990=========\n",
      "Iteraion 10, Train Loss: 0.00005339\n",
      "Iteraion 20, Train Loss: 0.00005222\n",
      "Iteraion 30, Train Loss: 0.00004898\n",
      "Epoch: 990, RMSE: 7.1245, MAE: 3.0044, MAPE: 32.5044,Train Loss: 0.00003083\n",
      "Epoch: 990, RMSE: 12.2337, MAE: 4.4479, MAPE: 33.3024,Test Loss: 0.00009009\n",
      "=========epoch 991=========\n",
      "Iteraion 10, Train Loss: 0.00004974\n",
      "Iteraion 20, Train Loss: 0.00005104\n",
      "Iteraion 30, Train Loss: 0.00005481\n",
      "Epoch: 991, RMSE: 7.1241, MAE: 3.0037, MAPE: 32.4750,Train Loss: 0.00003083\n",
      "Epoch: 991, RMSE: 12.2296, MAE: 4.4468, MAPE: 33.2859,Test Loss: 0.00009003\n",
      "=========epoch 992=========\n",
      "Iteraion 10, Train Loss: 0.00004835\n",
      "Iteraion 20, Train Loss: 0.00005246\n",
      "Iteraion 30, Train Loss: 0.00005115\n",
      "Epoch: 992, RMSE: 7.1163, MAE: 3.0023, MAPE: 32.5992,Train Loss: 0.00003076\n",
      "Epoch: 992, RMSE: 12.2182, MAE: 4.4426, MAPE: 33.3785,Test Loss: 0.00008986\n",
      "=========epoch 993=========\n",
      "Iteraion 10, Train Loss: 0.00004802\n",
      "Iteraion 20, Train Loss: 0.00005305\n",
      "Iteraion 30, Train Loss: 0.00005393\n",
      "Epoch: 993, RMSE: 7.1201, MAE: 3.0025, MAPE: 32.5346,Train Loss: 0.00003079\n",
      "Epoch: 993, RMSE: 12.2324, MAE: 4.4469, MAPE: 33.3471,Test Loss: 0.00009007\n",
      "=========epoch 994=========\n",
      "Iteraion 10, Train Loss: 0.00005240\n",
      "Iteraion 20, Train Loss: 0.00004950\n",
      "Iteraion 30, Train Loss: 0.00005210\n",
      "Epoch: 994, RMSE: 7.1196, MAE: 3.0029, MAPE: 32.5654,Train Loss: 0.00003079\n",
      "Epoch: 994, RMSE: 12.2330, MAE: 4.4469, MAPE: 33.3637,Test Loss: 0.00009008\n",
      "=========epoch 995=========\n",
      "Iteraion 10, Train Loss: 0.00005260\n",
      "Iteraion 20, Train Loss: 0.00005027\n",
      "Iteraion 30, Train Loss: 0.00005140\n",
      "Epoch: 995, RMSE: 7.1182, MAE: 3.0039, MAPE: 32.5954,Train Loss: 0.00003078\n",
      "Epoch: 995, RMSE: 12.2193, MAE: 4.4446, MAPE: 33.3824,Test Loss: 0.00008988\n",
      "=========epoch 996=========\n",
      "Iteraion 10, Train Loss: 0.00005797\n",
      "Iteraion 20, Train Loss: 0.00005054\n",
      "Iteraion 30, Train Loss: 0.00004917\n",
      "Epoch: 996, RMSE: 7.1297, MAE: 3.0046, MAPE: 32.4628,Train Loss: 0.00003088\n",
      "Epoch: 996, RMSE: 12.2363, MAE: 4.4477, MAPE: 33.2833,Test Loss: 0.00009013\n",
      "=========epoch 997=========\n",
      "Iteraion 10, Train Loss: 0.00005145\n",
      "Iteraion 20, Train Loss: 0.00004953\n",
      "Iteraion 30, Train Loss: 0.00005262\n",
      "Epoch: 997, RMSE: 7.1237, MAE: 3.0023, MAPE: 32.5170,Train Loss: 0.00003083\n",
      "Epoch: 997, RMSE: 12.2325, MAE: 4.4446, MAPE: 33.3234,Test Loss: 0.00009007\n",
      "=========epoch 998=========\n",
      "Iteraion 10, Train Loss: 0.00005646\n",
      "Iteraion 20, Train Loss: 0.00005235\n",
      "Iteraion 30, Train Loss: 0.00004666\n",
      "Epoch: 998, RMSE: 7.1224, MAE: 3.0022, MAPE: 32.5165,Train Loss: 0.00003081\n",
      "Epoch: 998, RMSE: 12.2227, MAE: 4.4431, MAPE: 33.3229,Test Loss: 0.00008993\n",
      "=========epoch 999=========\n",
      "Iteraion 10, Train Loss: 0.00005283\n",
      "Iteraion 20, Train Loss: 0.00005260\n",
      "Iteraion 30, Train Loss: 0.00005041\n",
      "Epoch: 999, RMSE: 7.1279, MAE: 3.0044, MAPE: 32.4862,Train Loss: 0.00003086\n",
      "Epoch: 999, RMSE: 12.2155, MAE: 4.4421, MAPE: 33.2874,Test Loss: 0.00008983\n",
      "=========epoch 1000=========\n",
      "Iteraion 10, Train Loss: 0.00005616\n",
      "Iteraion 20, Train Loss: 0.00005014\n",
      "Iteraion 30, Train Loss: 0.00005218\n",
      "Epoch: 1000, RMSE: 7.1172, MAE: 3.0022, MAPE: 32.5814,Train Loss: 0.00003077\n",
      "Epoch: 1000, RMSE: 12.2166, MAE: 4.4417, MAPE: 33.3683,Test Loss: 0.00008984\n"
     ]
    }
   ],
   "source": [
    "train_loss_list,test_loss_list = [],[]\n",
    "train_rmse_list,train_mae_list,train_mape_list = [],[],[]\n",
    "test_rmse_list,test_mae_list,test_mape_list = [],[],[]\n",
    "train_times = 0.0\n",
    "best_epoch, best_rmse, best_mae, best_mape, best_train_loss, best_test_loss = 0, float(\"inf\"), float(\"inf\"), float(\"inf\"), float(\"inf\"), float(\"inf\")\n",
    "for i in range(epochs):\n",
    "    print(\"=========epoch {}=========\".format(i + 1))\n",
    "    train_rmse,train_mae,train_mape,train_loss = train_one_epoch(model,train_dataset,loss_fn,optimizer)\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_rmse_list.append(train_rmse)\n",
    "    train_mae_list.append(train_mae)\n",
    "    train_mape_list.append(train_mape)\n",
    "    lr_scheduler.step()\n",
    "    print('Epoch: {}, RMSE: {:.4f}, MAE: {:.4f}, MAPE: {:.4f},Train Loss: {:.8f}'.format(\n",
    "        i + 1,train_rmse,train_mae,train_mape,train_loss))\n",
    "    # eval\n",
    "    test_rmse,test_mae,test_mape,test_loss = test(model,test_dataset,loss_fn)\n",
    "    test_loss_list.append(test_loss)\n",
    "    test_rmse_list.append(test_rmse)\n",
    "    test_mae_list.append(test_mae)\n",
    "    test_mape_list.append(test_mape)\n",
    "    if test_rmse<best_rmse:\n",
    "        best_epoch = i\n",
    "        best_rmse = test_rmse\n",
    "        best_mae = test_mae\n",
    "        best_mape = test_mape\n",
    "        best_train_loss = train_loss\n",
    "        best_test_loss = test_loss\n",
    "    print('Epoch: {}, RMSE: {:.4f}, MAE: {:.4f}, MAPE: {:.4f},Test Loss: {:.8f}'.format(\n",
    "        i + 1,test_rmse,test_mae,test_mape,test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "908c76df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"log.txt\",\"a\") as fp:\n",
    "    fp.write(\"model:{}, lr: {}, epochs:{}, batch size:{}, hidden_size:{}, train times:{}, Epoch:{}, RMSE:{}, MAE:{}, MAPE:{}, train loss:{}, test loss:{}\\n\".format(model_type,lr,epochs,batch_size,hidden_size,train_times,best_epoch, best_rmse, best_mae, best_mape, best_train_loss, best_test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fce99071",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [train_loss_list,test_loss_list,train_rmse_list,test_rmse_list,\n",
    "    train_mae_list,test_mae_list,train_mape_list,test_mape_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9833d088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 900x600 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " # metrics curve\n",
    "fname = \"{}_lr{}_b{}_h{}_d{}_metrics.png\".format(model_type,lr,batch_size,hidden_size,drop_prob)\n",
    "drawPlot(metrics,fname,[\"loss\",\"rmse\",\"mae\",\"mape\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82064885",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
